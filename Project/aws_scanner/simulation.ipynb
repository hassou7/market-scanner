{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb507f4-e480-47ad-aa58-d6bfe2cba34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook Simulation Code\n",
    "# Paste this into a single cell and run it.\n",
    "# Assumes:\n",
    "# - You're running this in a Jupyter notebook located in the same directory as aws_scanner_service.py\n",
    "# - The project structure is intact (e.g., scanner/ and run_parallel_scanner/ modules are accessible)\n",
    "# - All dependencies (pandas, asyncio, etc.) are installed\n",
    "# - This simulates Monday 2025-09-22 00:01:00 UTC (a date where ALL timeframes 1d/2d/3d/4d/1w are active)\n",
    "#   This corresponds closely to Sunday midnight UTC+1 (23:01 UTC Sunday 2025-09-21, but adjusted to trigger 1w on Monday UTC)\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import signal\n",
    "from datetime import datetime, timedelta, time\n",
    "import time as time_module\n",
    "import pandas as pd\n",
    "\n",
    "# Clear any existing cache\n",
    "try:\n",
    "    from scanner.main import kline_cache\n",
    "    kline_cache.clear()\n",
    "except ImportError:\n",
    "    print(\"Warning: Could not import kline_cache - ensure project path is set correctly\")\n",
    "\n",
    "# Set project paths (adjust if your notebook is not in the script's directory)\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)  # Assume notebook is in the script's subdir\n",
    "sys.path.insert(0, project_root)\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "# Configure logging for Jupyter (outputs to notebook)\n",
    "def setup_logging(debug_mode=False):\n",
    "    level = logging.DEBUG if debug_mode else logging.INFO\n",
    "    \n",
    "    logs_dir = os.path.join(current_dir, \"logs\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    \n",
    "    log_file = os.path.join(logs_dir, \"scanner_service.log\")\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "            logging.FileHandler(log_file)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    logging.getLogger('telegram').setLevel(logging.INFO)\n",
    "    logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "    logging.getLogger('httpcore').setLevel(logging.WARNING)\n",
    "    \n",
    "    return logging.getLogger(\"ScannerService\")\n",
    "\n",
    "logger = setup_logging(debug_mode=False)  # Set debug_mode=True for more verbose output\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "# Exchange categorization with fast/slow classification\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Fast exchanges (reliable, fast API responses)\n",
    "fast_spot_exchanges = [\n",
    "    \"binance_spot\",\n",
    "    \"bybit_spot\",\n",
    "    \"gateio_spot\"\n",
    "]\n",
    "\n",
    "fast_futures_exchanges = [\n",
    "    \"binance_futures\",\n",
    "    \"bybit_futures\", \n",
    "    \"gateio_futures\"\n",
    "]\n",
    "\n",
    "# Slow exchanges (slower API responses, need careful rate limiting)\n",
    "slow_spot_exchanges = [\n",
    "    \"kucoin_spot\",\n",
    "    \"mexc_spot\"\n",
    "]\n",
    "\n",
    "slow_futures_exchanges = [\n",
    "    \"mexc_futures\"\n",
    "]\n",
    "\n",
    "# All exchanges grouped by type\n",
    "all_fast_exchanges = fast_spot_exchanges + fast_futures_exchanges\n",
    "all_slow_exchanges = slow_spot_exchanges + slow_futures_exchanges\n",
    "all_spot_exchanges = fast_spot_exchanges + slow_spot_exchanges\n",
    "all_futures_exchanges = fast_futures_exchanges + slow_futures_exchanges\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "# Strategy configurations by type and priority\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Strategy classification\n",
    "native_strategies = [\n",
    "    \"confluence\", \"consolidation_breakout\", \"channel_breakout\", \n",
    "    \"loaded_bar\", \"trend_breakout\", \"pin_up\", \"sma50_breakout\"\n",
    "]\n",
    "\n",
    "composed_strategies = [\n",
    "    \"hbs_breakout\", \"vs_wakeup\"\n",
    "]\n",
    "\n",
    "futures_only_strategies = [\n",
    "    \"reversal_bar\", \"pin_down\"\n",
    "]\n",
    "\n",
    "# All timeframes to scan\n",
    "all_timeframes = [\"1d\", \"2d\", \"3d\", \"4d\", \"1w\"]\n",
    "\n",
    "# Main scan configurations - organized by priority and strategy type\n",
    "scan_configs = [\n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    # PRIORITY 1: FAST EXCHANGES - NATIVE STRATEGIES (highest priority for DB development)\n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"fast_native_strategies\",\n",
    "        \"timeframes\": all_timeframes,\n",
    "        \"strategies\": native_strategies,\n",
    "        \"exchanges\": fast_spot_exchanges + [\"binance_futures\"],\n",
    "        \"users\": [\"default\", \"user1\", \"user2\"],\n",
    "        \"send_telegram\": True,\n",
    "        \"min_volume_usd\": None,\n",
    "        \"priority\": 1,\n",
    "        \"exchange_type\": \"fast_mixed\",\n",
    "        \"strategy_type\": \"native\"\n",
    "    },\n",
    "    \n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    # PRIORITY 2: FAST EXCHANGES - COMPOSED STRATEGIES  \n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"fast_composed_strategies\",\n",
    "        \"timeframes\": all_timeframes,\n",
    "        \"strategies\": composed_strategies,\n",
    "        \"exchanges\": fast_spot_exchanges + [\"binance_futures\"],\n",
    "        \"users\": [\"default\", \"user1\", \"user2\"],\n",
    "        \"send_telegram\": True,\n",
    "        \"min_volume_usd\": None,\n",
    "        \"priority\": 2,\n",
    "        \"exchange_type\": \"fast_mixed\",\n",
    "        \"strategy_type\": \"composed\"\n",
    "    },\n",
    "    \n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    # PRIORITY 3: FAST FUTURES - FUTURES-ONLY STRATEGIES\n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"fast_futures_only\",\n",
    "        \"timeframes\": all_timeframes,\n",
    "        \"strategies\": futures_only_strategies,\n",
    "        \"exchanges\": fast_futures_exchanges,\n",
    "        \"users\": [\"default\"],\n",
    "        \"send_telegram\": True,\n",
    "        \"min_volume_usd\": None,\n",
    "        \"priority\": 3,\n",
    "        \"exchange_type\": \"fast_futures\",\n",
    "        \"strategy_type\": \"futures_only\"\n",
    "    },\n",
    "    \n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    # PRIORITY 4: SLOW SPOT - NATIVE STRATEGIES\n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"slow_native_strategies\",\n",
    "        \"timeframes\": all_timeframes,\n",
    "        \"strategies\": native_strategies,\n",
    "        \"exchanges\": slow_spot_exchanges,\n",
    "        \"users\": [\"default\", \"user1\", \"user2\"],\n",
    "        \"send_telegram\": True,\n",
    "        \"min_volume_usd\": None,\n",
    "        \"priority\": 4,\n",
    "        \"exchange_type\": \"slow_spot\",\n",
    "        \"strategy_type\": \"native\"\n",
    "    },\n",
    "    \n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    # PRIORITY 5: SLOW SPOT - COMPOSED STRATEGIES\n",
    "    # ────────────────────────────────────────────────────────────────────────────────────\n",
    "    {\n",
    "        \"name\": \"slow_composed_strategies\", \n",
    "        \"timeframes\": all_timeframes,\n",
    "        \"strategies\": composed_strategies,\n",
    "        \"exchanges\": slow_spot_exchanges,\n",
    "        \"users\": [\"default\", \"user1\", \"user2\"],\n",
    "        \"send_telegram\": True,\n",
    "        \"min_volume_usd\": None,\n",
    "        \"priority\": 5,\n",
    "        \"exchange_type\": \"slow_spot\",\n",
    "        \"strategy_type\": \"composed\"\n",
    "    }\n",
    "    \n",
    "    # Note: slow_futures_exchanges reserved for future use\n",
    "    # Can be added when needed with priority 6+\n",
    "]\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "# Optimized timeframe scheduling and cache management\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def get_aggregated_timeframes():\n",
    "    \"\"\"Return timeframes that require daily data aggregation\"\"\"\n",
    "    return [\"2d\", \"3d\", \"4d\"]\n",
    "\n",
    "def get_native_timeframes():\n",
    "    \"\"\"Return timeframes with native exchange support\"\"\"\n",
    "    return [\"1d\", \"1w\"]\n",
    "\n",
    "def should_clear_cache_for_session(timeframes):\n",
    "    \"\"\"\n",
    "    Determine if cache should be cleared after a session.\n",
    "    Clear cache after any session containing aggregated timeframes.\n",
    "    \"\"\"\n",
    "    aggregated_tfs = get_aggregated_timeframes()\n",
    "    return any(tf in aggregated_tfs for tf in timeframes)\n",
    "\n",
    "class OptimizedSessionManager:\n",
    "    \"\"\"Manages efficient data fetching and cache for aggregated timeframes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session_data_cache = {}  # Cache for 1d data used across aggregated timeframes\n",
    "        \n",
    "    def clear_session_cache(self):\n",
    "        \"\"\"Clear the session-level cache (daily data for aggregation)\"\"\"\n",
    "        self.session_data_cache.clear()\n",
    "        logger.info(\"Session-level cache cleared\")\n",
    "        \n",
    "    def needs_daily_data(self, timeframes):\n",
    "        \"\"\"Check if any timeframe in the list needs daily data for aggregation\"\"\"\n",
    "        return any(tf in get_aggregated_timeframes() for tf in timeframes)\n",
    "        \n",
    "    async def prepare_session_data(self, timeframes, exchanges):\n",
    "        \"\"\"\n",
    "        Pre-fetch daily data once if needed for aggregated timeframes.\n",
    "        This avoids multiple API calls for the same 1d data.\n",
    "        \"\"\"\n",
    "        if not self.needs_daily_data(timeframes):\n",
    "            return  # No aggregated timeframes, no prep needed\n",
    "            \n",
    "        logger.info(\"Pre-fetching daily data for aggregated timeframes optimization\")\n",
    "        \n",
    "        # We don't actually pre-fetch here as the parallel scanner handles this\n",
    "        # This is a placeholder for future optimization where we could pre-warm cache\n",
    "        # with a single daily fetch per exchange before running aggregated timeframes\n",
    "        pass\n",
    "\n",
    "# Global session manager\n",
    "session_manager = OptimizedSessionManager()\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "# Enhanced scheduling logic\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def get_next_candle_time(interval=\"1d\", now=None):\n",
    "    \"\"\"\n",
    "    Calculate time until next candle close for a given interval\n",
    "    \n",
    "    Args:\n",
    "        interval (str): Timeframe interval ('1d', '2d', '3d', '4d', '1w')\n",
    "        now (datetime, optional): Mock current time for testing\n",
    "        \n",
    "    Returns:\n",
    "        datetime: Next candle close time in UTC\n",
    "    \"\"\"\n",
    "    now = now or datetime.utcnow()\n",
    "    \n",
    "    if interval == \"1d\":\n",
    "        next_time = now.replace(hour=0, minute=1, second=0, microsecond=0)\n",
    "        if now >= next_time:\n",
    "            next_time += timedelta(days=1)\n",
    "    \n",
    "    elif interval == \"2d\":\n",
    "        reference_date = pd.Timestamp('2025-03-20').normalize()\n",
    "        today = pd.Timestamp(now.date())\n",
    "        days_diff = (today - reference_date).days\n",
    "        period = days_diff // 2\n",
    "        next_period_start = reference_date + timedelta(days=period * 2 + 2)\n",
    "        next_time = datetime.combine(next_period_start, time(0, 1, 0))\n",
    "        if now >= next_time:\n",
    "            next_time = datetime.combine(next_period_start + timedelta(days=2), time(0, 1, 0))\n",
    "        while next_time <= now:\n",
    "            next_time += timedelta(days=2)\n",
    "    \n",
    "    elif interval == \"3d\":\n",
    "        reference_date = pd.Timestamp('2025-03-20').normalize()\n",
    "        today = pd.Timestamp(now.date())\n",
    "        days_diff = (today - reference_date).days\n",
    "        period = days_diff // 3\n",
    "        next_period_start = reference_date + timedelta(days=period * 3 + 3)\n",
    "        next_time = datetime.combine(next_period_start, time(0, 1, 0))\n",
    "        if now >= next_time:\n",
    "            next_time = datetime.combine(next_period_start + timedelta(days=3), time(0, 1, 0))\n",
    "        while next_time <= now:\n",
    "            next_time += timedelta(days=3)\n",
    "    \n",
    "    elif interval == \"4d\":\n",
    "        reference_date = pd.Timestamp('2025-03-22').normalize()\n",
    "        today = pd.Timestamp(now.date())\n",
    "        days_diff = (today - reference_date).days\n",
    "        period = days_diff // 4\n",
    "        next_period_start = reference_date + timedelta(days=period * 4 + 4)\n",
    "        next_time = datetime.combine(next_period_start, time(0, 1, 0))\n",
    "        if now >= next_time:\n",
    "            next_time = datetime.combine(next_period_start + timedelta(days=4), time(0, 1, 0))\n",
    "        while next_time <= now:\n",
    "            next_time += timedelta(days=4)\n",
    "    \n",
    "    elif interval == \"1w\":\n",
    "        days_until_monday = (7 - now.weekday()) % 7\n",
    "        if days_until_monday == 0 and now.hour >= 0 and now.minute >= 1:\n",
    "            days_until_monday = 7\n",
    "        next_time = now.replace(hour=0, minute=1, second=0, microsecond=0)\n",
    "        next_time += timedelta(days=days_until_monday)\n",
    "        while next_time <= now:\n",
    "            next_time += timedelta(days=7)\n",
    "    \n",
    "    else:\n",
    "        logger.warning(f\"Unrecognized interval: {interval}, defaulting to 1d\")\n",
    "        return get_next_candle_time(\"1d\", now=now)\n",
    "    \n",
    "    return next_time\n",
    "\n",
    "def should_run_timeframe_today(timeframe, now=None):\n",
    "    \"\"\"\n",
    "    Check if a timeframe should run today based on aggregation schedule\n",
    "    \n",
    "    Args:\n",
    "        timeframe (str): Timeframe to check\n",
    "        now (datetime, optional): Mock current time for testing\n",
    "    \"\"\"\n",
    "    now = now or datetime.utcnow()\n",
    "    \n",
    "    if timeframe == \"1d\" or timeframe == \"1w\":\n",
    "        # Native timeframes\n",
    "        if timeframe == \"1w\":\n",
    "            return now.weekday() == 0  # Monday\n",
    "        return True  # Daily runs every day\n",
    "    \n",
    "    elif timeframe == \"2d\":\n",
    "        reference_date = pd.Timestamp('2025-03-20').normalize()\n",
    "        today = pd.Timestamp(now.date())\n",
    "        days_diff = (today - reference_date).days\n",
    "        return days_diff % 2 == 0\n",
    "    \n",
    "    elif timeframe == \"3d\":\n",
    "        reference_date = pd.Timestamp('2025-03-20').normalize()\n",
    "        today = pd.Timestamp(now.date())\n",
    "        days_diff = (today - reference_date).days\n",
    "        return days_diff % 3 == 0\n",
    "    \n",
    "    elif timeframe == \"4d\":\n",
    "        reference_date = pd.Timestamp('2025-03-22').normalize()\n",
    "        today = pd.Timestamp(now.date())\n",
    "        days_diff = (today - reference_date).days\n",
    "        return days_diff % 4 == 0\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_active_timeframes_for_today(now=None):\n",
    "    \"\"\"Get list of timeframes that should run today\"\"\"\n",
    "    all_timeframes = [\"1d\", \"2d\", \"3d\", \"4d\", \"1w\"]\n",
    "    return [tf for tf in all_timeframes if should_run_timeframe_today(tf, now=now)]\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "# Optimized scan execution with prioritization\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "async def run_optimized_scan(config, active_timeframes):\n",
    "    \"\"\"Run a single scan configuration with timeframe filtering\"\"\"\n",
    "    try:\n",
    "        from run_parallel_scanner import run_parallel_multi_timeframes_all_exchanges\n",
    "        \n",
    "        # Filter timeframes to only those active today and in config\n",
    "        scan_timeframes = [tf for tf in config[\"timeframes\"] if tf in active_timeframes]\n",
    "        \n",
    "        if not scan_timeframes:\n",
    "            logger.info(f\"Skipping {config['name']} - no active timeframes today\")\n",
    "            return 0\n",
    "        \n",
    "        logger.info(f\"Running {config['name']} scan for timeframes: {scan_timeframes}\")\n",
    "        logger.info(f\"  Strategies: {config['strategies']}\")\n",
    "        logger.info(f\"  Exchanges: {config['exchanges']}\")\n",
    "        logger.info(f\"  Priority: {config['priority']} ({config['exchange_type']})\")\n",
    "        \n",
    "        result = await run_parallel_multi_timeframes_all_exchanges(\n",
    "            timeframes=scan_timeframes,\n",
    "            strategies=config['strategies'],\n",
    "            exchanges=config['exchanges'],\n",
    "            users=config['users'],\n",
    "            send_telegram=config['send_telegram'],\n",
    "            min_volume_usd=config['min_volume_usd']\n",
    "        )\n",
    "        \n",
    "        signal_count = sum(len(signals) for signals in result.values())\n",
    "        logger.info(f\"Completed {config['name']}: {signal_count} signals found\")\n",
    "        \n",
    "        return signal_count\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in {config['name']} scan: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "async def run_prioritized_scans(active_timeframes):\n",
    "    \"\"\"\n",
    "    Run all scan configurations in priority order with optimized data fetching\n",
    "    \"\"\"\n",
    "    logger.info(\"═══════════════════════════════════════════════════════════════\")\n",
    "    logger.info(f\"Starting prioritized scan session for timeframes: {active_timeframes}\")\n",
    "    logger.info(\"═══════════════════════════════════════════════════════════════\")\n",
    "    \n",
    "    # Prepare session data if needed for aggregated timeframes\n",
    "    await session_manager.prepare_session_data(active_timeframes, all_spot_exchanges + all_futures_exchanges)\n",
    "    \n",
    "    # Sort configs by priority for execution order\n",
    "    sorted_configs = sorted(scan_configs, key=lambda x: x['priority'])\n",
    "    \n",
    "    total_signals = 0\n",
    "    \n",
    "    # Group configs by priority for potential parallel execution within priority levels\n",
    "    priority_groups = {}\n",
    "    for config in sorted_configs:\n",
    "        priority = config['priority']\n",
    "        if priority not in priority_groups:\n",
    "            priority_groups[priority] = []\n",
    "        priority_groups[priority].append(config)\n",
    "    \n",
    "    # Execute each priority group\n",
    "    for priority in sorted(priority_groups.keys()):\n",
    "        group_configs = priority_groups[priority]\n",
    "        logger.info(f\"Executing priority {priority} group ({len(group_configs)} configs)\")\n",
    "        \n",
    "        # Within each priority group, we can run configs in parallel\n",
    "        # But for now, run sequentially to respect API limits\n",
    "        group_signals = 0\n",
    "        for config in group_configs:\n",
    "            signals = await run_optimized_scan(config, active_timeframes)\n",
    "            group_signals += signals\n",
    "            \n",
    "            # Small delay between configs in same priority group\n",
    "            await asyncio.sleep(5)\n",
    "        \n",
    "        logger.info(f\"Priority {priority} complete: {group_signals} signals\")\n",
    "        total_signals += group_signals\n",
    "        \n",
    "        # Longer delay between priority groups to let fast exchanges complete\n",
    "        # before starting slow exchanges\n",
    "        if priority < max(priority_groups.keys()):\n",
    "            logger.info(\"Waiting before next priority group...\")\n",
    "            await asyncio.sleep(15)\n",
    "    \n",
    "    # Cache management - clear if session contained aggregated timeframes\n",
    "    if should_clear_cache_for_session(active_timeframes):\n",
    "        logger.info(\"Clearing cache after aggregated timeframes session\")\n",
    "        try:\n",
    "            kline_cache.clear()\n",
    "        except:\n",
    "            pass\n",
    "        session_manager.clear_session_cache()\n",
    "    \n",
    "    logger.info(\"═══════════════════════════════════════════════════════════════\")\n",
    "    logger.info(f\"Session complete: {total_signals} total signals across all priorities\")\n",
    "    logger.info(\"═══════════════════════════════════════════════════════════════\")\n",
    "    \n",
    "    return total_signals\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "# Simulation Runner (replaces the infinite scheduler loop)\n",
    "# ═════════════════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Mock time: 2025-09-22 00:01:00 UTC (Monday where ALL timeframes are active)\n",
    "mock_time = datetime(2025, 9, 22, 0, 1, 0)\n",
    "\n",
    "async def run_simulation():\n",
    "    \"\"\"Run a single simulated scan session with mocked time\"\"\"\n",
    "    logger.info(\"Starting Jupyter simulation of AWS Scanner Service\")\n",
    "    logger.info(f\"Mocked time: {mock_time.strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
    "    logger.info(f\"Strategy Classification:\")\n",
    "    logger.info(f\"  Native strategies: {native_strategies}\")\n",
    "    logger.info(f\"  Composed strategies: {composed_strategies}\")\n",
    "    logger.info(f\"  Futures-only strategies: {futures_only_strategies}\")\n",
    "    logger.info(f\"Exchange Classification:\")\n",
    "    logger.info(f\"  Fast spot exchanges: {fast_spot_exchanges}\")\n",
    "    logger.info(f\"  Fast futures exchanges: {fast_futures_exchanges}\")\n",
    "    logger.info(f\"  Slow spot exchanges: {slow_spot_exchanges}\")\n",
    "    logger.info(f\"  Slow futures exchanges: {slow_futures_exchanges}\")\n",
    "    logger.info(f\"Execution Priority:\")\n",
    "    logger.info(f\"  1. Fast Native → 2. Fast Composed → 3. Fast Futures-Only → 4. Slow Native → 5. Slow Composed\")\n",
    "    \n",
    "    # Get active timeframes under mock time (should be all 5)\n",
    "    active_timeframes = get_active_timeframes_for_today(now=mock_time)\n",
    "    logger.info(f\"Active timeframes today (mocked): {active_timeframes}\")\n",
    "    \n",
    "    if len(active_timeframes) != 5:\n",
    "        logger.warning(\"Not all timeframes are active - check mock date!\")\n",
    "        return 0\n",
    "    \n",
    "    # Clear caches for fresh start\n",
    "    try:\n",
    "        kline_cache.clear()\n",
    "    except:\n",
    "        pass\n",
    "    session_manager.clear_session_cache()\n",
    "    \n",
    "    # Run the prioritized scans\n",
    "    total_signals = await run_prioritized_scans(active_timeframes)\n",
    "    \n",
    "    logger.info(f\"Simulation complete: {total_signals} total signals\")\n",
    "    return total_signals\n",
    "\n",
    "# Run the simulation\n",
    "# (In Jupyter, this will execute asynchronously - output will appear in the notebook)\n",
    "await run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b05d7-ad50-455a-ba1e-59cbb502bf93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
