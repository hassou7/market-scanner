{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0803ceea-b095-4b1b-9e3f-7f61e2782d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BREAKOUT FOUND: GARI/USDT on 2024-10-28 - Up\n",
      "  BREAKOUT FOUND: GARI/USDT on 2024-11-04 - Down Reversal\n",
      "  BREAKOUT FOUND: GARI/USDT on 2025-05-05 - Up\n",
      "Processing ALKIMI/USDT (37/783)...\n",
      "Insufficient data for ALKIMI/USDT\n",
      "Processing STX/USDT (38/783)...\n",
      "  BREAKOUT FOUND: STX/USDT on 2021-05-17 - Down\n",
      "  BREAKOUT FOUND: STX/USDT on 2021-07-05 - Up\n",
      "  BREAKOUT FOUND: STX/USDT on 2021-08-30 - Up\n",
      "  BREAKOUT FOUND: STX/USDT on 2022-01-17 - Down\n",
      "  BREAKOUT FOUND: STX/USDT on 2022-08-15 - Down\n",
      "  BREAKOUT FOUND: STX/USDT on 2022-11-07 - Down\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 603\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backtest on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexchange\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Run backtest\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbacktest_breakout_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexchange\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[3], line 398\u001b[0m, in \u001b[0;36mbacktest_breakout_strategy\u001b[0;34m(strategy_name, exchange, limit)\u001b[0m\n\u001b[1;32m    394\u001b[0m relative_idx \u001b[38;5;241m=\u001b[39m bar_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(clean_df)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Detect breakout using relative index on clean OHLCV data\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     detected, result \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detected:\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  BREAKOUT FOUND: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_df\u001b[38;5;241m.\u001b[39mindex[bar_idx]\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirection\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mdetect_confluence_wrapper\u001b[0;34m(df, check_bar)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_confluence_wrapper\u001b[39m(df, check_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Wrapper for detect_confluence that checks both bullish and bearish directions.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Prioritizes reversals, then bullish, then bearish.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    Returns same format: (detected: bool, result: dict)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     detected_bull, result_bull \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_confluence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_bullish\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     detected_bear, result_bear \u001b[38;5;241m=\u001b[39m detect_confluence(df, check_bar\u001b[38;5;241m=\u001b[39mcheck_bar, is_bullish\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detected_bull \u001b[38;5;129;01mor\u001b[39;00m detected_bear:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# Prioritize reversal\u001b[39;00m\n",
      "File \u001b[0;32m~/work/Crypto/sevenfigures-bot/hbs_2025/Project/custom_strategies/confluence.py:140\u001b[0m, in \u001b[0;36mdetect_confluence\u001b[0;34m(df, doji_threshold, ctx_len, range_floor, len_fast, len_mid, len_slow, check_bar, is_bullish)\u001b[0m\n\u001b[1;32m    138\u001b[0m     recent \u001b[38;5;241m=\u001b[39m [v\u001b[38;5;241m.\u001b[39miloc[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m), i) \u001b[38;5;28;01mif\u001b[39;00m down_bar_vsa\u001b[38;5;241m.\u001b[39miloc[j]]\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recent:\n\u001b[0;32m--> 140\u001b[0m         \u001b[43mbroad_rel_high\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39miloc[i] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(recent)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# serious vol: current vs last opposite-direction bar\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m broad_rel_high\u001b[38;5;241m.\u001b[39miloc[i]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 885\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1895\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:2138\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_check_is_chained_assignment_possible()\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;66;03m# actually do the set\u001b[39;00m\n\u001b[0;32m-> 2138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_maybe_update_cacher(clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:399\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/blocks.py:1150\u001b[0m, in \u001b[0;36mBlock.setitem\u001b[0;34m(self, indexer, value, using_cow)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, indexer, value, using_cow: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Block:\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m    Attempt self.values[indexer] = value, possibly creating a new array.\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m    be a compatible shape.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1150\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_fill_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m     values \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/blocks.py:235\u001b[0m, in \u001b[0;36mBlock._standardize_fill_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_standardize_fill_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# if we are passed a scalar None, convert it here\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m _dtype_obj \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mis_valid_na_for_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    236\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:693\u001b[0m, in \u001b[0;36mis_valid_na_for_dtype\u001b[0;34m(obj, dtype)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m arr[notna(np\u001b[38;5;241m.\u001b[39masarray(arr))]\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_na_for_dtype\u001b[39m(obj, dtype: DtypeObj) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    694\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m    isna check that excludes incompatible dtypes\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m    bool\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_scalar(obj) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isna(obj):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SF BREAKOUT STRATEGY BACKTESTER\n",
    "\n",
    "This script backtests breakout strategies on historical 1w data from Seven Figures server.\n",
    "It automatically fetches all USDT pairs from Kucoin or Mexc and tests historical breakout patterns.\n",
    "\n",
    "FEATURES:\n",
    "- Auto-discovers all USDT pairs using SF service\n",
    "- Tests wedge_breakout, channel_breakout, consolidation_breakout, confluence, hbs_breakout strategies\n",
    "- Calculates 50SMA breakout overlap for comparison\n",
    "- Adds close position indicators (lower/middle/upper third)\n",
    "- Extracts confluence components: spread_breakout, high_volume, momentum_breakout, is_engulfing_reversal\n",
    "- Saves detailed results and summary statistics to CSV files\n",
    "\n",
    "USAGE:\n",
    "strategy = \"confluence\"  # or \"wedge_breakout\", \"channel_breakout\", \"consolidation_breakout\", \"hbs_breakout\"\n",
    "exchange = \"Kucoin\"          # or \"Mexc\"\n",
    "results = backtest_breakout_strategy(strategy, exchange, limit=200)\n",
    "save_backtest_results(results, strategy, exchange)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from exchanges.sf_pairs_service import SFPairsService\n",
    "from custom_strategies import detect_wedge_breakout, detect_channel_breakout, detect_consolidation_breakout, detect_confluence\n",
    "\n",
    "def detect_confluence_wrapper(df, check_bar=-1):\n",
    "    \"\"\"\n",
    "    Wrapper for detect_confluence that checks both bullish and bearish directions.\n",
    "    Prioritizes reversals, then bullish, then bearish.\n",
    "    Returns same format: (detected: bool, result: dict)\n",
    "    \"\"\"\n",
    "    detected_bull, result_bull = detect_confluence(df, check_bar=check_bar, is_bullish=True)\n",
    "    detected_bear, result_bear = detect_confluence(df, check_bar=check_bar, is_bullish=False)\n",
    "    \n",
    "    if detected_bull or detected_bear:\n",
    "        # Prioritize reversal\n",
    "        if result_bull.get('is_engulfing_reversal', False):\n",
    "            return True, result_bull\n",
    "        elif result_bear.get('is_engulfing_reversal', False):\n",
    "            return True, result_bear\n",
    "        elif detected_bull:\n",
    "            return True, result_bull\n",
    "        else:\n",
    "            return True, result_bear\n",
    "    return False, {}\n",
    "\n",
    "def detect_hbs_breakout(df, check_bar: int = -1):\n",
    "    \"\"\"\n",
    "    HBS breakout fires when:\n",
    "      • (Bullish confluence) AND (consolidation_breakout OR channel_breakout)\n",
    "      OR\n",
    "      • Bullish engulfing-reversal (regardless of consolidation/channel)\n",
    "\n",
    "    Returns (detected: bool, result: dict) compatible with the backtester.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abs_idx = check_bar if check_bar >= 0 else (len(df) + check_bar)\n",
    "        if abs_idx < 0 or abs_idx >= len(df):\n",
    "            return False, {}\n",
    "\n",
    "        # Components\n",
    "        cb_detected, cb_result = detect_consolidation_breakout(df, check_bar=check_bar)\n",
    "\n",
    "        chb_detected, chb_result = False, {}\n",
    "        if len(df) > 25:\n",
    "            chb_detected, chb_result = detect_channel_breakout(df, check_bar=check_bar)\n",
    "\n",
    "        # **Bullish** confluence only (we do NOT check bearish here)\n",
    "        cf_detected, cf_result = detect_confluence(df, check_bar=check_bar, is_bullish=True)\n",
    "        bullish_reversal = bool(cf_result.get(\"is_engulfing_reversal\", False))\n",
    "\n",
    "        # Core HBS\n",
    "        hbs_core = cf_detected and (cb_detected or chb_detected)\n",
    "\n",
    "        if hbs_core or bullish_reversal:\n",
    "            # Choose the breakout \"shape\" to report\n",
    "            if hbs_core:\n",
    "                if cb_detected and chb_detected:\n",
    "                    breakout_result = chb_result\n",
    "                    breakout_type = \"both\"\n",
    "                elif chb_detected:\n",
    "                    breakout_result = chb_result\n",
    "                    breakout_type = \"channel_breakout\"\n",
    "                else:\n",
    "                    breakout_result = cb_result\n",
    "                    breakout_type = \"consolidation_breakout\"\n",
    "            else:\n",
    "                # Reversal-only branch\n",
    "                breakout_result = {\"timestamp\": df.index[abs_idx]}\n",
    "                breakout_type = \"bullish_engulfing_reversal_only\"\n",
    "\n",
    "            # Build final result (keep confluence components for the table)\n",
    "            return True, {\n",
    "                \"direction\": cf_result.get(\"direction\", \"Up\"),  # may be \"Up Reversal\"\n",
    "                \"timestamp\": breakout_result.get(\"timestamp\", df.index[abs_idx]),\n",
    "                \"breakout_type\": breakout_type,\n",
    "                \"height_pct\": breakout_result.get(\"height_pct\", np.nan),\n",
    "                \"channel_direction\": breakout_result.get(\"channel_direction\", \"\"),\n",
    "                \"bars_inside\": breakout_result.get(\"bars_inside\"),\n",
    "\n",
    "                \"confluence_fired\": bool(cf_detected),\n",
    "                \"consolidation_fired\": bool(cb_detected),\n",
    "                \"channel_fired\": bool(chb_detected),\n",
    "\n",
    "                \"spread_breakout\": bool(cf_result.get(\"spread_breakout\", False)),\n",
    "                \"high_volume\": bool(cf_result.get(\"high_volume\", False)),\n",
    "                \"momentum_breakout\": bool(cf_result.get(\"momentum_breakout\", False)),\n",
    "                \"is_engulfing_reversal\": bool(bullish_reversal),\n",
    "            }\n",
    "\n",
    "        # Nothing to report\n",
    "        return False, {}\n",
    "\n",
    "    except Exception:\n",
    "        return False, {}\n",
    "\n",
    "\n",
    "def find_pivot_high(df, start_idx, lookback=3, lookahead=3):\n",
    "    \"\"\"Find next pivot high after start_idx\"\"\"\n",
    "    for i in range(start_idx + lookahead, len(df) - lookahead):\n",
    "        current_high = df['high'].iloc[i]\n",
    "        \n",
    "        # Check if this is a pivot high\n",
    "        is_pivot = True\n",
    "        for j in range(i - lookback, i + lookahead + 1):\n",
    "            if j != i and df['high'].iloc[j] >= current_high:\n",
    "                is_pivot = False\n",
    "                break\n",
    "        \n",
    "        if is_pivot:\n",
    "            return i, current_high\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def find_pivot_low(df, start_idx, lookback=3, lookahead=3):\n",
    "    \"\"\"Find next pivot low after start_idx\"\"\"\n",
    "    for i in range(start_idx + lookahead, len(df) - lookahead):\n",
    "        current_low = df['low'].iloc[i]\n",
    "        \n",
    "        # Check if this is a pivot low\n",
    "        is_pivot = True\n",
    "        for j in range(i - lookback, i + lookahead + 1):\n",
    "            if j != i and df['low'].iloc[j] <= current_low:\n",
    "                is_pivot = False\n",
    "                break\n",
    "        \n",
    "        if is_pivot:\n",
    "            return i, current_low\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def calculate_trade_returns(df, breakout_idx, entry_price, direction):\n",
    "    \"\"\"\n",
    "    Exit rules:\n",
    "      • Stop loss: close crosses the breakout bar's opposite extreme\n",
    "          - Long: close < breakout_low\n",
    "          - Short: close > breakout_high\n",
    "      • Take profit: first pivot in the direction of the trade\n",
    "          - Long: next pivot HIGH\n",
    "          - Short: next pivot LOW\n",
    "    MFE/MAE measured with intrabar extremes up to the exit bar (inclusive).\n",
    "    \"\"\"\n",
    "    # Need future bars to evaluate exit\n",
    "    if breakout_idx >= len(df) - 2:\n",
    "        return None\n",
    "\n",
    "    # Breakout bar extremes\n",
    "    breakout_low  = df['low'].iloc[breakout_idx]\n",
    "    breakout_high = df['high'].iloc[breakout_idx]\n",
    "\n",
    "    # Choose pivot target\n",
    "    if direction == 'Up':\n",
    "        pivot_idx, pivot_price = find_pivot_high(df, breakout_idx)\n",
    "        stop_threshold = breakout_low\n",
    "    else:  # 'Down'\n",
    "        pivot_idx, pivot_price = find_pivot_low(df, breakout_idx)\n",
    "        stop_threshold = breakout_high\n",
    "\n",
    "    # If no pivot is found, fall back to last bar as target (still respect stop)\n",
    "    if pivot_idx is None:\n",
    "        pivot_idx = len(df) - 1\n",
    "        pivot_price = df['close'].iloc[pivot_idx]\n",
    "        exit_reason_if_pivot = \"end_of_data\"\n",
    "    else:\n",
    "        exit_reason_if_pivot = \"pivot_take_profit\"\n",
    "\n",
    "    max_return = 0.0   # MFE\n",
    "    min_return = 0.0   # MAE\n",
    "    stop_hit   = False\n",
    "    exit_idx   = None\n",
    "    exit_price = None\n",
    "    exit_reason = None\n",
    "\n",
    "    # Scan bars after breakout until the pivot (or until a stop triggers earlier)\n",
    "    last_scan_idx = min(pivot_idx, len(df) - 1)\n",
    "    for i in range(breakout_idx + 1, last_scan_idx + 1):\n",
    "        close_i = df['close'].iloc[i]\n",
    "        high_i  = df['high'].iloc[i]\n",
    "        low_i   = df['low'].iloc[i]\n",
    "\n",
    "        if direction == 'Up':\n",
    "            # Intrabar excursions relative to entry\n",
    "            high_ret = (high_i  - entry_price) / entry_price * 100.0\n",
    "            low_ret  = (low_i   - entry_price) / entry_price * 100.0\n",
    "            # Update MFE/MAE first (so the stop bar's extremes are included)\n",
    "            max_return = max(max_return, high_ret)\n",
    "            min_return = min(min_return, low_ret)\n",
    "\n",
    "            # Stop on close below breakout_low\n",
    "            if close_i < stop_threshold:\n",
    "                stop_hit   = True\n",
    "                exit_idx   = i\n",
    "                exit_price = close_i\n",
    "                exit_reason = \"stop_loss\"\n",
    "                break\n",
    "        else:\n",
    "            # Short: gains when price goes down\n",
    "            high_ret = (entry_price - low_i ) / entry_price * 100.0  # favorable excursion\n",
    "            low_ret  = (entry_price - high_i) / entry_price * 100.0  # adverse excursion\n",
    "            max_return = max(max_return, high_ret)\n",
    "            min_return = min(min_return, low_ret)\n",
    "\n",
    "            # Stop on close above breakout_high\n",
    "            if close_i > stop_threshold:\n",
    "                stop_hit   = True\n",
    "                exit_idx   = i\n",
    "                exit_price = close_i\n",
    "                exit_reason = \"stop_loss\"\n",
    "                break\n",
    "\n",
    "    # If no stop, exit at pivot target\n",
    "    if not stop_hit:\n",
    "        exit_idx   = pivot_idx\n",
    "        exit_price = pivot_price\n",
    "        exit_reason = exit_reason_if_pivot\n",
    "\n",
    "        # Ensure MFE/MAE include the pivot bar if loop ended before including it\n",
    "        # (loop already included pivot bar because range is ... +1, but guard anyway)\n",
    "        high_i = df['high'].iloc[exit_idx]\n",
    "        low_i  = df['low'].iloc[exit_idx]\n",
    "        if direction == 'Up':\n",
    "            max_return = max(max_return, (high_i - entry_price) / entry_price * 100.0)\n",
    "            min_return = min(min_return, (low_i  - entry_price) / entry_price * 100.0)\n",
    "        else:\n",
    "            max_return = max(max_return, (entry_price - low_i ) / entry_price * 100.0)\n",
    "            min_return = min(min_return, (entry_price - high_i) / entry_price * 100.0)\n",
    "\n",
    "    # Final realized return\n",
    "    if direction == 'Up':\n",
    "        final_return = (exit_price - entry_price) / entry_price * 100.0\n",
    "    else:\n",
    "        final_return = (entry_price - exit_price) / entry_price * 100.0\n",
    "\n",
    "    holding_days = (df.index[exit_idx] - df.index[breakout_idx]).days\n",
    "\n",
    "    return {\n",
    "        'entry_price': entry_price,\n",
    "        'exit_price': exit_price,\n",
    "        'final_return_pct': final_return,\n",
    "        'max_favorable_excursion': max_return,\n",
    "        'max_adverse_excursion': min_return,\n",
    "        'holding_days': holding_days,\n",
    "        'exit_reason': exit_reason,\n",
    "        'stop_hit': stop_hit,\n",
    "        'trade_direction': direction,\n",
    "        'breakout_date': df.index[breakout_idx],\n",
    "        'exit_date': df.index[exit_idx],\n",
    "    }\n",
    "\n",
    "def get_close_position_indicator(high, low, close):\n",
    "    \"\"\"Generate close position indicator with 3-dot system\"\"\"\n",
    "    bar_range = high - low\n",
    "    if bar_range <= 0:\n",
    "        return \"○●○\", 50.0\n",
    "    \n",
    "    close_position_pct = ((close - low) / bar_range) * 100\n",
    "    \n",
    "    if close_position_pct <= 30:\n",
    "        indicator = \"●○○\"  # 0-30%\n",
    "    elif close_position_pct <= 70:\n",
    "        indicator = \"○●○\"  # 30-70%\n",
    "    else:\n",
    "        indicator = \"○○●\"  # 70-100%\n",
    "    \n",
    "    return indicator, close_position_pct\n",
    "\n",
    "def calculate_50sma_breakout_from_sf(df, bar_idx):\n",
    "    \"\"\"Check if bar is a 50SMA breakout using SF's sma_50 column\"\"\"\n",
    "    if bar_idx < 0 or bar_idx >= len(df):\n",
    "        return False, None, None\n",
    "    \n",
    "    # Get the original dataframe with SF columns to access sma_50\n",
    "    if 'sma_50' not in df.columns:\n",
    "        return False, None, None\n",
    "    \n",
    "    close = df['close'].iloc[bar_idx]\n",
    "    low = df['low'].iloc[bar_idx]\n",
    "    sma50_value = df['sma_50'].iloc[bar_idx]\n",
    "    \n",
    "    # Skip if SMA is NaN\n",
    "    if pd.isna(sma50_value):\n",
    "        return False, None, None\n",
    "    \n",
    "    # Basic breakout: close > SMA and low < SMA\n",
    "    is_breakout = close > sma50_value and low < sma50_value\n",
    "    \n",
    "    return is_breakout, sma50_value, close\n",
    "\n",
    "def backtest_breakout_strategy(strategy_name, exchange=\"Kucoin\", limit=500):\n",
    "    \"\"\"\n",
    "    Backtest a breakout strategy on all USDT pairs from SF server\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize SF service\n",
    "    sf_service = SFPairsService()\n",
    "    \n",
    "    print(f\"Fetching all USDT pairs from SF {exchange}...\")\n",
    "    \n",
    "    # Get all pairs from SF service\n",
    "    all_pairs_data = sf_service.get_pairs_of_exchange(exchange)\n",
    "    \n",
    "    # Filter for USDT pairs\n",
    "    usdt_pairs = [\n",
    "        f\"{pair['Token']}/USDT\" \n",
    "        for pair in all_pairs_data \n",
    "        if 'Quote' in pair and pair['Quote'].upper() == \"USDT\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(usdt_pairs)} USDT pairs on {exchange}\")\n",
    "    \n",
    "    # Strategy detector mapping\n",
    "    strategy_detectors = {\n",
    "        \"wedge_breakout\": detect_wedge_breakout,\n",
    "        \"channel_breakout\": detect_channel_breakout, \n",
    "        \"consolidation_breakout\": detect_consolidation_breakout,\n",
    "        \"hbs_breakout\": detect_hbs_breakout,\n",
    "        \"confluence\": detect_confluence_wrapper\n",
    "    }\n",
    "    \n",
    "    if strategy_name not in strategy_detectors:\n",
    "        raise ValueError(f\"Strategy {strategy_name} not supported. Use: {list(strategy_detectors.keys())}\")\n",
    "    \n",
    "    detector = strategy_detectors[strategy_name]\n",
    "    \n",
    "    # Results storage\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Starting {strategy_name} backtest on {len(usdt_pairs)} pairs...\")\n",
    "    \n",
    "    for i, pair in enumerate(usdt_pairs):\n",
    "        try:\n",
    "            symbol, quote = pair.split('/')\n",
    "            print(f\"Processing {pair} ({i+1}/{len(usdt_pairs)})...\")\n",
    "            \n",
    "            # Fetch historical data\n",
    "            raw_data = sf_service.get_ohlcv_for_pair(symbol, quote, exchange, \"1w\", limit)\n",
    "            \n",
    "            if raw_data is None or (hasattr(raw_data, '__len__') and len(raw_data) < 25):\n",
    "                print(f\"Insufficient data for {pair}\")\n",
    "                continue\n",
    "                \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(raw_data)\n",
    "            \n",
    "            # Fix datetime index first\n",
    "            if 'timestamp' in df.columns:\n",
    "                df.index = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "            elif 'time' in df.columns:\n",
    "                df.index = pd.to_datetime(df['time'], unit='ms')\n",
    "            elif 'datetime' in df.columns:\n",
    "                df.index = pd.to_datetime(df['datetime'])\n",
    "            else:\n",
    "                df.index = pd.date_range(start='2020-01-01', periods=len(df), freq='W')\n",
    "            \n",
    "            # Keep full dataframe initially (for SMA columns)\n",
    "            full_df = df.copy()\n",
    "            \n",
    "            # Select only OHLCV columns for strategy detection\n",
    "            required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"  Missing required columns: {missing_cols}\")\n",
    "                continue\n",
    "            \n",
    "            # Create clean OHLCV dataframe for strategy detection\n",
    "            clean_df = df[required_cols].copy()\n",
    "            \n",
    "            # Convert to numeric\n",
    "            for col in required_cols:\n",
    "                clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')\n",
    "            \n",
    "            # Drop NaN rows (only based on OHLCV columns)\n",
    "            clean_df = clean_df.dropna()\n",
    "            \n",
    "            # Sort by date\n",
    "            clean_df = clean_df.sort_index()\n",
    "            \n",
    "            if len(clean_df) < 25:\n",
    "                print(f\"  Insufficient data for {pair}\")\n",
    "                continue\n",
    "            \n",
    "            # Test each bar for breakouts (skip last few bars due to incomplete data)\n",
    "            bars_tested = 0\n",
    "            for bar_idx in range(25, len(clean_df) - 2):\n",
    "                bars_tested += 1\n",
    "                \n",
    "                # Convert absolute index to relative index for detector\n",
    "                relative_idx = bar_idx - len(clean_df)\n",
    "                \n",
    "                try:\n",
    "                    # Detect breakout using relative index on clean OHLCV data\n",
    "                    detected, result = detector(clean_df, check_bar=relative_idx)\n",
    "                    \n",
    "                    if detected:\n",
    "                        print(f\"  BREAKOUT FOUND: {pair} on {clean_df.index[bar_idx].strftime('%Y-%m-%d')} - {result.get('direction', 'Unknown')}\")\n",
    "                        \n",
    "                        # Get bar data from clean dataframe\n",
    "                        bar_date = clean_df.index[bar_idx]\n",
    "                        close = clean_df['close'].iloc[bar_idx]\n",
    "                        volume = clean_df['volume'].iloc[bar_idx]\n",
    "                        \n",
    "                        # Calculate close position\n",
    "                        close_indicator, close_pos_pct = get_close_position_indicator(\n",
    "                            clean_df['high'].iloc[bar_idx], \n",
    "                            clean_df['low'].iloc[bar_idx], \n",
    "                            close\n",
    "                        )\n",
    "                        \n",
    "                        # Check 50SMA breakout using SF's sma_50 column from full dataframe\n",
    "                        try:\n",
    "                            matching_row = full_df[full_df.index == bar_date]\n",
    "                            if len(matching_row) > 0:\n",
    "                                sma50_breakout, sma50_value, _ = calculate_50sma_breakout_from_sf(matching_row.iloc[0], 0)\n",
    "                            else:\n",
    "                                sma50_breakout, sma50_value = False, None\n",
    "                        except:\n",
    "                            sma50_breakout, sma50_value = False, None\n",
    "                        \n",
    "                        # Calculate trade returns if enough future data exists\n",
    "                        trade_returns = None\n",
    "                        if bar_idx < len(clean_df) - 10:\n",
    "                            # Normalize to Up/Down for PnL calc\n",
    "                            base_dir = 'Up' if 'Up' in result.get('direction', 'Up') else 'Down'\n",
    "                            trade_returns = calculate_trade_returns(clean_df, bar_idx, close, base_dir)\n",
    "\n",
    "                        \n",
    "                        # Store result with minimal columns\n",
    "                        breakout_result = {\n",
    "                            'pair': pair,\n",
    "                            'exchange': exchange,\n",
    "                            'date': bar_date,\n",
    "                            'strategy': strategy_name,\n",
    "                            'direction': result.get('direction', 'Unknown'),\n",
    "                            'close': close,\n",
    "                            'volume_usd': volume * close,\n",
    "                            'close_position_indicator': close_indicator,\n",
    "                            'close_position_pct': close_pos_pct,\n",
    "                            'sma50_breakout': sma50_breakout,\n",
    "                            'height_pct': result.get('height_pct', np.nan),\n",
    "                            'channel_direction': result.get('channel_direction', ''),\n",
    "                            # Extract confluence components\n",
    "                            'spread_breakout': result.get('spread_breakout', False),\n",
    "                            'high_volume': result.get('high_volume', False),\n",
    "                            'momentum_breakout': result.get('momentum_breakout', False),\n",
    "                            'is_engulfing_reversal': result.get('is_engulfing_reversal', False),\n",
    "                        }\n",
    "                        \n",
    "                        # Add trade return metrics if available\n",
    "                        if trade_returns:\n",
    "                            breakout_result.update({\n",
    "                                'entry_price': trade_returns['entry_price'],\n",
    "                                'exit_price': trade_returns['exit_price'],\n",
    "                                'return_pct': trade_returns['final_return_pct'],\n",
    "                                'stop_hit':    trade_returns['stop_hit'],\n",
    "                                'max_gain_pct': trade_returns['max_favorable_excursion'],\n",
    "                                'max_loss_pct': trade_returns['max_adverse_excursion'],\n",
    "                                'holding_days': trade_returns['holding_days'],\n",
    "                                'exit_reason': trade_returns['exit_reason'],\n",
    "                            })\n",
    "                        \n",
    "                        # Add HBS-specific info if it's an HBS breakout\n",
    "                        if strategy_name == 'hbs_breakout':\n",
    "                            breakout_result.update({\n",
    "                                'breakout_type': result.get('breakout_type', ''),\n",
    "                                'confluence_fired': result.get('confluence_fired', False),\n",
    "                                'consolidation_fired': result.get('consolidation_fired', False),\n",
    "                                'channel_fired': result.get('channel_fired', False),\n",
    "                            })\n",
    "                        \n",
    "                        all_results.append(breakout_result)\n",
    "                        \n",
    "                except Exception as detector_error:\n",
    "                    print(f\"  Error in detector at bar {bar_idx}: {str(detector_error)}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pair}: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(usdt_pairs)} pairs, found {len(all_results)} breakouts\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    if len(results_df) == 0:\n",
    "        print(\"No breakouts detected\")\n",
    "        return results_df\n",
    "    \n",
    "    # Sort by pair and date\n",
    "    results_df = results_df.sort_values(['pair', 'date'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def save_backtest_results(results_df, strategy_name, exchange):\n",
    "    \"\"\"Save backtest results to CSV with readable formatting\"\"\"\n",
    "    \n",
    "    if len(results_df) == 0:\n",
    "        print(\"No results to save\")\n",
    "        return\n",
    "    \n",
    "    # Create filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{strategy_name}_{exchange}_backtest_{timestamp}.csv\"\n",
    "    \n",
    "    # Save full results\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Full results saved to: {filename}\")\n",
    "    \n",
    "    # Create summary by pair\n",
    "    summary_data = []\n",
    "    \n",
    "    for pair in results_df['pair'].unique():\n",
    "        pair_data = results_df[results_df['pair'] == pair].copy()\n",
    "        \n",
    "        # Count breakouts\n",
    "        total_breakouts = len(pair_data)\n",
    "        up_breakouts = len(pair_data[pair_data['direction'] == 'Up'])\n",
    "        down_breakouts = len(pair_data[pair_data['direction'] == 'Down'])\n",
    "        sma50_breakouts = len(pair_data[pair_data['sma50_breakout'] == True])\n",
    "        \n",
    "        # Close position distribution\n",
    "        lower_third = len(pair_data[pair_data['close_position_pct'] <= 30])\n",
    "        middle_third = len(pair_data[(pair_data['close_position_pct'] > 30) & (pair_data['close_position_pct'] <= 70)])\n",
    "        upper_third = len(pair_data[pair_data['close_position_pct'] > 70])\n",
    "        \n",
    "        # Confluence components counts\n",
    "        spread_breakouts = len(pair_data[pair_data['spread_breakout'] == True])\n",
    "        high_volumes = len(pair_data[pair_data['high_volume'] == True])\n",
    "        momentum_breakouts = len(pair_data[pair_data['momentum_breakout'] == True])\n",
    "        engulfing_reversals = len(pair_data[pair_data['is_engulfing_reversal'] == True])\n",
    "        \n",
    "        summary_data.append({\n",
    "            'pair': pair,\n",
    "            'total_breakouts': total_breakouts,\n",
    "            'up_breakouts': up_breakouts, \n",
    "            'down_breakouts': down_breakouts,\n",
    "            'sma50_also_breakout': sma50_breakouts,\n",
    "            'sma50_overlap_pct': round(sma50_breakouts/total_breakouts*100, 1) if total_breakouts > 0 else 0,\n",
    "            'close_lower_third': lower_third,\n",
    "            'close_middle_third': middle_third,\n",
    "            'close_upper_third': upper_third,\n",
    "            'spread_breakouts': spread_breakouts,\n",
    "            'high_volumes': high_volumes,\n",
    "            'momentum_breakouts': momentum_breakouts,\n",
    "            'engulfing_reversals': engulfing_reversals,\n",
    "            'first_breakout': pair_data['date'].min().strftime('%Y-%m-%d'),\n",
    "            'last_breakout': pair_data['date'].max().strftime('%Y-%m-%d')\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_filename = f\"{strategy_name}_{exchange}_summary_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_filename, index=False)\n",
    "    print(f\"Summary saved to: {summary_filename}\")\n",
    "    \n",
    "    # Print readable summary\n",
    "    print(f\"\\n=== BACKTEST RESULTS SUMMARY ===\")\n",
    "    print(f\"Strategy: {strategy_name}\")\n",
    "    print(f\"Exchange: {exchange}\")\n",
    "    print(f\"Total pairs with breakouts: {len(summary_df)}\")\n",
    "    print(f\"Total breakouts detected: {len(results_df)}\")\n",
    "    print(f\"Breakouts also 50SMA breakout: {results_df['sma50_breakout'].sum()}\")\n",
    "    print(f\"50SMA overlap rate: {results_df['sma50_breakout'].sum()/len(results_df)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nDirection Distribution:\")\n",
    "    print(f\"Up breakouts: {(results_df['direction'] == 'Up').sum()}\")\n",
    "    print(f\"Down breakouts: {(results_df['direction'] == 'Down').sum()}\")\n",
    "    \n",
    "    print(f\"\\nClose Position Distribution:\")\n",
    "    print(f\"Lower third (●○○): {(results_df['close_position_pct'] <= 30).sum()}\")\n",
    "    print(f\"Middle third (○●○): {((results_df['close_position_pct'] > 30) & (results_df['close_position_pct'] <= 70)).sum()}\")\n",
    "    print(f\"Upper third (○○●): {(results_df['close_position_pct'] > 70).sum()}\")\n",
    "    \n",
    "    # Confluence components summary\n",
    "    print(f\"\\nConfluence Components Distribution:\")\n",
    "    print(f\"Spread Breakouts: {results_df['spread_breakout'].sum()}\")\n",
    "    print(f\"High Volume: {results_df['high_volume'].sum()}\")\n",
    "    print(f\"Momentum Breakouts: {results_df['momentum_breakout'].sum()}\")\n",
    "    print(f\"Engulfing Reversals: {results_df['is_engulfing_reversal'].sum()}\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(f\"\\n=== SAMPLE BREAKOUTS ===\")\n",
    "    sample_cols = ['pair', 'date', 'direction', 'close', 'close_position_indicator', \n",
    "                   'close_position_pct', 'sma50_breakout', 'volume_usd',\n",
    "                   'spread_breakout', 'high_volume', 'momentum_breakout', 'is_engulfing_reversal']\n",
    "    sample_df = results_df.head(10)[sample_cols]\n",
    "    print(sample_df.to_string(index=False))\n",
    "    \n",
    "    return filename, summary_filename\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose strategy to backtest\n",
    "    strategy = \"hbs_breakout\"  # or \"hbs_breakout\", \"channel_breakout\", \"consolidation_breakout\"\n",
    "    exchange = \"Kucoin\"          # or \"Mexc\" \n",
    "    \n",
    "    print(f\"Running {strategy} backtest on {exchange}...\")\n",
    "    \n",
    "    # Run backtest\n",
    "    results = backtest_breakout_strategy(strategy, exchange, limit=300)\n",
    "    \n",
    "    # Save results\n",
    "    if len(results) > 0:\n",
    "        save_backtest_results(results, strategy, exchange)\n",
    "    else:\n",
    "        print(\"No breakouts found in the tested data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06443f-8f2c-4088-a5e0-c2103e3dd0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
