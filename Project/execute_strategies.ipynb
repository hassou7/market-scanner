{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab56392-9bd5-48da-aef6-5dddab880329",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f094aaab-b05b-4019-b8f9-592a5076f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  RUNNING PARALLEL MULTI-TIMEFRAME SCAN ON ALL EXCHANGES\n",
      "================================================================================\n",
      "\n",
      "• Exchanges: binance_futures, binance_spot, bybit_spot, gateio_spot\n",
      "• Timeframes: 4h\n",
      "• Strategies: vs_wakeup\n",
      "• Notifications: Enabled\n",
      "• Recipients: default\n",
      "• Save to CSV: Disabled\n",
      "• Start time: 07:41:58\n",
      "\n",
      "Fetching market data...\n",
      "\n",
      "Processing timeframe: 4h\n",
      "4h: 4 FAST, 0 SLOW exchanges\n",
      "\n",
      "================================================================================\n",
      "  PHASE: FAST 4h (4 exchanges)\n",
      "================================================================================\n",
      "\n",
      "[07:41:58] Starting scan on binance_futures for 4h timeframe...\n",
      "[07:41:58] Starting scan on gateio_spot for 4h timeframe...\n",
      "[07:41:58] Starting scan on binance_spot for 4h timeframe...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added /home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project/Project to sys.path\n",
      "✓ Added /home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07:41:58] Starting scan on bybit_spot for 4h timeframe...\n",
      "Found 493 markets on Binance Futures for 4h timeframe\n",
      "Processing 493 symbols with parallel strategies (batch size: 25)\n",
      "Found 509 markets on Bybit for 4h timeframe\n",
      "Processing 509 symbols with parallel strategies (batch size: 25)\n",
      "Found 418 markets on Binance Spot for 4h timeframe\n",
      "Processing 418 symbols with parallel strategies (batch size: 25)\n",
      "Found 2198 markets on Gateio for 4h timeframe\n",
      "Processing 2198 symbols with parallel strategies (batch size: 25)\n",
      "[07:42:33] ✓ Completed binance_spot scan: 0 signals found\n",
      "[07:42:36] ✓ Completed bybit_spot scan: 0 signals found\n",
      "[07:42:37] ✓ Completed binance_futures scan: 0 signals found\n",
      "[07:44:15] ✓ Completed gateio_spot scan: 0 signals found\n",
      "\n",
      "================================================================================\n",
      "  PHASE: SLOW 4h (0 exchanges)\n",
      "================================================================================\n",
      "\n",
      "No exchanges in this phase.\n",
      "\n",
      "================================================================================\n",
      "  PARALLEL MULTI-TIMEFRAME MULTI-EXCHANGE SCAN RESULTS\n",
      "================================================================================\n",
      "\n",
      "Total signals found across all exchanges and timeframes: 0\n",
      "Start time: 07:41:58\n",
      "End time: 07:44:15\n",
      "Duration: 0:02:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run Simple Parallel Scan for Test Bar\n",
    "\n",
    "This script runs the test bar scan across multiple exchanges in parallel\n",
    "using a simplified parallel scanning approach that avoids console output issues.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Add project directory to path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(os.getcwd())\n",
    "print(f\"✓ Added {os.getcwd()} to sys.path\")\n",
    "\n",
    "# Import the simple parallel scanner\n",
    "from run_parallel_scanner import run_parallel_exchanges, run_parallel_multi_timeframes_all_exchanges, sf_exchanges_1w\n",
    "from scanner.main import kline_cache\n",
    "\n",
    "# Define exchanges\n",
    "futures_exchanges = [\"binance_futures\", \"bybit_futures\", \"mexc_futures\", \"gateio_futures\"]\n",
    "spot_exchanges = [\"binance_spot\", \"bybit_spot\", \"kucoin_spot\", \"mexc_spot\", \"gateio_spot\"]\n",
    "spot_exchanges_1w = [\"binance_spot\", \"bybit_spot\", \"gateio_spot\"] + sf_exchanges_1w\n",
    "fast_exchanges = [\"binance_futures\", \"binance_spot\", \"bybit_spot\", \"gateio_spot\"]\n",
    "slow_exchanges = [\"kucoin_spot\", \"mexc_spot\", \"mexc_futures\"]\n",
    "\n",
    "async def main():\n",
    "    # Clear cache for fresh data\n",
    "    kline_cache.clear()\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Run parallel scan for test bar strategy on spot exchanges\n",
    "    result = await run_parallel_exchanges(\n",
    "        timeframe=\"4h\",                    # Example timeframe\n",
    "        strategies=[\"hbs_breakout\", \"test_bar\", \"consolidation_breakout\", \"sma50_breakout\", \"trend_breakout\", \"pin_up\"],\n",
    "        # strategies=[\"reversal_bar\"],       \n",
    "        exchanges=spot_exchanges,          # Spot exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for Telegram notifications\n",
    "        send_telegram=True,                # Enable Telegram notifications\n",
    "        min_volume_usd=None,               # Use default volume threshold\n",
    "        save_to_csv=True                   # Enable saving to CSV\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Run multi-timeframe parallel scan\n",
    "    result = await run_parallel_multi_timeframes_all_exchanges(\n",
    "        timeframes=[\"4h\"],     # Multiple timeframes0\n",
    "        strategies= [\"vs_wakeup\"], #[\"confluence\", \"consolidation_breakout\", \"channel_breakout\", \"sma50_breakout\", \"loaded_bar\", \"pin_up\", \"trend_breakout\"], #\"engulfingReversal\", \"vs_wakeup],        # Strategies to scan\n",
    "        exchanges= fast_exchanges,          # Exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for notifications\n",
    "        send_telegram=True,                # Enable notifications\n",
    "        min_volume_usd=None,               # Use default volume threshold\n",
    "        save_to_csv=False                   # Enable saving to CSV\n",
    "    )\n",
    "     #\"\"\"\n",
    "    \n",
    "    print(\"Scan completed!\")\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c86d73-1174-45d1-b215-842f2609b235",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Verify OHLCV data from SF\n",
    "\n",
    "from exchanges.sf_pairs_service import SFPairsService\n",
    "import pandas as pd\n",
    "\n",
    "sf_service = SFPairsService()\n",
    "\n",
    "symbol = \"BTC\"\n",
    "quote = \"USDT\"\n",
    "exchange = \"Kucoin\"   # or \"Mexc\"\n",
    "timeframe = \"1w\"      # weekly\n",
    "limit = 2             # how many candles to fetch\n",
    "\n",
    "# Fetch OHLCV data\n",
    "raw_data = sf_service.get_ohlcv_for_pair(symbol, quote, exchange, timeframe, limit)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "# Normalize datetime\n",
    "if 'datetime' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.set_index('datetime')\n",
    "elif 'time' in df.columns:\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    df = df.set_index('time')\n",
    "\n",
    "# Keep only required columns\n",
    "df = df[['open', 'high', 'low', 'close', 'volume']].astype(float)\n",
    "\n",
    "# Print with timeframe in console\n",
    "print(f\"\\n📊 {symbol}/{quote} OHLCV ({timeframe}) on {exchange}\")\n",
    "print(df.tail(1))  # last row = current bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac56a-df34-4974-8cd6-8fb33f8d477a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Binance BTC dominated pairs - Confluence Scanner with Direct API\n",
    "\n",
    "from telegram.ext import Application\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import os\n",
    "import html\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# Add project path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from custom_strategies import detect_confluence\n",
    "\n",
    "class BinanceBTCConfluenceScanner:\n",
    "    def __init__(self, telegram_token, telegram_chat_id, timeframe, offset=1):\n",
    "        self.telegram_token = telegram_token\n",
    "        self.telegram_chat_id = telegram_chat_id\n",
    "        self.telegram_app = None\n",
    "        self.exchange = \"Binance\"\n",
    "        self.timeframe = timeframe\n",
    "        self.quote_currency = \"BTC\"\n",
    "        self.offset = offset\n",
    "        self.base_url = \"https://api.binance.com\"\n",
    "        self.session = None\n",
    "        \n",
    "        # Binance timeframe mapping - All available intervals\n",
    "        self.timeframe_map = {\n",
    "            # Minutes\n",
    "            \"1m\": \"1m\",\n",
    "            \"3m\": \"3m\", \n",
    "            \"5m\": \"5m\",\n",
    "            \"15m\": \"15m\",\n",
    "            \"30m\": \"30m\",\n",
    "            # Hours\n",
    "            \"1h\": \"1h\",\n",
    "            \"2h\": \"2h\",\n",
    "            \"4h\": \"4h\",\n",
    "            \"6h\": \"6h\",\n",
    "            \"8h\": \"8h\",\n",
    "            \"12h\": \"12h\",\n",
    "            # Days\n",
    "            \"1d\": \"1d\",\n",
    "            \"2d\": \"2d\",\n",
    "            \"3d\": \"3d\",\n",
    "            # Weeks/Months\n",
    "            \"1w\": \"1w\",\n",
    "            \"1M\": \"1M\"\n",
    "        }\n",
    "        \n",
    "    async def init_session(self):\n",
    "        \"\"\"Initialize aiohttp session\"\"\"\n",
    "        if self.session is None:\n",
    "            self.session = aiohttp.ClientSession()\n",
    "            \n",
    "    async def close_session(self):\n",
    "        \"\"\"Close aiohttp session\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "            self.session = None\n",
    "        \n",
    "    async def init_telegram(self):\n",
    "        if self.telegram_app is None:\n",
    "            self.telegram_app = Application.builder().token(self.telegram_token).build()\n",
    "\n",
    "    async def get_btc_pairs(self):\n",
    "        \"\"\"Get all BTC trading pairs from Binance\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.base_url}/api/v3/exchangeInfo\"\n",
    "            async with self.session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    \n",
    "                    # Filter for BTC pairs that are actively trading\n",
    "                    btc_pairs = []\n",
    "                    for symbol_info in data['symbols']:\n",
    "                        if (symbol_info['quoteAsset'] == 'BTC' and \n",
    "                            symbol_info['status'] == 'TRADING' and\n",
    "                            symbol_info['isSpotTradingAllowed']):\n",
    "                            \n",
    "                            btc_pairs.append({\n",
    "                                'symbol': symbol_info['symbol'],\n",
    "                                'baseAsset': symbol_info['baseAsset'],\n",
    "                                'quoteAsset': symbol_info['quoteAsset']\n",
    "                            })\n",
    "                    \n",
    "                    return btc_pairs\n",
    "                else:\n",
    "                    logging.error(f\"Error fetching exchange info: {response.status}\")\n",
    "                    return []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching BTC pairs: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    async def get_klines(self, symbol, interval, limit=100):\n",
    "        \"\"\"Get kline/candlestick data from Binance\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.base_url}/api/v3/klines\"\n",
    "            params = {\n",
    "                'symbol': symbol,\n",
    "                'interval': interval,\n",
    "                'limit': limit\n",
    "            }\n",
    "            \n",
    "            async with self.session.get(url, params=params) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    \n",
    "                    # Convert to DataFrame\n",
    "                    df = pd.DataFrame(data, columns=[\n",
    "                        'open_time', 'open', 'high', 'low', 'close', 'volume',\n",
    "                        'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "                        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "                    ])\n",
    "                    \n",
    "                    # Convert timestamps to datetime\n",
    "                    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "                    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')\n",
    "                    \n",
    "                    # Convert OHLCV to numeric\n",
    "                    for col in ['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    \n",
    "                    # Set datetime index\n",
    "                    df.set_index('open_time', inplace=True)\n",
    "                    \n",
    "                    # Select only OHLCV columns needed for confluence\n",
    "                    # Note: Using quote_asset_volume as it's the volume in BTC\n",
    "                    result_df = df[['open', 'high', 'low', 'close', 'quote_asset_volume']].copy()\n",
    "                    result_df.rename(columns={'quote_asset_volume': 'volume'}, inplace=True)\n",
    "                    \n",
    "                    return result_df\n",
    "                    \n",
    "                elif response.status == 429:\n",
    "                    # Rate limit hit, wait a bit\n",
    "                    await asyncio.sleep(1)\n",
    "                    return None\n",
    "                else:\n",
    "                    return None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching klines for {symbol}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    async def send_telegram_alert(self, results):\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            message = f\"🚨 Confluence Detection - Binance BTC Pairs {self.timeframe}\\n\\n\"\n",
    "            \n",
    "            # Map timeframe to TradingView format - Extended mapping\n",
    "            tv_timeframe_map = {\n",
    "                # Minutes\n",
    "                \"1m\": \"1\", \"3m\": \"3\", \"5m\": \"5\", \"15m\": \"15\", \"30m\": \"30\",\n",
    "                # Hours  \n",
    "                \"1h\": \"60\", \"2h\": \"120\", \"4h\": \"240\", \"6h\": \"360\", \"8h\": \"480\", \"12h\": \"720\",\n",
    "                # Days\n",
    "                \"1d\": \"1D\", \"2d\": \"2D\", \"3d\": \"3D\",\n",
    "                # Weeks/Months\n",
    "                \"1w\": \"1W\", \"1M\": \"1M\"\n",
    "            }\n",
    "            tv_timeframe = tv_timeframe_map.get(self.timeframe.lower(), self.timeframe)\n",
    "            \n",
    "            for result in results:\n",
    "                formatted_symbol = result['symbol']\n",
    "                tv_link = f\"https://www.tradingview.com/chart/?symbol=BINANCE:{formatted_symbol}&interval={tv_timeframe}\"\n",
    "                \n",
    "                # Escape HTML entities in the URL\n",
    "                escaped_link = html.escape(tv_link)\n",
    "                \n",
    "                # Format according to BTC specifications\n",
    "                time_str = \"\"\n",
    "                if result.get('timestamp') is not None:\n",
    "                    time_str = f\"Time: {result['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "                \n",
    "                message += (\n",
    "                    f\"Symbol: {result['symbol']}\\n\"\n",
    "                    f\"{time_str}\"\n",
    "                    f\"Volume BTC: ₿{result['volume_btc']:,.4f}\\n\"\n",
    "                    f\"Close: <a href='{escaped_link}'>₿{result['close']:.8f}</a>\\n\"\n",
    "                    f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                    f\"Close Off Low: {result['close_off_low']:.1f}%\\n\"\n",
    "                    f\"Momentum: {result['momentum_score']:.4f}\\n\"\n",
    "                    f\"{'='*30}\\n\"\n",
    "                )\n",
    "            \n",
    "            # Split message more carefully to avoid breaking HTML tags\n",
    "            max_length = 4000\n",
    "            \n",
    "            if len(message) > max_length:\n",
    "                # Split at natural breaks (between results) to avoid breaking HTML\n",
    "                sections = message.split('='*30 + '\\n')\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for section in sections:\n",
    "                    if len(current_chunk + section + '='*30 + '\\n') > max_length:\n",
    "                        if current_chunk:\n",
    "                            await self.telegram_app.bot.send_message(\n",
    "                                chat_id=self.telegram_chat_id,\n",
    "                                text=current_chunk.strip(),\n",
    "                                parse_mode='HTML',\n",
    "                                disable_web_page_preview=True\n",
    "                            )\n",
    "                        current_chunk = section + '\\n'\n",
    "                    else:\n",
    "                        current_chunk += section + '='*30 + '\\n'\n",
    "                \n",
    "                # Send remaining chunk\n",
    "                if current_chunk.strip():\n",
    "                    await self.telegram_app.bot.send_message(\n",
    "                        chat_id=self.telegram_chat_id,\n",
    "                        text=current_chunk.strip(),\n",
    "                        parse_mode='HTML',\n",
    "                        disable_web_page_preview=True\n",
    "                    )\n",
    "            else:\n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=message,\n",
    "                    parse_mode='HTML',\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sending Telegram alert: {str(e)}\")\n",
    "            \n",
    "            # Fallback: send without HTML formatting\n",
    "            try:\n",
    "                simple_message = f\"🚨 Confluence Detection - Binance BTC Pairs {self.timeframe}\\n\\n\"\n",
    "                for result in results:\n",
    "                    simple_message += (\n",
    "                        f\"Symbol: {result['symbol']}\\n\"\n",
    "                        f\"Volume BTC: ₿{result['volume_btc']:,.4f}\\n\"\n",
    "                        f\"Close: ₿{result['close']:.8f}\\n\"\n",
    "                        f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                        f\"Components: Vol={result['high_volume']}, Spread={result['spread_breakout']}, Mom={result['momentum_breakout']}\\n\\n\"\n",
    "                    )\n",
    "                \n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=simple_message,\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "            except Exception as fallback_error:\n",
    "                logging.error(f\"Fallback Telegram send also failed: {str(fallback_error)}\")\n",
    "\n",
    "    def scan_single_market(self, pair, df):\n",
    "        \"\"\"Scan a single market for Confluence pattern in the specified bar\"\"\"\n",
    "        try:\n",
    "            if df is None or len(df) < 50:  # Need enough data for confluence\n",
    "                return None\n",
    "            \n",
    "            # Calculate which bar to check based on offset\n",
    "            check_bar = -(self.offset + 1)  # offset=0 means current bar (-1), offset=1 means last closed (-2), etc.\n",
    "            \n",
    "            # Run confluence detection\n",
    "            detected, result = detect_confluence(df, check_bar=check_bar)\n",
    "            \n",
    "            if detected:\n",
    "                # Get the target bar values\n",
    "                target_close = df['close'].iloc[check_bar]\n",
    "                target_volume = df['volume'].iloc[check_bar]  # This is already in BTC\n",
    "                \n",
    "                confluence_result = {\n",
    "                    'symbol': pair['symbol'],\n",
    "                    'volume_btc': float(target_volume),\n",
    "                    'close': float(target_close),\n",
    "                    'volume': float(target_volume),\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                    'bar_range': result['bar_range'],\n",
    "                    'timestamp': df.index[check_bar] if hasattr(df.index, '__getitem__') else None\n",
    "                }\n",
    "                return confluence_result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pair['symbol']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    async def scan_all_markets(self):\n",
    "        \"\"\"Scan all BTC markets for Confluence pattern\"\"\"\n",
    "        await self.init_telegram()\n",
    "        \n",
    "        try:\n",
    "            # Define volume thresholds for BTC pairs based on timeframe\n",
    "            volume_thresholds = {\n",
    "                # Minutes - higher volume needed for shorter timeframes\n",
    "                \"1m\": 0.01,   \"3m\": 0.02,   \"5m\": 0.03,\n",
    "                \"15m\": 0.05,  \"30m\": 0.08,\n",
    "                # Hours\n",
    "                \"1h\": 0.1,    \"2h\": 0.15,   \"4h\": 0.2,\n",
    "                \"6h\": 0.25,   \"8h\": 0.3,    \"12h\": 0.35,\n",
    "                # Days\n",
    "                \"1d\": 0.4,    \"2d\": 0.8,    \"3d\": 1.2,\n",
    "                # Weeks/Months\n",
    "                \"1w\": 2.0,    \"1M\": 8.0\n",
    "            }\n",
    "            min_volume = volume_thresholds.get(self.timeframe.lower(), 0.1)  # Default 0.1 BTC\n",
    "            \n",
    "            # Create offset description\n",
    "            if self.offset == 0:\n",
    "                offset_desc = \"current candle\"\n",
    "            elif self.offset == 1:\n",
    "                offset_desc = \"last closed candle\"\n",
    "            else:\n",
    "                offset_desc = f\"{self.offset} candles ago\"\n",
    "            \n",
    "            print(f\"Scanning Binance BTC pairs for Confluence patterns in {offset_desc}...\")\n",
    "            print(f\"Timeframe: {self.timeframe}\")\n",
    "            print(f\"Minimum volume threshold: ₿{min_volume:.2f}\")\n",
    "            \n",
    "            # Get all BTC pairs from Binance\n",
    "            print(\"Fetching BTC pairs from Binance...\")\n",
    "            btc_pairs = await self.get_btc_pairs()\n",
    "            \n",
    "            if not btc_pairs:\n",
    "                print(\"No BTC pairs found or error fetching pairs\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"Found {len(btc_pairs)} BTC pairs to scan\")\n",
    "            \n",
    "            # Filter out stablecoins and obvious non-trading pairs\n",
    "            filtered_pairs = []\n",
    "            skip_tokens = ['USDT', 'USDC', 'BUSD', 'DAI', 'TUSD', 'USDD', 'FDUSD']\n",
    "            \n",
    "            for pair in btc_pairs:\n",
    "                if pair['baseAsset'] not in skip_tokens:\n",
    "                    filtered_pairs.append(pair)\n",
    "            \n",
    "            print(f\"After filtering: {len(filtered_pairs)} pairs to scan\")\n",
    "            \n",
    "            # Get Binance timeframe - fallback to 1d if not found\n",
    "            binance_interval = self.timeframe_map.get(self.timeframe.lower(), \"1d\")\n",
    "            \n",
    "            # Process all pairs with progress bar\n",
    "            all_results = []\n",
    "            successful_scans = 0\n",
    "            \n",
    "            with tqdm(total=len(filtered_pairs), desc=\"Scanning markets\") as pbar:\n",
    "                for pair in filtered_pairs:\n",
    "                    try:\n",
    "                        # Get OHLCV data from Binance\n",
    "                        df = await self.get_klines(pair['symbol'], binance_interval, 100)\n",
    "                        \n",
    "                        if df is None or len(df) < 50:\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        successful_scans += 1\n",
    "                        target_idx = -(self.offset + 1)\n",
    "                        \n",
    "                        # Update progress bar with current symbol\n",
    "                        pbar.set_description(f\"Scanning: {pair['symbol']} ({len(df)} candles)\")\n",
    "                        \n",
    "                        # Check volume threshold for the target candle\n",
    "                        try:\n",
    "                            target_candle_volume = float(df['volume'].iloc[target_idx])  # Already in BTC\n",
    "                            \n",
    "                            # Only process if volume meets threshold\n",
    "                            if target_candle_volume >= min_volume:\n",
    "                                result = self.scan_single_market(pair, df)\n",
    "                                if result:\n",
    "                                    all_results.append(result)\n",
    "                                    print(f\"Found Confluence: {pair['symbol']} 🎯\")\n",
    "                        except (IndexError, ValueError):\n",
    "                            pass  # Skip if we can't calculate volume\n",
    "                        \n",
    "                        # Add small delay to respect rate limits\n",
    "                        await asyncio.sleep(0.1)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        if \"429\" in str(e):\n",
    "                            # Rate limit - add longer delay\n",
    "                            await asyncio.sleep(2)\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            \n",
    "            print(f\"Successfully scanned {successful_scans}/{len(filtered_pairs)} pairs\")\n",
    "            \n",
    "            # Sort by volume\n",
    "            all_results.sort(key=lambda x: x['volume_btc'], reverse=True)\n",
    "            \n",
    "            # Send Telegram alert if we found any patterns\n",
    "            if all_results:\n",
    "                await self.send_telegram_alert(all_results)\n",
    "            \n",
    "            return all_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scanning markets: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            await self.close_session()\n",
    "\n",
    "async def run_binance_btc_confluence_scanner(timeframe, offset=1):\n",
    "    \"\"\"\n",
    "    Run the Binance BTC Confluence scanner\n",
    "    \n",
    "    Parameters:\n",
    "    timeframe (str): Time period - Available options:\n",
    "                    Minutes: 1m, 3m, 5m, 15m, 30m\n",
    "                    Hours: 1h, 2h, 4h, 6h, 8h, 12h  \n",
    "                    Days: 1d, 2d, 3d\n",
    "                    Weeks/Months: 1w, 1M\n",
    "    offset (int): Bar offset (0=current, 1=last closed, 2=two bars ago, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if offset == 0:\n",
    "        offset_desc = \"current candle\"\n",
    "    elif offset == 1:\n",
    "        offset_desc = \"last closed candle\"\n",
    "    else:\n",
    "        offset_desc = f\"{offset} candles ago\"\n",
    "    \n",
    "    print(f\"Starting Binance BTC Confluence scan for {offset_desc} on {timeframe}...\")\n",
    "    \n",
    "    # Use the confluence telegram token\n",
    "    telegram_token = \"8066329517:AAHVr6kufZWe8UqCKPfmsRhSPleNlt_7G-g\"\n",
    "    telegram_chat_id = \"375812423\"\n",
    "    \n",
    "    scanner = BinanceBTCConfluenceScanner(telegram_token, telegram_chat_id, timeframe, offset)\n",
    "    results = await scanner.scan_all_markets()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} Confluence patterns:\")\n",
    "        \n",
    "        # Convert results to DataFrame for console display\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Round numeric columns for BTC precision\n",
    "        df_results['volume_btc'] = df_results['volume_btc'].round(4)\n",
    "        df_results['close'] = df_results['close'].round(8)\n",
    "        df_results['volume'] = df_results['volume'].round(4)\n",
    "        df_results['volume_ratio'] = df_results['volume_ratio'].round(2)\n",
    "        df_results['close_off_low'] = df_results['close_off_low'].round(1)\n",
    "        df_results['momentum_score'] = df_results['momentum_score'].round(4)\n",
    "        \n",
    "        # Reorder columns for better display\n",
    "        display_cols = ['symbol', 'close', 'volume_btc', 'volume_ratio', 'close_off_low', \n",
    "                       'momentum_score', 'high_volume', 'spread_breakout', 'momentum_breakout']\n",
    "        available_cols = [col for col in display_cols if col in df_results.columns]\n",
    "        \n",
    "        # Display the results\n",
    "        print(df_results[available_cols])\n",
    "        \n",
    "        # Show component analysis\n",
    "        print(f\"\\n🔧 COMPONENT ANALYSIS:\")\n",
    "        vol_count = df_results['high_volume'].sum()\n",
    "        spread_count = df_results['spread_breakout'].sum()\n",
    "        momentum_count = df_results['momentum_breakout'].sum()\n",
    "        \n",
    "        print(f\"High Volume signals: {vol_count}/{len(results)} ({vol_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Spread Breakout signals: {spread_count}/{len(results)} ({spread_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Momentum Breakout signals: {momentum_count}/{len(results)} ({momentum_count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo Confluence patterns found in {offset_desc}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Apply nest_asyncio to allow async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage functions\n",
    "async def scan_binance_btc_current():\n",
    "    \"\"\"Scan current candle for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=0)\n",
    "\n",
    "async def scan_binance_btc_closed():\n",
    "    \"\"\"Scan last closed candle for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=1)\n",
    "\n",
    "async def scan_binance_btc_previous():\n",
    "    \"\"\"Scan two candles ago for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=2)\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution - modify parameters here\n",
    "    \"\"\"\n",
    "    timeframe = \"1w\"  # 1d, 2d, 3d, 1w\n",
    "    offset = 0        # 0 = current candle, 1 = last closed candle, 2 = two candles ago\n",
    "    \n",
    "    await run_binance_btc_confluence_scanner(timeframe, offset)\n",
    "\n",
    "# Run the async main function\n",
    "print(\"🔍 BINANCE BTC CONFLUENCE SCANNER\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Available timeframes:\")\n",
    "print(\"• Minutes: 1m, 3m, 5m, 15m, 30m\")\n",
    "print(\"• Hours: 1h, 2h, 4h, 6h, 8h, 12h\") \n",
    "print(\"• Days: 1d, 2d, 3d\")\n",
    "print(\"• Weeks/Months: 1w, 1M\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"• await main() - Run with default settings\")\n",
    "print(\"• await scan_binance_btc_current() - Scan current candle\")\n",
    "print(\"• await scan_binance_btc_closed() - Scan last closed candle\")\n",
    "print(\"• await scan_binance_btc_previous() - Scan two candles ago\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('timeframe', offset) - Custom scan\")\n",
    "print(\"\\nExamples:\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('4h', 1)  # 4-hour last closed\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('15m', 0) # 15-min current\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('1M', 1)  # Monthly last closed\")\n",
    "print(\"This scanner uses REAL Binance BTC pair volumes!\")\n",
    "\n",
    "# Uncomment to auto-run:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b52f4-ca65-425b-a41a-42b70a57aefa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Direct strategy debug of any pair on any exchange\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "from exchanges import MexcSpotClient, BybitSpotClient, GateioSpotClient, KucoinSpotClient, BinanceSpotClient, BinanceFuturesClient, BybitFuturesClient\n",
    "from custom_strategies import detect_volume_surge, detect_weak_uptrend, detect_pin_down\n",
    "from breakout_vsa import vsa_detector, breakout_bar_vsa, stop_bar_vsa, reversal_bar_vsa, start_bar_vsa, loaded_bar_vsa, test_bar_vsa\n",
    "\n",
    "async def test_strategy(exchange_client_class, timeframe, symbol, strategy_name):\n",
    "    client = exchange_client_class(timeframe=timeframe)\n",
    "    await client.init_session()\n",
    "    df = await client.fetch_klines(symbol)\n",
    "    await client.close_session()\n",
    "    \n",
    "    if df is None or len(df) < 10:\n",
    "        print(f\"No data fetched for {symbol} or insufficient data (< 10 bars)\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{timeframe} Candles for {symbol}:\")\n",
    "    print(df.tail(5))\n",
    "    last_row = df.iloc[-1]\n",
    "    volume_usd = last_row['volume'] * last_row['close']\n",
    "    print(f\"Last Bar: volume_usd={volume_usd:.2f}, close={last_row['close']}, volume={last_row['volume']:.2f}\")\n",
    "    \n",
    "    # Different handling based on strategy type\n",
    "    if strategy_name == \"volume_surge\":\n",
    "        # Use detect_volume_surge directly\n",
    "        detected, result = detect_volume_surge(df)\n",
    "        \n",
    "        print(f\"\\nVolume Surge Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nVolume Surge Details:\")\n",
    "            print(f\"  Date: {result['timestamp']}\")\n",
    "            print(f\"  Close: ${result['close_price']:,.8f}\")\n",
    "            print(f\"  Volume: {result['volume']:,.2f}\")\n",
    "            print(f\"  Volume USD: ${result['volume_usd']:,.2f}\")\n",
    "            print(f\"  Volume Ratio: {result['volume_ratio']:,.2f}x\")\n",
    "            print(f\"  Score: {result['score']:,.2f}\")\n",
    "            print(f\"  Price Extreme: {result['price_extreme']}\")\n",
    "    \n",
    "    elif strategy_name == \"pin_down\":\n",
    "        from custom_strategies import detect_pin_down\n",
    "        detected, result = detect_pin_down(df)\n",
    "        \n",
    "        print(f\"\\nPin Down Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nPin Down Details:\")\n",
    "            for key, value in result.items():\n",
    "                if key != 'symbol':  # Skip symbol as we already know it\n",
    "                    print(f\"  {key}: {value}\")\n",
    "    \n",
    "    elif strategy_name == \"weak_uptrend\":\n",
    "        from custom_strategies import detect_weak_uptrend\n",
    "        detected, result = detect_weak_uptrend(df)\n",
    "        \n",
    "        print(f\"\\nWeak Uptrend Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nWeak Uptrend Details:\")\n",
    "            for key, value in result.items():\n",
    "                if key != 'symbol':  # Skip symbol as we already know it\n",
    "                    print(f\"  {key}: {value}\")\n",
    "    \n",
    "    else:\n",
    "        # For VSA strategies, import the appropriate get_params\n",
    "        if strategy_name == \"reversal_bar\":\n",
    "            from breakout_vsa.strategies.reversal_bar import get_params\n",
    "        elif strategy_name == \"breakout_bar\":\n",
    "            from breakout_vsa.strategies.breakout_bar import get_params\n",
    "        elif strategy_name == \"loaded_bar\":\n",
    "            from breakout_vsa.strategies.loaded_bar import get_params\n",
    "        elif strategy_name == \"stop_bar\":\n",
    "            from breakout_vsa.strategies.stop_bar import get_params\n",
    "        elif strategy_name == \"start_bar\":\n",
    "            from breakout_vsa.strategies.start_bar import get_params\n",
    "        else:\n",
    "            print(f\"Unknown strategy: {strategy_name}\")\n",
    "            return\n",
    "        \n",
    "        # Use vsa_detector with strategy-specific params\n",
    "        params = get_params()\n",
    "        condition, result = vsa_detector(df, params)\n",
    "        \n",
    "        strategy_display_name = strategy_name.replace('_vsa', '').replace('_', ' ').title()\n",
    "        print(f\"\\n{strategy_display_name} Detection Results:\")\n",
    "        print(f\"Current Bar (index -1): {condition.iloc[-1]}\")\n",
    "        if len(df) > 1:\n",
    "            print(f\"Last Closed Bar (index -2): {condition.iloc[-2]}\")\n",
    "        \n",
    "        if condition.iloc[-1] or (len(df) > 1 and condition.iloc[-2]):\n",
    "            detected_idx = -1 if condition.iloc[-1] else -2\n",
    "            volume_mean = df['volume'].rolling(7).mean().iloc[detected_idx]\n",
    "            bar_range = df['high'].iloc[detected_idx] - df['low'].iloc[detected_idx]\n",
    "            close_off_low = (df['close'].iloc[detected_idx] - df['low'].iloc[detected_idx]) / bar_range * 100 if bar_range > 0 else 0\n",
    "            volume_usd_detected = df['volume'].iloc[detected_idx] * df['close'].iloc[detected_idx]\n",
    "            \n",
    "            arctan_ratio = result['arctan_ratio'].iloc[detected_idx]  # From result DataFrame\n",
    "            \n",
    "            print(f\"\\nDetected at index {detected_idx} ({'Current' if detected_idx == -1 else 'Last Closed'} Bar):\")\n",
    "            print(f\"  Date: {df.index[detected_idx]}\")\n",
    "            print(f\"  Close: ${df['close'].iloc[detected_idx]:,.8f}\")\n",
    "            print(f\"  Volume Ratio: {df['volume'].iloc[detected_idx] / volume_mean if volume_mean > 0 else 0:.2f}x\")\n",
    "            print(f\"  {timeframe} Volume: ${volume_usd_detected:.2f}\")\n",
    "            print(f\"  Close Off Low: {close_off_low:.1f}%\")\n",
    "            print(f\"  Angular Ratio: {arctan_ratio:.2f}\")\n",
    "\n",
    "# Define the test case\n",
    "exchange_client = GateioSpotClient\n",
    "timeframe = \"1w\"\n",
    "symbol = \"PRCL_USDT\"\n",
    "strategy = \"loaded_bar\"\n",
    "await test_strategy(exchange_client, timeframe, symbol, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9bf3d-5437-413f-a6a8-2496412059e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#zip the project\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Go to parent directory of your project\n",
    "os.chdir(\"/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025\")\n",
    "\n",
    "# Create the zip file (this will include everything inside 'hbs_2025')\n",
    "shutil.make_archive(\"Project_VSA_2025_backup\", 'zip', \"Project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c74d8-f489-49e8-8596-c36b3960fbe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 -> Debug built weekly candles for mexc and kucoin \n",
    "import sys\n",
    "import os\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "import asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from exchanges.kucoin_client import KucoinClient\n",
    "from breakout_vsa.core import calculate_start_bar\n",
    "\n",
    "from scanner.main import kline_cache\n",
    "kline_cache.clear()  # Clear cache for fresh data\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "async def debug_start_bar_detection():\n",
    "    # Initialize client\n",
    "    client = KucoinClient(timeframe=\"1w\")\n",
    "    await client.init_session()\n",
    "    \n",
    "    # Symbol to debug\n",
    "    symbol = \"TAO-USDT\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch data\n",
    "        df = await client.fetch_klines(symbol)\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"Weekly candles for {symbol}:\")\n",
    "            print(df.tail())\n",
    "            \n",
    "            # Add intermediate calculations to see what's happening\n",
    "            # This is a modified version of calculate_start_bar that adds debugging\n",
    "            lookback = 5\n",
    "            volume_lookback = 30\n",
    "            volume_percentile = 50\n",
    "            low_percentile = 75\n",
    "            range_percentile = 75\n",
    "            close_off_lows_percent = 50\n",
    "            prev_close_range = 75\n",
    "            \n",
    "            # Calculate basic bar characteristics\n",
    "            df['bar_range'] = df['high'] - df['low']\n",
    "            df['volume_rank'] = df['volume'].rolling(lookback).apply(\n",
    "                lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                raw=True\n",
    "            )\n",
    "            \n",
    "            # Calculate rolling values\n",
    "            df['macro_low'] = df['low'].rolling(volume_lookback).min()\n",
    "            df['macro_high'] = df['high'].rolling(volume_lookback).max()\n",
    "            df['highest_high'] = df['high'].rolling(lookback).max()\n",
    "            \n",
    "            # Volume conditions\n",
    "            df['volume_sma'] = df['volume'].rolling(volume_lookback).mean()\n",
    "            df['volume_std'] = df['volume'].rolling(volume_lookback).std()\n",
    "            df['excess_volume'] = df['volume'] > (df['volume_sma'] + 3.0 * df['volume_std'])\n",
    "            \n",
    "            # Range conditions\n",
    "            df['range_sma'] = df['bar_range'].rolling(volume_lookback).mean()\n",
    "            df['range_std'] = df['bar_range'].rolling(volume_lookback).std()\n",
    "            df['excess_range'] = df['bar_range'] > (df['range_sma'] + 3.0 * df['range_std'])\n",
    "            \n",
    "            # Volume percentile condition\n",
    "            def is_in_top_percent(series, length, percent):\n",
    "                ranks = series.rolling(length).apply(\n",
    "                    lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                    raw=True\n",
    "                )\n",
    "                return ranks >= percent\n",
    "            \n",
    "            def is_in_bottom_percent(series, length, percent):\n",
    "                ranks = series.rolling(length).apply(\n",
    "                    lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                    raw=True\n",
    "                )\n",
    "                return ranks <= percent\n",
    "            \n",
    "            # Volume conditions\n",
    "            df['is_higher_volume'] = is_in_top_percent(df['volume'], lookback, volume_percentile)\n",
    "            df['is_high_volume'] = (df['volume'] > 0.75 * df['volume_sma']) & (df['volume'] > df['volume'].shift(1))\n",
    "            \n",
    "            # Price action conditions\n",
    "            df['has_higher_high'] = df['high'] > df['high'].shift(1)\n",
    "            df['no_narrow_range'] = is_in_top_percent(df['bar_range'], lookback, range_percentile)\n",
    "            \n",
    "            # Low price condition\n",
    "            df['is_in_the_lows'] = (\n",
    "                (df['low'] - df['macro_low']).abs() < df['bar_range']\n",
    "            ) | is_in_bottom_percent(df['low'], volume_lookback, low_percentile)\n",
    "            \n",
    "            # Close position conditions\n",
    "            df['close_in_the_highs'] = (\n",
    "                (df['close'] - df['low']) / df['bar_range']\n",
    "            ) >= (close_off_lows_percent / 100)\n",
    "            \n",
    "            # Previous close distance condition\n",
    "            df['far_prev_close'] = (\n",
    "                (df['close'] - df['close'].shift(1)).abs() >=\n",
    "                (df['bar_range'].shift(1) * (prev_close_range / 100))\n",
    "            )\n",
    "            \n",
    "            # New highs condition\n",
    "            df['new_highs'] = df['high'] >= 0.75 * df['highest_high']\n",
    "            \n",
    "            # Optional strength condition\n",
    "            df['strong_close'] = df['close'] >= df['highest_high'].shift(1)\n",
    "            \n",
    "            # Now check the actual values for the last few bars\n",
    "            last_rows = df.tail(3)\n",
    "            \n",
    "            print(\"\\nAnalyzing last 3 bars:\")\n",
    "            for idx, row in last_rows.iterrows():\n",
    "                print(f\"\\nBar at {idx.strftime('%Y-%m-%d')}:\")\n",
    "                print(f\"  is_high_volume: {row['is_high_volume']}\")\n",
    "                print(f\"  has_higher_high: {row['has_higher_high']}\")\n",
    "                print(f\"  no_narrow_range: {row['no_narrow_range']}\")\n",
    "                print(f\"  close_in_the_highs: {row['close_in_the_highs']}\")\n",
    "                print(f\"  far_prev_close: {row['far_prev_close']}\")\n",
    "                print(f\"  excess_range: {row['excess_range']}\")\n",
    "                print(f\"  excess_volume: {row['excess_volume']}\")\n",
    "                print(f\"  new_highs: {row['new_highs']}\")\n",
    "                print(f\"  is_in_the_lows: {row['is_in_the_lows']}\")\n",
    "                print(f\"  volume: {row['volume']}, volume_sma: {row['volume_sma']}\")\n",
    "                print(f\"  bar_range: {row['bar_range']}, range_sma: {row['range_sma']}\")\n",
    "                \n",
    "            # Run the original function to confirm\n",
    "            start_bar_pattern = calculate_start_bar(df)\n",
    "            print(f\"\\nFinal Start Bar detection result:\")\n",
    "            print(start_bar_pattern.tail(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug: {str(e)}\")\n",
    "    finally:\n",
    "        await client.close_session()\n",
    "\n",
    "# Replace the last part of your script with this:\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # For Jupyter/IPython environments\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        asyncio.run(debug_start_bar_detection())\n",
    "    except ImportError:\n",
    "        # For regular Python environments\n",
    "        asyncio.run(debug_start_bar_detection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dffeddd-c2c8-4f75-887f-9ed7d1961a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project_VSA_2025_backup.zip'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip the project\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Go to parent directory of your project\n",
    "os.chdir(\"/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025\")\n",
    "\n",
    "# Create the zip file (this will include everything inside 'hbs_2025')\n",
    "shutil.make_archive(\"Project_VSA_2025_backup\", 'zip', \"Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29951e-0a3b-4d0b-928c-0f2bfce7c11c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ARCHIVE - Confluence Scanner with bar offset\n",
    "\n",
    "from telegram.ext import Application\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import os\n",
    "import html\n",
    "\n",
    "# Add project path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from exchanges.sf_pairs_service import SFPairsService\n",
    "from custom_strategies import detect_confluence\n",
    "\n",
    "class ConfluenceScanner:\n",
    "    def __init__(self, telegram_token, telegram_chat_id, exchange, timeframe, offset=1):\n",
    "        self.telegram_token = telegram_token\n",
    "        self.telegram_chat_id = telegram_chat_id\n",
    "        self.telegram_app = None\n",
    "        self.exchange = exchange\n",
    "        self.timeframe = timeframe\n",
    "        self.offset = offset  # Added offset parameter\n",
    "        self.sf_service = SFPairsService()\n",
    "        \n",
    "    async def init_telegram(self):\n",
    "        if self.telegram_app is None:\n",
    "            self.telegram_app = Application.builder().token(self.telegram_token).build()\n",
    "\n",
    "    async def send_telegram_alert(self, results):\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            message = f\"🚨 Confluence Detection - {self.exchange} {self.timeframe}\\n\\n\"\n",
    "            \n",
    "            # Map timeframe to TradingView format\n",
    "            tv_timeframe_map = {\n",
    "                \"1d\": \"1D\",\n",
    "                \"2d\": \"2D\",\n",
    "                \"1w\": \"1W\"\n",
    "            }\n",
    "            tv_timeframe = tv_timeframe_map.get(self.timeframe.lower(), self.timeframe)\n",
    "            \n",
    "            for result in results:\n",
    "                exchange_name = self.exchange.upper()\n",
    "                formatted_symbol = f\"{result['symbol']}\"\n",
    "                tv_link = f\"https://www.tradingview.com/chart/?symbol={exchange_name}:{formatted_symbol}&interval={tv_timeframe}\"\n",
    "                \n",
    "                # Escape HTML entities in the URL\n",
    "                escaped_link = html.escape(tv_link)\n",
    "                \n",
    "                # Format according to specified requirements\n",
    "                time_str = \"\"\n",
    "                if result.get('timestamp') is not None:\n",
    "                    time_str = f\"Time: {result['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "                \n",
    "                message += (\n",
    "                    f\"Symbol: {result['symbol']}\\n\"\n",
    "                    f\"{time_str}\"\n",
    "                    f\"Volume USD: ${result['volume_usd']:,.2f}\\n\"\n",
    "                    f\"Close: <a href='{escaped_link}'>${result['close']:,.8f}</a>\\n\"\n",
    "                    f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                    f\"Close Off Low: {result['close_off_low']:.1f}%\\n\"\n",
    "                    f\"{'='*30}\\n\"\n",
    "                )\n",
    "            \n",
    "            # Split message more carefully to avoid breaking HTML tags\n",
    "            max_length = 4000  # Reduced from 4096 to be safer\n",
    "            \n",
    "            if len(message) > max_length:\n",
    "                # Split at natural breaks (between results) to avoid breaking HTML\n",
    "                sections = message.split('='*30 + '\\n')\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for section in sections:\n",
    "                    if len(current_chunk + section + '='*30 + '\\n') > max_length:\n",
    "                        if current_chunk:\n",
    "                            await self.telegram_app.bot.send_message(\n",
    "                                chat_id=self.telegram_chat_id,\n",
    "                                text=current_chunk.strip(),\n",
    "                                parse_mode='HTML',\n",
    "                                disable_web_page_preview=True\n",
    "                            )\n",
    "                        current_chunk = section + '\\n'\n",
    "                    else:\n",
    "                        current_chunk += section + '='*30 + '\\n'\n",
    "                \n",
    "                # Send remaining chunk\n",
    "                if current_chunk.strip():\n",
    "                    await self.telegram_app.bot.send_message(\n",
    "                        chat_id=self.telegram_chat_id,\n",
    "                        text=current_chunk.strip(),\n",
    "                        parse_mode='HTML',\n",
    "                        disable_web_page_preview=True\n",
    "                    )\n",
    "            else:\n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=message,\n",
    "                    parse_mode='HTML',\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sending Telegram alert: {str(e)}\")\n",
    "            \n",
    "            # Fallback: send without HTML formatting\n",
    "            try:\n",
    "                simple_message = f\"🚨 Confluence Detection - {self.exchange} {self.timeframe}\\n\\n\"\n",
    "                for result in results:\n",
    "                    simple_message += (\n",
    "                        f\"Symbol: {result['symbol']}\\n\"\n",
    "                        f\"Volume USD: ${result['volume_usd']:,.2f}\\n\"\n",
    "                        f\"Close: ${result['close']:,.8f}\\n\"\n",
    "                        f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                        f\"Components: Vol={result['high_volume']}, Spread={result['spread_breakout']}, Mom={result['momentum_breakout']}\\n\\n\"\n",
    "                    )\n",
    "                \n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=simple_message,\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "            except Exception as fallback_error:\n",
    "                logging.error(f\"Fallback Telegram send also failed: {str(fallback_error)}\")\n",
    "\n",
    "    def prepare_sf_data(self, raw_df):\n",
    "        \"\"\"Convert SF data to confluence-compatible format\"\"\"\n",
    "        if raw_df is None or len(raw_df) == 0:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(raw_df)\n",
    "        \n",
    "        # Convert datetime column to pandas datetime and set as index\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df = df.set_index('datetime')\n",
    "        elif 'time' in df.columns:\n",
    "            # Convert Unix timestamp to datetime\n",
    "            df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "            df = df.set_index('time')\n",
    "        \n",
    "        # Select only OHLCV columns needed for confluence\n",
    "        required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        available_cols = [col for col in required_cols if col in df.columns]\n",
    "        \n",
    "        if len(available_cols) != 5:\n",
    "            return None\n",
    "        \n",
    "        # Select and clean data\n",
    "        result_df = df[required_cols].copy()\n",
    "        \n",
    "        # Ensure numeric types\n",
    "        for col in required_cols:\n",
    "            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "        \n",
    "        # Drop any NaN rows\n",
    "        result_df = result_df.dropna()\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def scan_single_market(self, pair, ohlcv_data):\n",
    "        \"\"\"Scan a single market for Confluence pattern in the specified bar\"\"\"\n",
    "        try:\n",
    "            # Prepare data for confluence analysis\n",
    "            df = self.prepare_sf_data(ohlcv_data)\n",
    "            \n",
    "            if df is None or len(df) < 50:  # Need enough data for confluence\n",
    "                return None\n",
    "            \n",
    "            # Calculate which bar to check based on offset\n",
    "            check_bar = -(self.offset + 1)  # offset=0 means current bar (-1), offset=1 means last closed (-2), etc.\n",
    "            \n",
    "            # Run confluence detection\n",
    "            detected, result = detect_confluence(df, check_bar=check_bar)\n",
    "            \n",
    "            if detected:\n",
    "                # Calculate volume in USD for the target bar\n",
    "                target_close = df['close'].iloc[check_bar]\n",
    "                target_volume = df['volume'].iloc[check_bar]\n",
    "                volume_usd = float(target_close) * float(target_volume)\n",
    "                \n",
    "                confluence_result = {\n",
    "                    'symbol': f\"{pair['Token']}{pair['Quote']}\",\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'close': float(target_close),\n",
    "                    'volume': float(target_volume),\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                    'bar_range': result['bar_range']\n",
    "                }\n",
    "                return confluence_result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pair['Token']}{pair['Quote']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    async def scan_all_markets(self):\n",
    "        \"\"\"Scan all markets for Confluence pattern\"\"\"\n",
    "        await self.init_telegram()\n",
    "        try:\n",
    "            # Define volume thresholds\n",
    "            volume_thresholds = {\n",
    "                \"1w\": 300000,\n",
    "                \"2d\": 100000,\n",
    "                \"1d\": 50000\n",
    "            }\n",
    "            min_volume = volume_thresholds.get(self.timeframe.lower(), 50000)\n",
    "            \n",
    "            # Create offset description\n",
    "            if self.offset == 0:\n",
    "                offset_desc = \"current candle\"\n",
    "            elif self.offset == 1:\n",
    "                offset_desc = \"last closed candle\"\n",
    "            else:\n",
    "                offset_desc = f\"{self.offset} candles ago\"\n",
    "            \n",
    "            print(f\"Scanning for Confluence patterns in {offset_desc}...\")\n",
    "            print(f\"Minimum volume threshold: ${min_volume:,.0f}\")\n",
    "            \n",
    "            # Get all pairs from SF service\n",
    "            pairs = self.sf_service.get_pairs_of_exchange(self.exchange)\n",
    "            print(f\"Found {len(pairs)} markets to scan...\")\n",
    "            \n",
    "            # Process all pairs with progress bar\n",
    "            all_results = []\n",
    "            with tqdm(total=len(pairs), desc=\"Scanning markets\") as pbar:\n",
    "                for pair in pairs:\n",
    "                    try:\n",
    "                        # Get OHLCV data from SF service\n",
    "                        ohlcv_data = self.sf_service.get_ohlcv_for_pair(\n",
    "                            pair['Token'], \n",
    "                            pair['Quote'], \n",
    "                            self.exchange, \n",
    "                            self.timeframe, \n",
    "                            100  # Get more data for confluence analysis\n",
    "                        )\n",
    "                        \n",
    "                        if ohlcv_data is None or len(ohlcv_data) == 0:\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        df = pd.DataFrame(ohlcv_data)\n",
    "                        \n",
    "                        # Check if we have enough data\n",
    "                        if len(df) >= 50:  # Need enough for confluence analysis\n",
    "                            target_idx = -(self.offset + 1)  # Adjust index based on offset\n",
    "                            \n",
    "                            # Check volume threshold for the target candle\n",
    "                            try:\n",
    "                                target_candle_volume = float(df['close'].iloc[target_idx]) * float(df['volume'].iloc[target_idx])\n",
    "                                \n",
    "                                # Only process if volume meets threshold\n",
    "                                if target_candle_volume >= min_volume:\n",
    "                                    result = self.scan_single_market(pair, ohlcv_data)\n",
    "                                    if result:\n",
    "                                        all_results.append(result)\n",
    "                                        print(f\"Found Confluence: {pair['Token']}{pair['Quote']} 🎯\")\n",
    "                            except (IndexError, ValueError):\n",
    "                                pass  # Skip if we can't calculate volume\n",
    "                                    \n",
    "                    except Exception as e:\n",
    "                        if \"500\" not in str(e):  # Don't log 500 errors\n",
    "                            logging.error(f\"Error processing {pair['Token']}{pair['Quote']}: {str(e)}\")\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            \n",
    "            # Sort by volume\n",
    "            all_results.sort(key=lambda x: x['volume_usd'], reverse=True)\n",
    "            \n",
    "            # Send Telegram alert if we found any patterns\n",
    "            if all_results:\n",
    "                await self.send_telegram_alert(all_results)\n",
    "            \n",
    "            return all_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scanning markets: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "async def run_confluence_scanner(exchange, timeframe, offset=1):\n",
    "    \"\"\"\n",
    "    Run the Confluence scanner\n",
    "    \n",
    "    Parameters:\n",
    "    exchange (str): Exchange name (Kucoin, Mexc, Binance)\n",
    "    timeframe (str): Time period (1d, 2d, 1w)\n",
    "    offset (int): Bar offset (0=current, 1=last closed, 2=two bars ago, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if offset == 0:\n",
    "        offset_desc = \"current candle\"\n",
    "    elif offset == 1:\n",
    "        offset_desc = \"last closed candle\"\n",
    "    else:\n",
    "        offset_desc = f\"{offset} candles ago\"\n",
    "    \n",
    "    print(f\"Starting Confluence scan for {offset_desc} on {exchange} {timeframe}...\")\n",
    "    \n",
    "    # Use the confluence telegram token from your big project config\n",
    "    # You should replace this with the actual token from utils/config.py TELEGRAM_TOKENS[\"confluence\"]\n",
    "    telegram_token = \"8066329517:AAHVr6kufZWe8UqCKPfmsRhSPleNlt_7G-g\"  # Replace with confluence token\n",
    "    telegram_chat_id = \"375812423\"  # Your chat ID\n",
    "    \n",
    "    scanner = ConfluenceScanner(telegram_token, telegram_chat_id, exchange, timeframe, offset)\n",
    "    results = await scanner.scan_all_markets()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} Confluence patterns:\")\n",
    "        \n",
    "        # Convert results to DataFrame for console display\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Round numeric columns\n",
    "        df_results['volume_usd'] = df_results['volume_usd'].round(2)\n",
    "        df_results['close'] = df_results['close'].round(8)\n",
    "        df_results['volume'] = df_results['volume'].round(2)\n",
    "        df_results['volume_ratio'] = df_results['volume_ratio'].round(2)\n",
    "        df_results['close_off_low'] = df_results['close_off_low'].round(1)\n",
    "        df_results['momentum_score'] = df_results['momentum_score'].round(4)\n",
    "        \n",
    "        # Reorder columns for better display\n",
    "        display_cols = ['symbol', 'close', 'volume_usd', 'volume_ratio', 'close_off_low', \n",
    "                       'momentum_score', 'high_volume', 'spread_breakout', 'momentum_breakout']\n",
    "        available_cols = [col for col in display_cols if col in df_results.columns]\n",
    "        \n",
    "        # Display the results\n",
    "        print(df_results[available_cols])\n",
    "        \n",
    "        # Show component analysis\n",
    "        print(f\"\\n🔧 COMPONENT ANALYSIS:\")\n",
    "        vol_count = df_results['high_volume'].sum()\n",
    "        spread_count = df_results['spread_breakout'].sum()\n",
    "        momentum_count = df_results['momentum_breakout'].sum()\n",
    "        \n",
    "        print(f\"High Volume signals: {vol_count}/{len(results)} ({vol_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Spread Breakout signals: {spread_count}/{len(results)} ({spread_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Momentum Breakout signals: {momentum_count}/{len(results)} ({momentum_count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo Confluence patterns found in {offset_desc}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Apply nest_asyncio to allow async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage functions\n",
    "async def scan_current_confluence():\n",
    "    \"\"\"Scan current candle for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=0)\n",
    "\n",
    "async def scan_closed_confluence():\n",
    "    \"\"\"Scan last closed candle for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=1)\n",
    "\n",
    "async def scan_previous_confluence():\n",
    "    \"\"\"Scan two candles ago for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=2)\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution - modify parameters here\n",
    "    \"\"\"\n",
    "    exchange = \"Mexc\"  # Binance, Kucoin, Mexc\n",
    "    timeframe = \"1w\"     # 1d, 2d, 1w\n",
    "    offset = 0           # 0 = current candle, 1 = last closed candle, 2 = two candles ago\n",
    "    \n",
    "    await run_confluence_scanner(exchange, timeframe, offset)\n",
    "\n",
    "# Run the async main function\n",
    "print(\"🔍 CONFLUENCE SCANNER\")\n",
    "print(\"=\" * 30)\n",
    "print(\"Available functions:\")\n",
    "print(\"• await main() - Run with default settings\")\n",
    "print(\"• await scan_current_confluence() - Scan current candle\")\n",
    "print(\"• await scan_closed_confluence() - Scan last closed candle\")\n",
    "print(\"• await scan_previous_confluence() - Scan two candles ago\")\n",
    "print(\"• await run_confluence_scanner('Exchange', 'timeframe', offset) - Custom scan\")\n",
    "print(\"\\nExample: await main()\")\n",
    "\n",
    "# Uncomment to auto-run:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0c836-afa7-4fba-94d7-0dfafef0f8c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Standalone HLC Bar Chart Plotter\n",
    "\"\"\"\n",
    "Standalone HLC Bar Chart Plotter\n",
    "A reusable function for plotting HLC bars with optional pattern highlighting\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_hlc_bars(data, highlighted_bars=None, title=\"HLC Chart\", symbol=\"SYMBOL\", \n",
    "                  interval=\"1d\", semilog=False, highlight_color=\"fuchsia\", \n",
    "                  highlight_label=\"Pattern\", figsize=(14, 10), show_volume=True):\n",
    "    \"\"\"\n",
    "    Plot HLC bar chart with optional pattern highlighting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame with columns: ['datetime', 'high', 'low', 'close', 'volume']\n",
    "        - datetime: timestamp column (will be used for x-axis)\n",
    "        - high: high prices\n",
    "        - low: low prices  \n",
    "        - close: close prices\n",
    "        - volume: volume data (optional if show_volume=False)\n",
    "        \n",
    "    highlighted_bars : pandas.Series or list/array, optional\n",
    "        Boolean series or array indicating which bars to highlight\n",
    "        Length must match data length\n",
    "        \n",
    "    title : str, default \"HLC Chart\"\n",
    "        Chart title\n",
    "        \n",
    "    symbol : str, default \"SYMBOL\" \n",
    "        Symbol name for display\n",
    "        \n",
    "    interval : str, default \"1d\"\n",
    "        Time interval for date formatting (1m, 5m, 15m, 30m, 1h, 4h, 1d, 3d, 1w, 1M)\n",
    "        \n",
    "    semilog : bool, default False\n",
    "        Use logarithmic scale for price chart\n",
    "        \n",
    "    highlight_color : str, default \"fuchsia\"\n",
    "        Color for highlighted bars\n",
    "        \n",
    "    highlight_label : str, default \"Pattern\"\n",
    "        Label for highlighted bars in legend\n",
    "        \n",
    "    figsize : tuple, default (14, 10)\n",
    "        Figure size (width, height)\n",
    "        \n",
    "    show_volume : bool, default True\n",
    "        Whether to show volume subplot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The created figure object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise ValueError(\"data must be a pandas DataFrame\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['datetime', 'high', 'low', 'close']\n",
    "    if show_volume:\n",
    "        required_cols.append('volume')\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        raise ValueError(\"data DataFrame is empty\")\n",
    "    \n",
    "    # Validate highlighted_bars\n",
    "    if highlighted_bars is not None:\n",
    "        if len(highlighted_bars) != len(data):\n",
    "            raise ValueError(\"highlighted_bars length must match data length\")\n",
    "        # Convert to boolean array\n",
    "        highlighted_bars = np.array(highlighted_bars, dtype=bool)\n",
    "    else:\n",
    "        highlighted_bars = np.zeros(len(data), dtype=bool)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    if show_volume:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, \n",
    "                                       gridspec_kw={'height_ratios': [3, 1]})\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "        ax2 = None\n",
    "    \n",
    "    # Convert dates to numbers for plotting\n",
    "    dates = mdates.date2num(data['datetime'])\n",
    "    \n",
    "    # Calculate margins and tick length\n",
    "    date_range = dates[-1] - dates[0] if len(dates) > 1 else 1\n",
    "    margin = date_range * 0.05\n",
    "    \n",
    "    # Calculate actual bar spacing for consistent tick length\n",
    "    if len(dates) > 1:\n",
    "        avg_bar_spacing = date_range / (len(dates) - 1)\n",
    "        tick_length = avg_bar_spacing * 0.4  # 40% of bar spacing\n",
    "        volume_bar_width = avg_bar_spacing * 0.8\n",
    "    else:\n",
    "        tick_length = date_range * 0.01\n",
    "        volume_bar_width = date_range * 0.02\n",
    "    \n",
    "    # Draw HLC bars\n",
    "    for i, (date, high, low, close) in enumerate(zip(dates, data['high'], data['low'], data['close'])):\n",
    "        is_highlighted = highlighted_bars[i]\n",
    "        color = highlight_color if is_highlighted else 'black'\n",
    "        line_width = 1.2\n",
    "        \n",
    "        # Vertical line from low to high\n",
    "        ax1.plot([date, date], [low, high], color=color, linewidth=line_width, solid_capstyle='butt')\n",
    "        \n",
    "        # Horizontal tick mark for close (on the right side)\n",
    "        ax1.plot([date, date + tick_length], [close, close], color=color, \n",
    "                linewidth=line_width+0.5, solid_capstyle='butt')\n",
    "    \n",
    "    # Configure price chart\n",
    "    if semilog:\n",
    "        ax1.set_yscale('log')\n",
    "        scale_text = \"Semilog Scale\"\n",
    "    else:\n",
    "        scale_text = \"Linear Scale\"\n",
    "    \n",
    "    ax1.set_title(f'{symbol} {title} - {scale_text}', fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel('Price', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis based on timeframe\n",
    "    _format_datetime_axis(ax1, interval)\n",
    "    ax1.set_xlim(dates[0] - margin, dates[-1] + margin)\n",
    "    \n",
    "    # Volume chart\n",
    "    if show_volume and ax2 is not None:\n",
    "        volume_colors = [highlight_color if highlighted_bars[i] else 'orange' for i in range(len(dates))]\n",
    "        volume_edges = ['darkmagenta' if highlighted_bars[i] else 'darkorange' for i in range(len(dates))]\n",
    "        \n",
    "        ax2.bar(dates, data['volume'], width=volume_bar_width, alpha=0.6, \n",
    "                color=volume_colors, edgecolor=volume_edges)\n",
    "        ax2.set_ylabel('Volume', fontsize=12)\n",
    "        ax2.set_xlabel('Date', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        _format_datetime_axis(ax2, interval)\n",
    "        ax2.set_xlim(dates[0] - margin, dates[-1] + margin)\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "    else:\n",
    "        ax1.set_xlabel('Date', fontsize=12)\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='black', linewidth=2, label='High-Low Range'),\n",
    "        Line2D([0], [0], color='black', linewidth=3, label='Close Price (right tick)')\n",
    "    ]\n",
    "    \n",
    "    # Add highlighted bars to legend if any exist\n",
    "    if highlighted_bars.any():\n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0], color=highlight_color, linewidth=3, label=highlight_label)\n",
    "        )\n",
    "        highlight_count = highlighted_bars.sum()\n",
    "    else:\n",
    "        highlight_count = 0\n",
    "    \n",
    "    ax1.legend(handles=legend_elements, loc='upper left', title=scale_text)\n",
    "    \n",
    "    # Add pattern count if patterns exist\n",
    "    if highlight_count > 0:\n",
    "        ax1.text(0.99, 0.95, f'{highlight_label}: {highlight_count}', \n",
    "                transform=ax1.transAxes, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=10)\n",
    "    \n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def _format_datetime_axis(ax, interval):\n",
    "    \"\"\"Helper function to format datetime axis based on interval\"\"\"\n",
    "    if interval in ['1m', '5m', '15m', '30m']:\n",
    "        ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
    "    elif interval in ['1h', '2h', '4h', '6h', '12h']:\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    elif interval in ['1d', '3d']:\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    else:  # weekly, monthly\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Example usage and test function\n",
    "def example_usage():\n",
    "    \"\"\"Example showing how to use the plot_hlc_bars function\"\"\"\n",
    "    \n",
    "    # Create sample data\n",
    "    import datetime\n",
    "    dates = pd.date_range('2023-01-01', periods=100, freq='D')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate realistic OHLC data\n",
    "    closes = 100 + np.cumsum(np.random.randn(100) * 0.02)\n",
    "    highs = closes + np.random.rand(100) * 5\n",
    "    lows = closes - np.random.rand(100) * 5\n",
    "    volumes = np.random.rand(100) * 1000000\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'high': highs,\n",
    "        'low': lows, \n",
    "        'close': closes,\n",
    "        'volume': volumes\n",
    "    })\n",
    "    \n",
    "    # Create some random pattern detections\n",
    "    pattern_detected = np.random.choice([True, False], size=100, p=[0.1, 0.9])\n",
    "    \n",
    "    # Plot with pattern highlighting\n",
    "    fig = plot_hlc_bars(\n",
    "        data=data,\n",
    "        highlighted_bars=pattern_detected,\n",
    "        title=\"Daily Chart with Pattern Detection\",\n",
    "        symbol=\"EXAMPLE\",\n",
    "        interval=\"1d\",\n",
    "        semilog=False,\n",
    "        highlight_color=\"red\",\n",
    "        highlight_label=\"Detected Pattern\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Scanner integration example\n",
    "def scanner_integration_example():\n",
    "    \"\"\"Example of how to integrate with a scanner function\"\"\"\n",
    "    \n",
    "    def my_pattern_scanner(data):\n",
    "        \"\"\"\n",
    "        Example scanner function - replace with your actual scanner logic\n",
    "        Returns boolean array indicating pattern detection\n",
    "        \"\"\"\n",
    "        # Example: detect when close > 20-period moving average\n",
    "        ma20 = data['close'].rolling(20).mean()\n",
    "        pattern = (data['close'] > ma20) & (data['volume'] > data['volume'].rolling(10).mean())\n",
    "        return pattern.fillna(False)\n",
    "    \n",
    "    # Your data loading logic here\n",
    "    # data = load_your_data()  # Replace with actual data loading\n",
    "    \n",
    "    # For demo, create sample data\n",
    "    dates = pd.date_range('2023-01-01', periods=200, freq='D')\n",
    "    np.random.seed(42)\n",
    "    closes = 100 + np.cumsum(np.random.randn(200) * 0.02)\n",
    "    highs = closes + np.random.rand(200) * 3\n",
    "    lows = closes - np.random.rand(200) * 3\n",
    "    volumes = np.random.rand(200) * 1000000\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'high': highs,\n",
    "        'low': lows,\n",
    "        'close': closes,\n",
    "        'volume': volumes\n",
    "    })\n",
    "    \n",
    "    # Run your scanner\n",
    "    detected_patterns = my_pattern_scanner(data)\n",
    "    \n",
    "    # Plot only if patterns are detected\n",
    "    if detected_patterns.any():\n",
    "        print(f\"Patterns detected! Found {detected_patterns.sum()} occurrences\")\n",
    "        fig = plot_hlc_bars(\n",
    "            data=data,\n",
    "            highlighted_bars=detected_patterns,\n",
    "            title=\"Scanner Results\",\n",
    "            symbol=\"SCANNED_SYMBOL\",\n",
    "            interval=\"1d\",\n",
    "            highlight_color=\"lime\",\n",
    "            highlight_label=\"Scanner Hit\"\n",
    "        )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No patterns detected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run example\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff2c22-5671-487f-971c-fbd0c3d6a64b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dashboard\n",
    "from pyngrok import ngrok\n",
    "import os, sys, subprocess, time\n",
    "\n",
    "# --- ngrok auth (DO NOT commit your token) ---\n",
    "ngrok.set_auth_token(\"31mFDQNYuBuJw7mTKNxyDZLbZag_4Q1mV2EEggMBGYecRRZyF\")\n",
    "\n",
    "# Clean old tunnels\n",
    "try:\n",
    "    ngrok.kill()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Pick a port\n",
    "port = 8501\n",
    "\n",
    "# Launch Streamlit dashboard in the background\n",
    "# Ensure working directory contains dashboard.py\n",
    "cmd = [\"streamlit\", \"run\", \"dashboard.py\", \"--server.address\", \"0.0.0.0\", \"--server.port\", str(port)]\n",
    "proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Give Streamlit a moment to boot\n",
    "time.sleep(3)\n",
    "\n",
    "# Create public tunnel\n",
    "public_url = ngrok.connect(addr=port)\n",
    "print(\"Streamlit is running! Access it at:\", public_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f15de0-44e1-4090-bfa5-b10adc1dee19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Simple Historical Signal Scanner - Scan a specific pair or all pairs for historical signals\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our scanner module\n",
    "from direct_test import (\n",
    "    HistoricalSignalScanner, \n",
    "    save_signals_to_csv,\n",
    "    analyze_signals,\n",
    "    filter_signals\n",
    ")\n",
    "\n",
    "print(\"Historical Signal Scanner Ready!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN SCANNING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "async def scan_historical_signals(symbol=None, exchange=\"binance_futures\", timeframe=\"1d\", \n",
    "                                 strategies=None, max_symbols=None):\n",
    "    \"\"\"\n",
    "    Scan for historical signals\n",
    "    \n",
    "    Args:\n",
    "        symbol: Specific symbol to scan (e.g., \"BTCUSDT\") or None to scan all\n",
    "        exchange: Exchange to use (default: \"binance_futures\")\n",
    "        timeframe: Timeframe to analyze (default: \"1d\")\n",
    "        strategies: List of strategies or None for all available\n",
    "        max_symbols: Max number of symbols to scan when symbol=None (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: All detected signals\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default strategies if none provided\n",
    "    if strategies is None:\n",
    "        strategies = ['hbs_breakout', 'confluence', 'wedge_breakout', 'channel_breakout', \n",
    "                     'consolidation_breakout', 'sma50_breakout']\n",
    "    \n",
    "    # Default max symbols\n",
    "    if max_symbols is None:\n",
    "        max_symbols = 50\n",
    "    \n",
    "    scanner = HistoricalSignalScanner(exchange, timeframe)\n",
    "    \n",
    "    try:\n",
    "        await scanner.initialize()\n",
    "        print(f\"Initialized {exchange} scanner for {timeframe} timeframe\")\n",
    "        \n",
    "        if symbol:\n",
    "            # Scan specific symbol\n",
    "            print(f\"Scanning {symbol}...\")\n",
    "            results = await scanner.scan_symbol_historical(symbol, strategies)\n",
    "            df = scanner.create_signals_dataframe(results)\n",
    "            \n",
    "            if not df.empty:\n",
    "                filename = f\"{symbol}_{exchange}_{timeframe}_signals\"\n",
    "                save_signals_to_csv(df, filename)\n",
    "                print(f\"\\nFound {len(df)} signals for {symbol}\")\n",
    "                analyze_signals(df)\n",
    "            else:\n",
    "                print(f\"No signals found for {symbol}\")\n",
    "                \n",
    "        else:\n",
    "            # Scan all symbols\n",
    "            print(f\"Getting symbols from {exchange}...\")\n",
    "            symbols = await scanner.get_all_symbols(limit=max_symbols)\n",
    "            print(f\"Scanning {len(symbols)} symbols...\")\n",
    "            \n",
    "            results = await scanner.scan_multiple_symbols(symbols, strategies)\n",
    "            df = scanner.create_signals_dataframe(results)\n",
    "            \n",
    "            if not df.empty:\n",
    "                filename = f\"all_symbols_{exchange}_{timeframe}_{len(symbols)}pairs_signals\"\n",
    "                save_signals_to_csv(df, filename)\n",
    "                print(f\"\\nFound {len(df)} total signals across {len(symbols)} symbols\")\n",
    "                analyze_signals(df)\n",
    "                \n",
    "                # Show top symbols by signal count\n",
    "                print(f\"\\nTop symbols by signal count:\")\n",
    "                symbol_counts = df['symbol'].value_counts().head(10)\n",
    "                for sym, count in symbol_counts.items():\n",
    "                    print(f\"  {sym}: {count} signals\")\n",
    "            else:\n",
    "                print(f\"No signals found across {len(symbols)} symbols\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    finally:\n",
    "        await scanner.close()\n",
    "\n",
    "# =============================================================================\n",
    "# QUICK COMMANDS\n",
    "# =============================================================================\n",
    "\n",
    "async def scan_pair(symbol, exchange=\"binance_futures\", timeframe=\"1d\"):\n",
    "    \"\"\"Quick scan for a specific trading pair\"\"\"\n",
    "    return await scan_historical_signals(symbol=symbol, exchange=exchange, timeframe=timeframe)\n",
    "\n",
    "async def scan_all_pairs(exchange=\"binance_futures\", timeframe=\"1d\", max_pairs=50):\n",
    "    \"\"\"Quick scan for all trading pairs\"\"\"\n",
    "    return await scan_historical_signals(symbol=None, exchange=exchange, \n",
    "                                        timeframe=timeframe, max_symbols=max_pairs)\n",
    "\n",
    "\n",
    "df = await scan_historical_signals(exchange=\"mexc_spot\", timeframe=\"2d\",\n",
    "    symbol=\"NOS_USDT\",\n",
    "    strategies=['hbs_breakout']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1af8c6-534b-4a1e-9a0c-448cef9f1bd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#ASCII CHART + Testing mexc 1w candles\n",
    "\n",
    "import requests\n",
    "import datetime as dt\n",
    "\n",
    "MEXC_URL = \"https://api.mexc.com/api/v3/klines\"\n",
    "SYMBOL = \"STBLUSDT\"\n",
    "INTERVAL = \"1W\"         # MEXC uses uppercase W for weekly klines\n",
    "LIMIT = 50              # ask for more than 5 so we can always slice down\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"console-fetch/1.0 (+https://example.com)\"}\n",
    "\n",
    "def fetch_mexc_weekly(symbol=SYMBOL, limit=LIMIT):\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": INTERVAL,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    r = requests.get(MEXC_URL, params=params, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not isinstance(data, list) or not data:\n",
    "        raise RuntimeError(f\"Unexpected response: {data}\")\n",
    "\n",
    "    # Each kline: [openTime, open, high, low, close, volume, closeTime, ...]\n",
    "    candles = []\n",
    "    for row in data:\n",
    "        open_time_ms = int(row[0])\n",
    "        candles.append({\n",
    "            \"time\": dt.datetime.utcfromtimestamp(open_time_ms / 1000.0),\n",
    "            \"open\": float(row[1]),\n",
    "            \"high\": float(row[2]),\n",
    "            \"low\":  float(row[3]),\n",
    "            \"close\": float(row[4]),\n",
    "            \"volume\": float(row[5]),\n",
    "        })\n",
    "    return candles\n",
    "\n",
    "def ascii_candles(candles, width=40):\n",
    "    \"\"\"\n",
    "    Render simple ASCII candles for a small set of bars.\n",
    "    For each candle, draw a range [low..high] as a line, and mark:\n",
    "      O = open, C = close. Up candles also show '+' at close; down show '-'.\n",
    "    The scale is shared across the provided candles.\n",
    "    \"\"\"\n",
    "    if not candles:\n",
    "        return\n",
    "\n",
    "    lo = min(c[\"low\"] for c in candles)\n",
    "    hi = max(c[\"high\"] for c in candles)\n",
    "    if hi == lo:\n",
    "        hi = lo + 1e-9  # avoid zero range\n",
    "\n",
    "    def pos(price):\n",
    "        # map a price into [0..width-1]\n",
    "        return int(round((price - lo) / (hi - lo) * (width - 1)))\n",
    "\n",
    "    lines = []\n",
    "    for c in candles:\n",
    "        line = [\" \"] * width\n",
    "        low_p, high_p = pos(c[\"low\"]), pos(c[\"high\"])\n",
    "        open_p, close_p = pos(c[\"open\"]), pos(c[\"close\"])\n",
    "        # draw range\n",
    "        for i in range(low_p, high_p + 1):\n",
    "            line[i] = \"─\"\n",
    "        # mark open/close\n",
    "        line[open_p] = \"O\"\n",
    "        if close_p == open_p:\n",
    "            # nudge to show both if equal\n",
    "            close_p = min(width - 1, close_p + 1)\n",
    "        line[close_p] = \"C\"\n",
    "        # up/down marker at close\n",
    "        line[close_p] = \"+\" if c[\"close\"] >= c[\"open\"] else \"-\"\n",
    "        lines.append(\"\".join(line))\n",
    "    return lines, lo, hi\n",
    "\n",
    "def fmt_num(x):\n",
    "    # compact formatting for prices\n",
    "    if x >= 1000:\n",
    "        return f\"{x:,.0f}\"\n",
    "    return f\"{x:,.2f}\"\n",
    "\n",
    "def main():\n",
    "    candles = fetch_mexc_weekly()\n",
    "    last14 = candles[-14:]  # last 5 weekly bars\n",
    "\n",
    "    # Print a small info table\n",
    "    print(\"Last 5 weekly candles (MEXC spot, BTCUSDT, 1W):\")\n",
    "    print(\"Date (UTC)     |     Open       High        Low       Close        Vol\")\n",
    "    print(\"-\" * 74)\n",
    "    for c in last14:\n",
    "        print(f\"{c['time'].date()} | {fmt_num(c['open']).rjust(10)}  {fmt_num(c['high']).rjust(10)}  \"\n",
    "              f\"{fmt_num(c['low']).rjust(10)}  {fmt_num(c['close']).rjust(10)}  {fmt_num(c['volume']).rjust(10)}\")\n",
    "\n",
    "    print(\"\\nASCII chart (shared scale across the 14 bars):\")\n",
    "    chart, lo, hi = ascii_candles(last14, width=48)\n",
    "    # Put labels and lines together\n",
    "    for i, (c, line) in enumerate(zip(last14, chart)):\n",
    "        label = c[\"time\"].strftime(\"%Y-%m-%d\")\n",
    "        print(f\"{label} | {line} | O={fmt_num(c['open'])} C={fmt_num(c['close'])}\")\n",
    "\n",
    "    print(f\"\\nScale: low={fmt_num(lo)}  high={fmt_num(hi)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d459a-e948-4e9c-b015-46ee526359cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Breakout monitor for current & previous bar (HBS-aligned) =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, sys\n",
    "\n",
    "# Make sure your Project path is on sys.path (adjust if needed)\n",
    "sys.path.append('/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project/Project')\n",
    "\n",
    "# ---- Tunables (match your detector) ----\n",
    "HA_MA_LENGTH = 13\n",
    "AMA_PERIOD   = 2\n",
    "AMA_FAST     = 1\n",
    "AMA_SLOW     = 15\n",
    "JS_SMOOTH    = 13\n",
    "JS_POWER     = 5\n",
    "\n",
    "# ---------- Data fetch ----------\n",
    "def fetch_bybit_data(symbol, timeframe=\"1w\", limit=100):\n",
    "    symbol_clean = symbol.replace(\"USDT\", \"\")\n",
    "    url = \"https://api.bybit.com/v5/market/kline\"\n",
    "    params = {\"category\":\"spot\",\"symbol\":f\"{symbol_clean}USDT\",\"interval\":\"W\",\"limit\":limit}\n",
    "    r = requests.get(url, params=params, timeout=10)\n",
    "    data = r.json()\n",
    "    if data.get(\"retCode\") != 0:\n",
    "        print(f\"Bybit API error for {symbol}: {data.get('retMsg')}\")\n",
    "        return None\n",
    "    rows = [{\n",
    "        \"timestamp\": pd.to_datetime(int(k[0]), unit=\"ms\"),\n",
    "        \"open\": float(k[1]), \"high\": float(k[2]),\n",
    "        \"low\":  float(k[3]), \"close\": float(k[4]),\n",
    "        \"volume\": float(k[5]),\n",
    "    } for k in reversed(data[\"result\"][\"list\"])]\n",
    "    df = pd.DataFrame(rows).set_index(\"timestamp\")\n",
    "    return df[[\"open\",\"high\",\"low\",\"close\",\"volume\"]]\n",
    "\n",
    "# ---------- Helpers (mirror your detector) ----------\n",
    "def ema(s: pd.Series, span: int) -> pd.Series:\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def wma(s: pd.Series, length: int) -> pd.Series:\n",
    "    weights = np.arange(1, length + 1, dtype=float)\n",
    "    return s.rolling(length, min_periods=1).apply(\n",
    "        lambda x: np.dot(x, weights[-len(x):]) / weights[-len(x):].sum(),\n",
    "        raw=True\n",
    "    )\n",
    "\n",
    "def atr_wilder(h: pd.Series, l: pd.Series, c: pd.Series, length: int = 7) -> pd.Series:\n",
    "    prev_close = c.shift(1)\n",
    "    tr = pd.concat([(h - l).abs(), (h - prev_close).abs(), (l - prev_close).abs()], axis=1).max(axis=1)\n",
    "    return tr.ewm(alpha=1/length, adjust=False).mean()\n",
    "\n",
    "def ama(series, period=2, period_fast=2, period_slow=30, epsilon=1e-10):\n",
    "    n = period + 1\n",
    "    src = np.asarray(series, dtype=float)\n",
    "    hh = pd.Series(src).rolling(window=n, min_periods=1).max().values\n",
    "    ll = pd.Series(src).rolling(window=n, min_periods=1).min().values\n",
    "    mltp = np.where((hh - ll) != 0, np.abs(2 * src - ll - hh) / (hh - ll + epsilon), 0.0)\n",
    "    sc_fastest = 2 / (period_fast + 1)\n",
    "    sc_slowest = 2 / (period_slow + 1)\n",
    "    sc = (mltp * (sc_fastest - sc_slowest) + sc_slowest) ** 2\n",
    "    sc = np.nan_to_num(sc, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    out = np.zeros_like(src)\n",
    "    out[:period] = src[:period]\n",
    "    for i in range(period, len(src)):\n",
    "        out[i] = out[i - 1] + sc[i] * (src[i] - out[i - 1])\n",
    "    return pd.Series(out)\n",
    "\n",
    "def jsmooth(src, smooth, power):\n",
    "    s = np.asarray(src, dtype=float)\n",
    "    beta = 0.45 * (smooth - 1) / (0.45 * (smooth - 1) + 2)\n",
    "    alpha = beta ** power\n",
    "    n = len(s)\n",
    "    jma = np.zeros(n)\n",
    "    e0 = np.zeros(n); e1 = np.zeros(n); e2 = np.zeros(n)\n",
    "    e0[0] = s[0]; e1[0] = 0.0; e2[0] = 0.0; jma[0] = s[0]\n",
    "    for i in range(1, n):\n",
    "        e0[i] = (1 - alpha) * s[i] + alpha * e0[i - 1]\n",
    "        e1[i] = (s[i] - e0[i]) * (1 - beta) + beta * e1[i - 1]\n",
    "        e2[i] = (e0[i] - jma[i - 1]) * ((1 - alpha) ** 2) + (alpha ** 2) * e2[i - 1]\n",
    "        jma[i] = jma[i - 1] + e2[i]\n",
    "    return pd.Series(jma)\n",
    "\n",
    "def pivot_calc(osc: list[float], LBL=2, LBR=2, highlow='high') -> list[float]:\n",
    "    n = len(osc)\n",
    "    piv = [np.nan]*n\n",
    "    if n == 0: return piv\n",
    "    for center in range(LBL + LBR, n):\n",
    "        ref_index = center - LBR\n",
    "        ref = osc[ref_index]\n",
    "        left, right = ref_index - LBL, ref_index + LBR\n",
    "        if left < 0 or right >= n: continue\n",
    "        ok = True\n",
    "        for j in range(left, right + 1):\n",
    "            if j == ref_index: continue\n",
    "            if highlow == 'high':\n",
    "                if osc[j] >= ref: ok = False; break\n",
    "            else:\n",
    "                if osc[j] <= ref: ok = False; break\n",
    "        if ok: piv[ref_index] = ref\n",
    "    return piv\n",
    "\n",
    "# ---------- Build the same columns your (updated) detector uses ----------\n",
    "def build_detector_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[\"atr_7\"] = atr_wilder(d[\"high\"], d[\"low\"], d[\"close\"], 7)\n",
    "\n",
    "    # lac and AMA(2, fast=1, slow=15)\n",
    "    d[\"lac\"] = (d[\"open\"] + d[\"close\"])/2 + (((d[\"close\"] - d[\"open\"]) / (d[\"high\"] - d[\"low\"] + 1e-6)) * ((d[\"close\"] - d[\"open\"]).abs()/2))\n",
    "    d[\"habclose\"] = ama(d[\"lac\"].values, period=AMA_PERIOD, period_fast=AMA_FAST, period_slow=AMA_SLOW).values\n",
    "\n",
    "    # recursive habopen\n",
    "    ho = np.zeros(len(d), dtype=float)\n",
    "    ho[0] = (d[\"open\"].iloc[0] + d[\"close\"].iloc[0]) / 2.0\n",
    "    for i in range(1, len(d)):\n",
    "        ho[i] = (ho[i-1] + d[\"habclose\"].iloc[i-1]) / 2.0\n",
    "    d[\"habopen\"] = ho\n",
    "\n",
    "    # HA high/low\n",
    "    d[\"habhigh\"] = pd.concat([d[\"high\"], d[\"habopen\"], d[\"habclose\"]], axis=1).max(axis=1)\n",
    "    d[\"hablow\"]  = pd.concat([d[\"low\"],  d[\"habopen\"], d[\"habclose\"]], axis=1).min(axis=1)\n",
    "\n",
    "    # jsmooth and s_habhigh/s_hablow\n",
    "    d[\"jsmooth_habhigh\"] = jsmooth(d[\"habhigh\"].values, JS_SMOOTH, JS_POWER).values\n",
    "    d[\"jsmooth_hablow\"]  = jsmooth(d[\"hablow\"].values,  JS_SMOOTH, JS_POWER).values\n",
    "\n",
    "    ema_high = ema(pd.Series(d[\"jsmooth_habhigh\"], index=d.index), span=HA_MA_LENGTH)\n",
    "    wma_high = wma(pd.Series(d[\"jsmooth_habhigh\"], index=d.index), length=HA_MA_LENGTH)\n",
    "    d[\"s_habhigh\"] = (ema_high + wma_high) / 2\n",
    "    d[\"s_hablow\"]  = ema(pd.Series(d[\"jsmooth_hablow\"], index=d.index), span=HA_MA_LENGTH)\n",
    "\n",
    "    # Pivot-high level (+LBR shift/ffill)\n",
    "    ph = pivot_calc(d[\"high\"].astype(float).tolist(), 2, 2, \"high\")\n",
    "    d[\"ph\"] = pd.Series(ph, index=d.index).shift(2)\n",
    "    d[\"ph_range\"] = d[\"ph\"].ffill()\n",
    "\n",
    "    # Levels & booleans\n",
    "    d[\"level_sh\"] = d[\"s_habhigh\"] + 0.1*d[\"atr_7\"]             # breakout_condition level\n",
    "    d[\"level_ph\"] = d[\"ph_range\"]   + 0.3*d[\"atr_7\"]             # UpWeGo level\n",
    "\n",
    "    d[\"breakout_condition\"] = d[\"close\"] > d[\"level_sh\"]\n",
    "    d[\"breakout_prev\"]      = d[\"breakout_condition\"].shift(1).fillna(False)\n",
    "\n",
    "    d[\"breakup\"] = d[\"close\"] >= d[\"level_ph\"]                   # pivot breakout\n",
    "    bu1 = d[\"breakup\"].shift(1).fillna(False)\n",
    "    bu2 = d[\"breakup\"].shift(2).fillna(False)\n",
    "    pivot_updated = d[\"ph_range\"].ne(d[\"ph_range\"].shift(1)).fillna(False)\n",
    "    d[\"upwego\"] = d[\"breakup\"] & ((d[\"breakup\"] & ~bu1) | (bu1 & ~bu2) | pivot_updated)\n",
    "\n",
    "    return d\n",
    "\n",
    "# ---------- Monitor two bars: -2 (closed) and -1 (current) ----------\n",
    "def monitor_breakout(symbol: str):\n",
    "    df = fetch_bybit_data(symbol, \"1w\", 100)\n",
    "    if df is None or len(df) < 10:\n",
    "        print(\"No data.\")\n",
    "        return\n",
    "    d = build_detector_columns(df)\n",
    "\n",
    "    i_prev = len(d) - 2  # -2\n",
    "    i_curr = len(d) - 1  # -1\n",
    "\n",
    "    def row(i):\n",
    "        return {\n",
    "            \"time\": d.index[i],\n",
    "            \"close\": float(d[\"close\"].iloc[i]),\n",
    "            \"s_habhigh\": float(d[\"s_habhigh\"].iloc[i]),\n",
    "            \"atr7\": float(d[\"atr_7\"].iloc[i]),\n",
    "            \"level_sh\": float(d[\"level_sh\"].iloc[i]),\n",
    "            \"breakout_cond\": bool(d[\"breakout_condition\"].iloc[i]),\n",
    "            \"breakout_prev\": bool(d[\"breakout_prev\"].iloc[i]),\n",
    "            \"PH_range\": float(d[\"ph_range\"].iloc[i]) if not pd.isna(d[\"ph_range\"].iloc[i]) else None,\n",
    "            \"level_ph\": float(d[\"level_ph\"].iloc[i]) if not pd.isna(d[\"level_ph\"].iloc[i]) else None,\n",
    "            \"breakup\": bool(d[\"breakup\"].iloc[i]),\n",
    "            \"upwego\": bool(d[\"upwego\"].iloc[i]),\n",
    "        }\n",
    "\n",
    "    print(f\"\\n=== {symbol} @ 1W  (monitoring -2 and -1) ===\")\n",
    "    print(f\"Settings: AMA(period={AMA_PERIOD}, fast={AMA_FAST}, slow={AMA_SLOW}), \"\n",
    "          f\"HA_MA_LENGTH={HA_MA_LENGTH}, JS(smooth={JS_SMOOTH}, power={JS_POWER})\")\n",
    "    print(\"Last 4 bars:\")\n",
    "    print(df.tail(4), \"\\n\")\n",
    "\n",
    "    R_prev = row(i_prev)\n",
    "    R_curr = row(i_curr)\n",
    "\n",
    "    def p(R, tag):\n",
    "        print(f\"[{tag}] {R['time']}\")\n",
    "        print(f\"  close={R['close']:.8f}\")\n",
    "        print(f\"  s_habhigh={R['s_habhigh']:.8f}  atr7={R['atr7']:.8f}\")\n",
    "        print(f\"  level_sh=s_habhigh+0.1*atr7 = {R['level_sh']:.8f}\")\n",
    "        print(f\"  breakout_condition  = {R['breakout_cond']}   (prev={R['breakout_prev']})\")\n",
    "        print(f\"  ph_range={R['PH_range']}  level_ph={R['level_ph']}\")\n",
    "        print(f\"  breakup(pivot>=)    = {R['breakup']}  upwego={R['upwego']}\\n\")\n",
    "\n",
    "    p(R_prev, \"BAR -2 (last closed)\")\n",
    "    p(R_curr, \"BAR -1 (current)\")\n",
    "\n",
    "    # Support set @ -2 (mirrors detector)\n",
    "    conds_prev = {\n",
    "        \"upwego\": d[\"upwego\"].iloc[i_prev],\n",
    "        \"atr_trend\": (d[\"atr_7\"].iloc[i_prev] - d[\"atr_7\"].iloc[i_prev-1]) >= 0.01*max(1e-12, d[\"atr_7\"].iloc[i_prev-1]),\n",
    "        \"ma_bull\": ema(d[\"habclose\"],5).iloc[i_prev] > ema(d[\"habopen\"],10).iloc[i_prev],\n",
    "        \"ha_momentum\": d[\"habclose\"].iloc[i_prev] > d[\"habopen\"].iloc[i_prev],\n",
    "        \"flagup\": (d[\"high\"].iloc[i_prev] > d[\"high\"].iloc[i_prev-1]) and \\\n",
    "                  ((d[\"high\"].iloc[i_prev]-d[\"close\"].iloc[i_prev]) < (d[\"close\"].iloc[i_prev]-d[\"low\"].iloc[i_prev])),\n",
    "    }\n",
    "    print(\"Support set @ -2:\", {k: bool(v) for k,v in conds_prev.items()})\n",
    "    print(\"Breakout edge @ -2 (False->True)?\", bool(d[\"breakout_condition\"].iloc[i_prev] and not d[\"breakout_prev\"].iloc[i_prev]))\n",
    "\n",
    "# ---- run ----\n",
    "monitor_breakout(\"ETHFIUSDT\")\n",
    "# Try also: monitor_breakout(\"SOSOUSDT\"); monitor_breakout(\"ATHUSDT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a30e9-779a-4d4c-9740-70f078638bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
