{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab56392-9bd5-48da-aef6-5dddab880329",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f094aaab-b05b-4019-b8f9-592a5076f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  RUNNING PARALLEL MULTI-TIMEFRAME SCAN ON ALL EXCHANGES\n",
      "================================================================================\n",
      "\n",
      "• Exchanges: binance_spot\n",
      "• Timeframes: 1w\n",
      "• Strategies: hbs_breakout\n",
      "• Check bar: current\n",
      "• Notifications: Enabled\n",
      "• Recipients: default\n",
      "• Save to CSV: Disabled\n",
      "• Start time: 14:15:53\n",
      "\n",
      "Fetching market data...\n",
      "\n",
      "Processing timeframe: 1w\n",
      "1w: 1 FAST, 0 SLOW exchanges\n",
      "\n",
      "================================================================================\n",
      "  PHASE: FAST 1w (1 exchanges)\n",
      "================================================================================\n",
      "\n",
      "[14:15:53] Starting scan on binance_spot for 1w timeframe (check_bar=current)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added /home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 421 markets on Binance Spot for 1w timeframe\n",
      "Processing 421 symbols with parallel strategies (batch size: 25)\n",
      "[14:16:51] ✓ Completed binance_spot scan: 0 signals found\n",
      "\n",
      "================================================================================\n",
      "  PHASE: SLOW 1w (0 exchanges)\n",
      "================================================================================\n",
      "\n",
      "No exchanges in this phase.\n",
      "\n",
      "================================================================================\n",
      "  PARALLEL MULTI-TIMEFRAME MULTI-EXCHANGE SCAN RESULTS\n",
      "================================================================================\n",
      "\n",
      "Total signals found across all exchanges and timeframes: 0\n",
      "Start time: 14:15:53\n",
      "End time: 14:16:51\n",
      "Duration: 0:00:58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run Simple Parallel Scan for Test Bar\n",
    "\n",
    "This script runs the test bar scan across multiple exchanges in parallel\n",
    "using a simplified parallel scanning approach that avoids console output issues.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "print(f\"✓ Added {os.getcwd()} to sys.path\")\n",
    "\n",
    "# Import the simple parallel scanner\n",
    "from run_parallel_scanner import run_parallel_exchanges, run_parallel_multi_timeframes_all_exchanges, sf_exchanges_1w\n",
    "from scanner.main import run_scanner, kline_cache\n",
    "\n",
    "# Define exchanges\n",
    "futures_exchanges = [\"binance_futures\", \"bybit_futures\", \"mexc_futures\", \"gateio_futures\"]\n",
    "spot_exchanges = [\"binance_spot\", \"bybit_spot\", \"kucoin_spot\", \"mexc_spot\", \"gateio_spot\"]\n",
    "spot_exchanges_1w = [\"binance_spot\", \"bybit_spot\", \"gateio_spot\"] + sf_exchanges_1w\n",
    "fast_exchanges = [\"binance_futures\", \"binance_spot\", \"bybit_spot\", \"gateio_spot\"]\n",
    "slow_exchanges = [\"kucoin_spot\", \"mexc_spot\", \"mexc_futures\"]\n",
    "\n",
    "async def main():\n",
    "    # Clear cache for fresh data\n",
    "    kline_cache.clear()\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Run parallel scan for test bar strategy on spot exchanges\n",
    "    result = await run_parallel_exchanges(\n",
    "        timeframe=\"4h\",                    # Example timeframe\n",
    "        strategies=[\"hbs_breakout\", \"test_bar\", \"consolidation_breakout\", \"sma50_breakout\", \"trend_breakout\", \"pin_up\"],\n",
    "        # strategies=[\"reversal_bar\"],       \n",
    "        exchanges=spot_exchanges,          # Spot exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for Telegram notifications\n",
    "        send_telegram=True,                # Enable Telegram notifications\n",
    "        min_volume_usd=None,               # Use default volume threshold\n",
    "        save_to_csv=True,                   # Enable saving to CSV\n",
    "        check_bar=\"both\"                   # NEW PARAMETER: \"last_closed\", \"current\", or \"both\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Run multi-timeframe parallel scan\n",
    "    result = await run_parallel_multi_timeframes_all_exchanges(\n",
    "        timeframes=[\"1w\"],     # Multiple timeframes0\n",
    "        strategies= [\"hbs_breakout\"], #[\"confluence\", \"consolidation_breakout\", \"channel_breakout\", \"sma50_breakout\", \"loaded_bar\", \"pin_up\", \"trend_breakout\"], #\"engulfingReversal\", \"vs_wakeup],        # Strategies to scan\n",
    "        exchanges= [\"binance_spot\"], #fast_exchanges,          # Exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for notifications\n",
    "        send_telegram=True,                # Enable notifications\n",
    "        min_volume_usd=None,               # Use default volume threshold\n",
    "        save_to_csv=False,                   # Enable saving to CSV\n",
    "        check_bar=\"current\"                   # NEW PARAMETER: \"last_closed\", \"current\", or \"both\"\n",
    "    )\n",
    "     #\"\"\"\n",
    "    \n",
    "    print(\"Scan completed!\")\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac137db-fee2-4cf5-bd34-d20b90b9ccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed problematic directory: /home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project/Project\n",
      "Fixed path structure. Restart kernel now.\n"
     ]
    }
   ],
   "source": [
    "# Remove the problematic nested directory completely\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "problematic_path = '/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project/Project'\n",
    "if os.path.exists(problematic_path):\n",
    "    shutil.rmtree(problematic_path)\n",
    "    print(f\"Removed problematic directory: {problematic_path}\")\n",
    "\n",
    "# Clear sys.path of any references to the nested directory\n",
    "import sys\n",
    "sys.path = [p for p in sys.path if 'Project/Project' not in p]\n",
    "\n",
    "# Ensure only the correct path is in sys.path\n",
    "correct_path = '/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project'\n",
    "if correct_path not in sys.path:\n",
    "    sys.path.insert(0, correct_path)\n",
    "\n",
    "print(\"Fixed path structure. Restart kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c86d73-1174-45d1-b215-842f2609b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify OHLCV data from SF\n",
    "\n",
    "from exchanges.sf_pairs_service import SFPairsService\n",
    "import pandas as pd\n",
    "\n",
    "sf_service = SFPairsService()\n",
    "\n",
    "symbol = \"BTC\"\n",
    "quote = \"USDT\"\n",
    "exchange = \"Kucoin\"   # or \"Mexc\"\n",
    "timeframe = \"1w\"      # weekly\n",
    "limit = 2             # how many candles to fetch\n",
    "\n",
    "# Fetch OHLCV data\n",
    "raw_data = sf_service.get_ohlcv_for_pair(symbol, quote, exchange, timeframe, limit)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "# Normalize datetime\n",
    "if 'datetime' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.set_index('datetime')\n",
    "elif 'time' in df.columns:\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    df = df.set_index('time')\n",
    "\n",
    "# Keep only required columns\n",
    "df = df[['open', 'high', 'low', 'close', 'volume']].astype(float)\n",
    "\n",
    "# Print with timeframe in console\n",
    "print(f\"\\n📊 {symbol}/{quote} OHLCV ({timeframe}) on {exchange}\")\n",
    "print(df.tail(1))  # last row = current bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac56a-df34-4974-8cd6-8fb33f8d477a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Binance BTC dominated pairs - Confluence Scanner with Direct API\n",
    "\n",
    "from telegram.ext import Application\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import os\n",
    "import html\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# Add project path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from custom_strategies import detect_confluence\n",
    "\n",
    "class BinanceBTCConfluenceScanner:\n",
    "    def __init__(self, telegram_token, telegram_chat_id, timeframe, offset=1):\n",
    "        self.telegram_token = telegram_token\n",
    "        self.telegram_chat_id = telegram_chat_id\n",
    "        self.telegram_app = None\n",
    "        self.exchange = \"Binance\"\n",
    "        self.timeframe = timeframe\n",
    "        self.quote_currency = \"BTC\"\n",
    "        self.offset = offset\n",
    "        self.base_url = \"https://api.binance.com\"\n",
    "        self.session = None\n",
    "        \n",
    "        # Binance timeframe mapping - All available intervals\n",
    "        self.timeframe_map = {\n",
    "            # Minutes\n",
    "            \"1m\": \"1m\",\n",
    "            \"3m\": \"3m\", \n",
    "            \"5m\": \"5m\",\n",
    "            \"15m\": \"15m\",\n",
    "            \"30m\": \"30m\",\n",
    "            # Hours\n",
    "            \"1h\": \"1h\",\n",
    "            \"2h\": \"2h\",\n",
    "            \"4h\": \"4h\",\n",
    "            \"6h\": \"6h\",\n",
    "            \"8h\": \"8h\",\n",
    "            \"12h\": \"12h\",\n",
    "            # Days\n",
    "            \"1d\": \"1d\",\n",
    "            \"2d\": \"2d\",\n",
    "            \"3d\": \"3d\",\n",
    "            # Weeks/Months\n",
    "            \"1w\": \"1w\",\n",
    "            \"1M\": \"1M\"\n",
    "        }\n",
    "        \n",
    "    async def init_session(self):\n",
    "        \"\"\"Initialize aiohttp session\"\"\"\n",
    "        if self.session is None:\n",
    "            self.session = aiohttp.ClientSession()\n",
    "            \n",
    "    async def close_session(self):\n",
    "        \"\"\"Close aiohttp session\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "            self.session = None\n",
    "        \n",
    "    async def init_telegram(self):\n",
    "        if self.telegram_app is None:\n",
    "            self.telegram_app = Application.builder().token(self.telegram_token).build()\n",
    "\n",
    "    async def get_btc_pairs(self):\n",
    "        \"\"\"Get all BTC trading pairs from Binance\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.base_url}/api/v3/exchangeInfo\"\n",
    "            async with self.session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    \n",
    "                    # Filter for BTC pairs that are actively trading\n",
    "                    btc_pairs = []\n",
    "                    for symbol_info in data['symbols']:\n",
    "                        if (symbol_info['quoteAsset'] == 'BTC' and \n",
    "                            symbol_info['status'] == 'TRADING' and\n",
    "                            symbol_info['isSpotTradingAllowed']):\n",
    "                            \n",
    "                            btc_pairs.append({\n",
    "                                'symbol': symbol_info['symbol'],\n",
    "                                'baseAsset': symbol_info['baseAsset'],\n",
    "                                'quoteAsset': symbol_info['quoteAsset']\n",
    "                            })\n",
    "                    \n",
    "                    return btc_pairs\n",
    "                else:\n",
    "                    logging.error(f\"Error fetching exchange info: {response.status}\")\n",
    "                    return []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching BTC pairs: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    async def get_klines(self, symbol, interval, limit=100):\n",
    "        \"\"\"Get kline/candlestick data from Binance\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.base_url}/api/v3/klines\"\n",
    "            params = {\n",
    "                'symbol': symbol,\n",
    "                'interval': interval,\n",
    "                'limit': limit\n",
    "            }\n",
    "            \n",
    "            async with self.session.get(url, params=params) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    \n",
    "                    # Convert to DataFrame\n",
    "                    df = pd.DataFrame(data, columns=[\n",
    "                        'open_time', 'open', 'high', 'low', 'close', 'volume',\n",
    "                        'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "                        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "                    ])\n",
    "                    \n",
    "                    # Convert timestamps to datetime\n",
    "                    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "                    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')\n",
    "                    \n",
    "                    # Convert OHLCV to numeric\n",
    "                    for col in ['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    \n",
    "                    # Set datetime index\n",
    "                    df.set_index('open_time', inplace=True)\n",
    "                    \n",
    "                    # Select only OHLCV columns needed for confluence\n",
    "                    # Note: Using quote_asset_volume as it's the volume in BTC\n",
    "                    result_df = df[['open', 'high', 'low', 'close', 'quote_asset_volume']].copy()\n",
    "                    result_df.rename(columns={'quote_asset_volume': 'volume'}, inplace=True)\n",
    "                    \n",
    "                    return result_df\n",
    "                    \n",
    "                elif response.status == 429:\n",
    "                    # Rate limit hit, wait a bit\n",
    "                    await asyncio.sleep(1)\n",
    "                    return None\n",
    "                else:\n",
    "                    return None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching klines for {symbol}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    async def send_telegram_alert(self, results):\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            message = f\"🚨 Confluence Detection - Binance BTC Pairs {self.timeframe}\\n\\n\"\n",
    "            \n",
    "            # Map timeframe to TradingView format - Extended mapping\n",
    "            tv_timeframe_map = {\n",
    "                # Minutes\n",
    "                \"1m\": \"1\", \"3m\": \"3\", \"5m\": \"5\", \"15m\": \"15\", \"30m\": \"30\",\n",
    "                # Hours  \n",
    "                \"1h\": \"60\", \"2h\": \"120\", \"4h\": \"240\", \"6h\": \"360\", \"8h\": \"480\", \"12h\": \"720\",\n",
    "                # Days\n",
    "                \"1d\": \"1D\", \"2d\": \"2D\", \"3d\": \"3D\",\n",
    "                # Weeks/Months\n",
    "                \"1w\": \"1W\", \"1M\": \"1M\"\n",
    "            }\n",
    "            tv_timeframe = tv_timeframe_map.get(self.timeframe.lower(), self.timeframe)\n",
    "            \n",
    "            for result in results:\n",
    "                formatted_symbol = result['symbol']\n",
    "                tv_link = f\"https://www.tradingview.com/chart/?symbol=BINANCE:{formatted_symbol}&interval={tv_timeframe}\"\n",
    "                \n",
    "                # Escape HTML entities in the URL\n",
    "                escaped_link = html.escape(tv_link)\n",
    "                \n",
    "                # Format according to BTC specifications\n",
    "                time_str = \"\"\n",
    "                if result.get('timestamp') is not None:\n",
    "                    time_str = f\"Time: {result['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "                \n",
    "                message += (\n",
    "                    f\"Symbol: {result['symbol']}\\n\"\n",
    "                    f\"{time_str}\"\n",
    "                    f\"Volume BTC: ₿{result['volume_btc']:,.4f}\\n\"\n",
    "                    f\"Close: <a href='{escaped_link}'>₿{result['close']:.8f}</a>\\n\"\n",
    "                    f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                    f\"Close Off Low: {result['close_off_low']:.1f}%\\n\"\n",
    "                    f\"Momentum: {result['momentum_score']:.4f}\\n\"\n",
    "                    f\"{'='*30}\\n\"\n",
    "                )\n",
    "            \n",
    "            # Split message more carefully to avoid breaking HTML tags\n",
    "            max_length = 4000\n",
    "            \n",
    "            if len(message) > max_length:\n",
    "                # Split at natural breaks (between results) to avoid breaking HTML\n",
    "                sections = message.split('='*30 + '\\n')\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for section in sections:\n",
    "                    if len(current_chunk + section + '='*30 + '\\n') > max_length:\n",
    "                        if current_chunk:\n",
    "                            await self.telegram_app.bot.send_message(\n",
    "                                chat_id=self.telegram_chat_id,\n",
    "                                text=current_chunk.strip(),\n",
    "                                parse_mode='HTML',\n",
    "                                disable_web_page_preview=True\n",
    "                            )\n",
    "                        current_chunk = section + '\\n'\n",
    "                    else:\n",
    "                        current_chunk += section + '='*30 + '\\n'\n",
    "                \n",
    "                # Send remaining chunk\n",
    "                if current_chunk.strip():\n",
    "                    await self.telegram_app.bot.send_message(\n",
    "                        chat_id=self.telegram_chat_id,\n",
    "                        text=current_chunk.strip(),\n",
    "                        parse_mode='HTML',\n",
    "                        disable_web_page_preview=True\n",
    "                    )\n",
    "            else:\n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=message,\n",
    "                    parse_mode='HTML',\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sending Telegram alert: {str(e)}\")\n",
    "            \n",
    "            # Fallback: send without HTML formatting\n",
    "            try:\n",
    "                simple_message = f\"🚨 Confluence Detection - Binance BTC Pairs {self.timeframe}\\n\\n\"\n",
    "                for result in results:\n",
    "                    simple_message += (\n",
    "                        f\"Symbol: {result['symbol']}\\n\"\n",
    "                        f\"Volume BTC: ₿{result['volume_btc']:,.4f}\\n\"\n",
    "                        f\"Close: ₿{result['close']:.8f}\\n\"\n",
    "                        f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                        f\"Components: Vol={result['high_volume']}, Spread={result['spread_breakout']}, Mom={result['momentum_breakout']}\\n\\n\"\n",
    "                    )\n",
    "                \n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=simple_message,\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "            except Exception as fallback_error:\n",
    "                logging.error(f\"Fallback Telegram send also failed: {str(fallback_error)}\")\n",
    "\n",
    "    def scan_single_market(self, pair, df):\n",
    "        \"\"\"Scan a single market for Confluence pattern in the specified bar\"\"\"\n",
    "        try:\n",
    "            if df is None or len(df) < 50:  # Need enough data for confluence\n",
    "                return None\n",
    "            \n",
    "            # Calculate which bar to check based on offset\n",
    "            check_bar = -(self.offset + 1)  # offset=0 means current bar (-1), offset=1 means last closed (-2), etc.\n",
    "            \n",
    "            # Run confluence detection\n",
    "            detected, result = detect_confluence(df, check_bar=check_bar)\n",
    "            \n",
    "            if detected:\n",
    "                # Get the target bar values\n",
    "                target_close = df['close'].iloc[check_bar]\n",
    "                target_volume = df['volume'].iloc[check_bar]  # This is already in BTC\n",
    "                \n",
    "                confluence_result = {\n",
    "                    'symbol': pair['symbol'],\n",
    "                    'volume_btc': float(target_volume),\n",
    "                    'close': float(target_close),\n",
    "                    'volume': float(target_volume),\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                    'bar_range': result['bar_range'],\n",
    "                    'timestamp': df.index[check_bar] if hasattr(df.index, '__getitem__') else None\n",
    "                }\n",
    "                return confluence_result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pair['symbol']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    async def scan_all_markets(self):\n",
    "        \"\"\"Scan all BTC markets for Confluence pattern\"\"\"\n",
    "        await self.init_telegram()\n",
    "        \n",
    "        try:\n",
    "            # Define volume thresholds for BTC pairs based on timeframe\n",
    "            volume_thresholds = {\n",
    "                # Minutes - higher volume needed for shorter timeframes\n",
    "                \"1m\": 0.01,   \"3m\": 0.02,   \"5m\": 0.03,\n",
    "                \"15m\": 0.05,  \"30m\": 0.08,\n",
    "                # Hours\n",
    "                \"1h\": 0.1,    \"2h\": 0.15,   \"4h\": 0.2,\n",
    "                \"6h\": 0.25,   \"8h\": 0.3,    \"12h\": 0.35,\n",
    "                # Days\n",
    "                \"1d\": 0.4,    \"2d\": 0.8,    \"3d\": 1.2,\n",
    "                # Weeks/Months\n",
    "                \"1w\": 2.0,    \"1M\": 8.0\n",
    "            }\n",
    "            min_volume = volume_thresholds.get(self.timeframe.lower(), 0.1)  # Default 0.1 BTC\n",
    "            \n",
    "            # Create offset description\n",
    "            if self.offset == 0:\n",
    "                offset_desc = \"current candle\"\n",
    "            elif self.offset == 1:\n",
    "                offset_desc = \"last closed candle\"\n",
    "            else:\n",
    "                offset_desc = f\"{self.offset} candles ago\"\n",
    "            \n",
    "            print(f\"Scanning Binance BTC pairs for Confluence patterns in {offset_desc}...\")\n",
    "            print(f\"Timeframe: {self.timeframe}\")\n",
    "            print(f\"Minimum volume threshold: ₿{min_volume:.2f}\")\n",
    "            \n",
    "            # Get all BTC pairs from Binance\n",
    "            print(\"Fetching BTC pairs from Binance...\")\n",
    "            btc_pairs = await self.get_btc_pairs()\n",
    "            \n",
    "            if not btc_pairs:\n",
    "                print(\"No BTC pairs found or error fetching pairs\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"Found {len(btc_pairs)} BTC pairs to scan\")\n",
    "            \n",
    "            # Filter out stablecoins and obvious non-trading pairs\n",
    "            filtered_pairs = []\n",
    "            skip_tokens = ['USDT', 'USDC', 'BUSD', 'DAI', 'TUSD', 'USDD', 'FDUSD']\n",
    "            \n",
    "            for pair in btc_pairs:\n",
    "                if pair['baseAsset'] not in skip_tokens:\n",
    "                    filtered_pairs.append(pair)\n",
    "            \n",
    "            print(f\"After filtering: {len(filtered_pairs)} pairs to scan\")\n",
    "            \n",
    "            # Get Binance timeframe - fallback to 1d if not found\n",
    "            binance_interval = self.timeframe_map.get(self.timeframe.lower(), \"1d\")\n",
    "            \n",
    "            # Process all pairs with progress bar\n",
    "            all_results = []\n",
    "            successful_scans = 0\n",
    "            \n",
    "            with tqdm(total=len(filtered_pairs), desc=\"Scanning markets\") as pbar:\n",
    "                for pair in filtered_pairs:\n",
    "                    try:\n",
    "                        # Get OHLCV data from Binance\n",
    "                        df = await self.get_klines(pair['symbol'], binance_interval, 100)\n",
    "                        \n",
    "                        if df is None or len(df) < 50:\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        successful_scans += 1\n",
    "                        target_idx = -(self.offset + 1)\n",
    "                        \n",
    "                        # Update progress bar with current symbol\n",
    "                        pbar.set_description(f\"Scanning: {pair['symbol']} ({len(df)} candles)\")\n",
    "                        \n",
    "                        # Check volume threshold for the target candle\n",
    "                        try:\n",
    "                            target_candle_volume = float(df['volume'].iloc[target_idx])  # Already in BTC\n",
    "                            \n",
    "                            # Only process if volume meets threshold\n",
    "                            if target_candle_volume >= min_volume:\n",
    "                                result = self.scan_single_market(pair, df)\n",
    "                                if result:\n",
    "                                    all_results.append(result)\n",
    "                                    print(f\"Found Confluence: {pair['symbol']} 🎯\")\n",
    "                        except (IndexError, ValueError):\n",
    "                            pass  # Skip if we can't calculate volume\n",
    "                        \n",
    "                        # Add small delay to respect rate limits\n",
    "                        await asyncio.sleep(0.1)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        if \"429\" in str(e):\n",
    "                            # Rate limit - add longer delay\n",
    "                            await asyncio.sleep(2)\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            \n",
    "            print(f\"Successfully scanned {successful_scans}/{len(filtered_pairs)} pairs\")\n",
    "            \n",
    "            # Sort by volume\n",
    "            all_results.sort(key=lambda x: x['volume_btc'], reverse=True)\n",
    "            \n",
    "            # Send Telegram alert if we found any patterns\n",
    "            if all_results:\n",
    "                await self.send_telegram_alert(all_results)\n",
    "            \n",
    "            return all_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scanning markets: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            await self.close_session()\n",
    "\n",
    "async def run_binance_btc_confluence_scanner(timeframe, offset=1):\n",
    "    \"\"\"\n",
    "    Run the Binance BTC Confluence scanner\n",
    "    \n",
    "    Parameters:\n",
    "    timeframe (str): Time period - Available options:\n",
    "                    Minutes: 1m, 3m, 5m, 15m, 30m\n",
    "                    Hours: 1h, 2h, 4h, 6h, 8h, 12h  \n",
    "                    Days: 1d, 2d, 3d\n",
    "                    Weeks/Months: 1w, 1M\n",
    "    offset (int): Bar offset (0=current, 1=last closed, 2=two bars ago, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if offset == 0:\n",
    "        offset_desc = \"current candle\"\n",
    "    elif offset == 1:\n",
    "        offset_desc = \"last closed candle\"\n",
    "    else:\n",
    "        offset_desc = f\"{offset} candles ago\"\n",
    "    \n",
    "    print(f\"Starting Binance BTC Confluence scan for {offset_desc} on {timeframe}...\")\n",
    "    \n",
    "    # Use the confluence telegram token\n",
    "    telegram_token = \"8066329517:AAHVr6kufZWe8UqCKPfmsRhSPleNlt_7G-g\"\n",
    "    telegram_chat_id = \"375812423\"\n",
    "    \n",
    "    scanner = BinanceBTCConfluenceScanner(telegram_token, telegram_chat_id, timeframe, offset)\n",
    "    results = await scanner.scan_all_markets()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} Confluence patterns:\")\n",
    "        \n",
    "        # Convert results to DataFrame for console display\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Round numeric columns for BTC precision\n",
    "        df_results['volume_btc'] = df_results['volume_btc'].round(4)\n",
    "        df_results['close'] = df_results['close'].round(8)\n",
    "        df_results['volume'] = df_results['volume'].round(4)\n",
    "        df_results['volume_ratio'] = df_results['volume_ratio'].round(2)\n",
    "        df_results['close_off_low'] = df_results['close_off_low'].round(1)\n",
    "        df_results['momentum_score'] = df_results['momentum_score'].round(4)\n",
    "        \n",
    "        # Reorder columns for better display\n",
    "        display_cols = ['symbol', 'close', 'volume_btc', 'volume_ratio', 'close_off_low', \n",
    "                       'momentum_score', 'high_volume', 'spread_breakout', 'momentum_breakout']\n",
    "        available_cols = [col for col in display_cols if col in df_results.columns]\n",
    "        \n",
    "        # Display the results\n",
    "        print(df_results[available_cols])\n",
    "        \n",
    "        # Show component analysis\n",
    "        print(f\"\\n🔧 COMPONENT ANALYSIS:\")\n",
    "        vol_count = df_results['high_volume'].sum()\n",
    "        spread_count = df_results['spread_breakout'].sum()\n",
    "        momentum_count = df_results['momentum_breakout'].sum()\n",
    "        \n",
    "        print(f\"High Volume signals: {vol_count}/{len(results)} ({vol_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Spread Breakout signals: {spread_count}/{len(results)} ({spread_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Momentum Breakout signals: {momentum_count}/{len(results)} ({momentum_count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo Confluence patterns found in {offset_desc}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Apply nest_asyncio to allow async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage functions\n",
    "async def scan_binance_btc_current():\n",
    "    \"\"\"Scan current candle for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=0)\n",
    "\n",
    "async def scan_binance_btc_closed():\n",
    "    \"\"\"Scan last closed candle for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=1)\n",
    "\n",
    "async def scan_binance_btc_previous():\n",
    "    \"\"\"Scan two candles ago for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=2)\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution - modify parameters here\n",
    "    \"\"\"\n",
    "    timeframe = \"1w\"  # 1d, 2d, 3d, 1w\n",
    "    offset = 0        # 0 = current candle, 1 = last closed candle, 2 = two candles ago\n",
    "    \n",
    "    await run_binance_btc_confluence_scanner(timeframe, offset)\n",
    "\n",
    "# Run the async main function\n",
    "print(\"🔍 BINANCE BTC CONFLUENCE SCANNER\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Available timeframes:\")\n",
    "print(\"• Minutes: 1m, 3m, 5m, 15m, 30m\")\n",
    "print(\"• Hours: 1h, 2h, 4h, 6h, 8h, 12h\") \n",
    "print(\"• Days: 1d, 2d, 3d\")\n",
    "print(\"• Weeks/Months: 1w, 1M\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"• await main() - Run with default settings\")\n",
    "print(\"• await scan_binance_btc_current() - Scan current candle\")\n",
    "print(\"• await scan_binance_btc_closed() - Scan last closed candle\")\n",
    "print(\"• await scan_binance_btc_previous() - Scan two candles ago\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('timeframe', offset) - Custom scan\")\n",
    "print(\"\\nExamples:\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('4h', 1)  # 4-hour last closed\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('15m', 0) # 15-min current\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('1M', 1)  # Monthly last closed\")\n",
    "print(\"This scanner uses REAL Binance BTC pair volumes!\")\n",
    "\n",
    "# Uncomment to auto-run:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b52f4-ca65-425b-a41a-42b70a57aefa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Direct strategy debug of any pair on any exchange\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "from exchanges import MexcSpotClient, BybitSpotClient, GateioSpotClient, KucoinSpotClient, BinanceSpotClient, BinanceFuturesClient, BybitFuturesClient\n",
    "from custom_strategies import detect_volume_surge, detect_weak_uptrend, detect_pin_down\n",
    "from breakout_vsa import vsa_detector, breakout_bar_vsa, stop_bar_vsa, reversal_bar_vsa, start_bar_vsa, loaded_bar_vsa, test_bar_vsa\n",
    "\n",
    "async def test_strategy(exchange_client_class, timeframe, symbol, strategy_name):\n",
    "    client = exchange_client_class(timeframe=timeframe)\n",
    "    await client.init_session()\n",
    "    df = await client.fetch_klines(symbol)\n",
    "    await client.close_session()\n",
    "    \n",
    "    if df is None or len(df) < 10:\n",
    "        print(f\"No data fetched for {symbol} or insufficient data (< 10 bars)\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{timeframe} Candles for {symbol}:\")\n",
    "    print(df.tail(5))\n",
    "    last_row = df.iloc[-1]\n",
    "    volume_usd = last_row['volume'] * last_row['close']\n",
    "    print(f\"Last Bar: volume_usd={volume_usd:.2f}, close={last_row['close']}, volume={last_row['volume']:.2f}\")\n",
    "    \n",
    "    # Different handling based on strategy type\n",
    "    if strategy_name == \"volume_surge\":\n",
    "        # Use detect_volume_surge directly\n",
    "        detected, result = detect_volume_surge(df)\n",
    "        \n",
    "        print(f\"\\nVolume Surge Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nVolume Surge Details:\")\n",
    "            print(f\"  Date: {result['timestamp']}\")\n",
    "            print(f\"  Close: ${result['close_price']:,.8f}\")\n",
    "            print(f\"  Volume: {result['volume']:,.2f}\")\n",
    "            print(f\"  Volume USD: ${result['volume_usd']:,.2f}\")\n",
    "            print(f\"  Volume Ratio: {result['volume_ratio']:,.2f}x\")\n",
    "            print(f\"  Score: {result['score']:,.2f}\")\n",
    "            print(f\"  Price Extreme: {result['price_extreme']}\")\n",
    "    \n",
    "    elif strategy_name == \"pin_down\":\n",
    "        from custom_strategies import detect_pin_down\n",
    "        detected, result = detect_pin_down(df)\n",
    "        \n",
    "        print(f\"\\nPin Down Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nPin Down Details:\")\n",
    "            for key, value in result.items():\n",
    "                if key != 'symbol':  # Skip symbol as we already know it\n",
    "                    print(f\"  {key}: {value}\")\n",
    "    \n",
    "    elif strategy_name == \"weak_uptrend\":\n",
    "        from custom_strategies import detect_weak_uptrend\n",
    "        detected, result = detect_weak_uptrend(df)\n",
    "        \n",
    "        print(f\"\\nWeak Uptrend Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nWeak Uptrend Details:\")\n",
    "            for key, value in result.items():\n",
    "                if key != 'symbol':  # Skip symbol as we already know it\n",
    "                    print(f\"  {key}: {value}\")\n",
    "    \n",
    "    else:\n",
    "        # For VSA strategies, import the appropriate get_params\n",
    "        if strategy_name == \"reversal_bar\":\n",
    "            from breakout_vsa.strategies.reversal_bar import get_params\n",
    "        elif strategy_name == \"breakout_bar\":\n",
    "            from breakout_vsa.strategies.breakout_bar import get_params\n",
    "        elif strategy_name == \"loaded_bar\":\n",
    "            from breakout_vsa.strategies.loaded_bar import get_params\n",
    "        elif strategy_name == \"stop_bar\":\n",
    "            from breakout_vsa.strategies.stop_bar import get_params\n",
    "        elif strategy_name == \"start_bar\":\n",
    "            from breakout_vsa.strategies.start_bar import get_params\n",
    "        else:\n",
    "            print(f\"Unknown strategy: {strategy_name}\")\n",
    "            return\n",
    "        \n",
    "        # Use vsa_detector with strategy-specific params\n",
    "        params = get_params()\n",
    "        condition, result = vsa_detector(df, params)\n",
    "        \n",
    "        strategy_display_name = strategy_name.replace('_vsa', '').replace('_', ' ').title()\n",
    "        print(f\"\\n{strategy_display_name} Detection Results:\")\n",
    "        print(f\"Current Bar (index -1): {condition.iloc[-1]}\")\n",
    "        if len(df) > 1:\n",
    "            print(f\"Last Closed Bar (index -2): {condition.iloc[-2]}\")\n",
    "        \n",
    "        if condition.iloc[-1] or (len(df) > 1 and condition.iloc[-2]):\n",
    "            detected_idx = -1 if condition.iloc[-1] else -2\n",
    "            volume_mean = df['volume'].rolling(7).mean().iloc[detected_idx]\n",
    "            bar_range = df['high'].iloc[detected_idx] - df['low'].iloc[detected_idx]\n",
    "            close_off_low = (df['close'].iloc[detected_idx] - df['low'].iloc[detected_idx]) / bar_range * 100 if bar_range > 0 else 0\n",
    "            volume_usd_detected = df['volume'].iloc[detected_idx] * df['close'].iloc[detected_idx]\n",
    "            \n",
    "            arctan_ratio = result['arctan_ratio'].iloc[detected_idx]  # From result DataFrame\n",
    "            \n",
    "            print(f\"\\nDetected at index {detected_idx} ({'Current' if detected_idx == -1 else 'Last Closed'} Bar):\")\n",
    "            print(f\"  Date: {df.index[detected_idx]}\")\n",
    "            print(f\"  Close: ${df['close'].iloc[detected_idx]:,.8f}\")\n",
    "            print(f\"  Volume Ratio: {df['volume'].iloc[detected_idx] / volume_mean if volume_mean > 0 else 0:.2f}x\")\n",
    "            print(f\"  {timeframe} Volume: ${volume_usd_detected:.2f}\")\n",
    "            print(f\"  Close Off Low: {close_off_low:.1f}%\")\n",
    "            print(f\"  Angular Ratio: {arctan_ratio:.2f}\")\n",
    "\n",
    "# Define the test case\n",
    "exchange_client = GateioSpotClient\n",
    "timeframe = \"1w\"\n",
    "symbol = \"PRCL_USDT\"\n",
    "strategy = \"loaded_bar\"\n",
    "await test_strategy(exchange_client, timeframe, symbol, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9bf3d-5437-413f-a6a8-2496412059e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#zip the project\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Go to parent directory of your project\n",
    "os.chdir(\"/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025\")\n",
    "\n",
    "# Create the zip file (this will include everything inside 'hbs_2025')\n",
    "shutil.make_archive(\"Project_VSA_2025_backup\", 'zip', \"Project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c74d8-f489-49e8-8596-c36b3960fbe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 -> Debug built weekly candles for mexc and kucoin \n",
    "import sys\n",
    "import os\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "import asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from exchanges.kucoin_client import KucoinClient\n",
    "from breakout_vsa.core import calculate_start_bar\n",
    "\n",
    "from scanner.main import kline_cache\n",
    "kline_cache.clear()  # Clear cache for fresh data\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "async def debug_start_bar_detection():\n",
    "    # Initialize client\n",
    "    client = KucoinClient(timeframe=\"1w\")\n",
    "    await client.init_session()\n",
    "    \n",
    "    # Symbol to debug\n",
    "    symbol = \"TAO-USDT\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch data\n",
    "        df = await client.fetch_klines(symbol)\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"Weekly candles for {symbol}:\")\n",
    "            print(df.tail())\n",
    "            \n",
    "            # Add intermediate calculations to see what's happening\n",
    "            # This is a modified version of calculate_start_bar that adds debugging\n",
    "            lookback = 5\n",
    "            volume_lookback = 30\n",
    "            volume_percentile = 50\n",
    "            low_percentile = 75\n",
    "            range_percentile = 75\n",
    "            close_off_lows_percent = 50\n",
    "            prev_close_range = 75\n",
    "            \n",
    "            # Calculate basic bar characteristics\n",
    "            df['bar_range'] = df['high'] - df['low']\n",
    "            df['volume_rank'] = df['volume'].rolling(lookback).apply(\n",
    "                lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                raw=True\n",
    "            )\n",
    "            \n",
    "            # Calculate rolling values\n",
    "            df['macro_low'] = df['low'].rolling(volume_lookback).min()\n",
    "            df['macro_high'] = df['high'].rolling(volume_lookback).max()\n",
    "            df['highest_high'] = df['high'].rolling(lookback).max()\n",
    "            \n",
    "            # Volume conditions\n",
    "            df['volume_sma'] = df['volume'].rolling(volume_lookback).mean()\n",
    "            df['volume_std'] = df['volume'].rolling(volume_lookback).std()\n",
    "            df['excess_volume'] = df['volume'] > (df['volume_sma'] + 3.0 * df['volume_std'])\n",
    "            \n",
    "            # Range conditions\n",
    "            df['range_sma'] = df['bar_range'].rolling(volume_lookback).mean()\n",
    "            df['range_std'] = df['bar_range'].rolling(volume_lookback).std()\n",
    "            df['excess_range'] = df['bar_range'] > (df['range_sma'] + 3.0 * df['range_std'])\n",
    "            \n",
    "            # Volume percentile condition\n",
    "            def is_in_top_percent(series, length, percent):\n",
    "                ranks = series.rolling(length).apply(\n",
    "                    lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                    raw=True\n",
    "                )\n",
    "                return ranks >= percent\n",
    "            \n",
    "            def is_in_bottom_percent(series, length, percent):\n",
    "                ranks = series.rolling(length).apply(\n",
    "                    lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                    raw=True\n",
    "                )\n",
    "                return ranks <= percent\n",
    "            \n",
    "            # Volume conditions\n",
    "            df['is_higher_volume'] = is_in_top_percent(df['volume'], lookback, volume_percentile)\n",
    "            df['is_high_volume'] = (df['volume'] > 0.75 * df['volume_sma']) & (df['volume'] > df['volume'].shift(1))\n",
    "            \n",
    "            # Price action conditions\n",
    "            df['has_higher_high'] = df['high'] > df['high'].shift(1)\n",
    "            df['no_narrow_range'] = is_in_top_percent(df['bar_range'], lookback, range_percentile)\n",
    "            \n",
    "            # Low price condition\n",
    "            df['is_in_the_lows'] = (\n",
    "                (df['low'] - df['macro_low']).abs() < df['bar_range']\n",
    "            ) | is_in_bottom_percent(df['low'], volume_lookback, low_percentile)\n",
    "            \n",
    "            # Close position conditions\n",
    "            df['close_in_the_highs'] = (\n",
    "                (df['close'] - df['low']) / df['bar_range']\n",
    "            ) >= (close_off_lows_percent / 100)\n",
    "            \n",
    "            # Previous close distance condition\n",
    "            df['far_prev_close'] = (\n",
    "                (df['close'] - df['close'].shift(1)).abs() >=\n",
    "                (df['bar_range'].shift(1) * (prev_close_range / 100))\n",
    "            )\n",
    "            \n",
    "            # New highs condition\n",
    "            df['new_highs'] = df['high'] >= 0.75 * df['highest_high']\n",
    "            \n",
    "            # Optional strength condition\n",
    "            df['strong_close'] = df['close'] >= df['highest_high'].shift(1)\n",
    "            \n",
    "            # Now check the actual values for the last few bars\n",
    "            last_rows = df.tail(3)\n",
    "            \n",
    "            print(\"\\nAnalyzing last 3 bars:\")\n",
    "            for idx, row in last_rows.iterrows():\n",
    "                print(f\"\\nBar at {idx.strftime('%Y-%m-%d')}:\")\n",
    "                print(f\"  is_high_volume: {row['is_high_volume']}\")\n",
    "                print(f\"  has_higher_high: {row['has_higher_high']}\")\n",
    "                print(f\"  no_narrow_range: {row['no_narrow_range']}\")\n",
    "                print(f\"  close_in_the_highs: {row['close_in_the_highs']}\")\n",
    "                print(f\"  far_prev_close: {row['far_prev_close']}\")\n",
    "                print(f\"  excess_range: {row['excess_range']}\")\n",
    "                print(f\"  excess_volume: {row['excess_volume']}\")\n",
    "                print(f\"  new_highs: {row['new_highs']}\")\n",
    "                print(f\"  is_in_the_lows: {row['is_in_the_lows']}\")\n",
    "                print(f\"  volume: {row['volume']}, volume_sma: {row['volume_sma']}\")\n",
    "                print(f\"  bar_range: {row['bar_range']}, range_sma: {row['range_sma']}\")\n",
    "                \n",
    "            # Run the original function to confirm\n",
    "            start_bar_pattern = calculate_start_bar(df)\n",
    "            print(f\"\\nFinal Start Bar detection result:\")\n",
    "            print(start_bar_pattern.tail(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug: {str(e)}\")\n",
    "    finally:\n",
    "        await client.close_session()\n",
    "\n",
    "# Replace the last part of your script with this:\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # For Jupyter/IPython environments\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        asyncio.run(debug_start_bar_detection())\n",
    "    except ImportError:\n",
    "        # For regular Python environments\n",
    "        asyncio.run(debug_start_bar_detection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffeddd-c2c8-4f75-887f-9ed7d1961a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip the project\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Go to parent directory of your project\n",
    "os.chdir(\"/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025\")\n",
    "\n",
    "# Create the zip file (this will include everything inside 'hbs_2025')\n",
    "shutil.make_archive(\"Project_VSA_2025_backup\", 'zip', \"Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29951e-0a3b-4d0b-928c-0f2bfce7c11c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ARCHIVE - Confluence Scanner with bar offset\n",
    "\n",
    "from telegram.ext import Application\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import os\n",
    "import html\n",
    "\n",
    "# Add project path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from exchanges.sf_pairs_service import SFPairsService\n",
    "from custom_strategies import detect_confluence\n",
    "\n",
    "class ConfluenceScanner:\n",
    "    def __init__(self, telegram_token, telegram_chat_id, exchange, timeframe, offset=1):\n",
    "        self.telegram_token = telegram_token\n",
    "        self.telegram_chat_id = telegram_chat_id\n",
    "        self.telegram_app = None\n",
    "        self.exchange = exchange\n",
    "        self.timeframe = timeframe\n",
    "        self.offset = offset  # Added offset parameter\n",
    "        self.sf_service = SFPairsService()\n",
    "        \n",
    "    async def init_telegram(self):\n",
    "        if self.telegram_app is None:\n",
    "            self.telegram_app = Application.builder().token(self.telegram_token).build()\n",
    "\n",
    "    async def send_telegram_alert(self, results):\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            message = f\"🚨 Confluence Detection - {self.exchange} {self.timeframe}\\n\\n\"\n",
    "            \n",
    "            # Map timeframe to TradingView format\n",
    "            tv_timeframe_map = {\n",
    "                \"1d\": \"1D\",\n",
    "                \"2d\": \"2D\",\n",
    "                \"1w\": \"1W\"\n",
    "            }\n",
    "            tv_timeframe = tv_timeframe_map.get(self.timeframe.lower(), self.timeframe)\n",
    "            \n",
    "            for result in results:\n",
    "                exchange_name = self.exchange.upper()\n",
    "                formatted_symbol = f\"{result['symbol']}\"\n",
    "                tv_link = f\"https://www.tradingview.com/chart/?symbol={exchange_name}:{formatted_symbol}&interval={tv_timeframe}\"\n",
    "                \n",
    "                # Escape HTML entities in the URL\n",
    "                escaped_link = html.escape(tv_link)\n",
    "                \n",
    "                # Format according to specified requirements\n",
    "                time_str = \"\"\n",
    "                if result.get('timestamp') is not None:\n",
    "                    time_str = f\"Time: {result['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "                \n",
    "                message += (\n",
    "                    f\"Symbol: {result['symbol']}\\n\"\n",
    "                    f\"{time_str}\"\n",
    "                    f\"Volume USD: ${result['volume_usd']:,.2f}\\n\"\n",
    "                    f\"Close: <a href='{escaped_link}'>${result['close']:,.8f}</a>\\n\"\n",
    "                    f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                    f\"Close Off Low: {result['close_off_low']:.1f}%\\n\"\n",
    "                    f\"{'='*30}\\n\"\n",
    "                )\n",
    "            \n",
    "            # Split message more carefully to avoid breaking HTML tags\n",
    "            max_length = 4000  # Reduced from 4096 to be safer\n",
    "            \n",
    "            if len(message) > max_length:\n",
    "                # Split at natural breaks (between results) to avoid breaking HTML\n",
    "                sections = message.split('='*30 + '\\n')\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for section in sections:\n",
    "                    if len(current_chunk + section + '='*30 + '\\n') > max_length:\n",
    "                        if current_chunk:\n",
    "                            await self.telegram_app.bot.send_message(\n",
    "                                chat_id=self.telegram_chat_id,\n",
    "                                text=current_chunk.strip(),\n",
    "                                parse_mode='HTML',\n",
    "                                disable_web_page_preview=True\n",
    "                            )\n",
    "                        current_chunk = section + '\\n'\n",
    "                    else:\n",
    "                        current_chunk += section + '='*30 + '\\n'\n",
    "                \n",
    "                # Send remaining chunk\n",
    "                if current_chunk.strip():\n",
    "                    await self.telegram_app.bot.send_message(\n",
    "                        chat_id=self.telegram_chat_id,\n",
    "                        text=current_chunk.strip(),\n",
    "                        parse_mode='HTML',\n",
    "                        disable_web_page_preview=True\n",
    "                    )\n",
    "            else:\n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=message,\n",
    "                    parse_mode='HTML',\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sending Telegram alert: {str(e)}\")\n",
    "            \n",
    "            # Fallback: send without HTML formatting\n",
    "            try:\n",
    "                simple_message = f\"🚨 Confluence Detection - {self.exchange} {self.timeframe}\\n\\n\"\n",
    "                for result in results:\n",
    "                    simple_message += (\n",
    "                        f\"Symbol: {result['symbol']}\\n\"\n",
    "                        f\"Volume USD: ${result['volume_usd']:,.2f}\\n\"\n",
    "                        f\"Close: ${result['close']:,.8f}\\n\"\n",
    "                        f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                        f\"Components: Vol={result['high_volume']}, Spread={result['spread_breakout']}, Mom={result['momentum_breakout']}\\n\\n\"\n",
    "                    )\n",
    "                \n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=simple_message,\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "            except Exception as fallback_error:\n",
    "                logging.error(f\"Fallback Telegram send also failed: {str(fallback_error)}\")\n",
    "\n",
    "    def prepare_sf_data(self, raw_df):\n",
    "        \"\"\"Convert SF data to confluence-compatible format\"\"\"\n",
    "        if raw_df is None or len(raw_df) == 0:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(raw_df)\n",
    "        \n",
    "        # Convert datetime column to pandas datetime and set as index\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df = df.set_index('datetime')\n",
    "        elif 'time' in df.columns:\n",
    "            # Convert Unix timestamp to datetime\n",
    "            df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "            df = df.set_index('time')\n",
    "        \n",
    "        # Select only OHLCV columns needed for confluence\n",
    "        required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        available_cols = [col for col in required_cols if col in df.columns]\n",
    "        \n",
    "        if len(available_cols) != 5:\n",
    "            return None\n",
    "        \n",
    "        # Select and clean data\n",
    "        result_df = df[required_cols].copy()\n",
    "        \n",
    "        # Ensure numeric types\n",
    "        for col in required_cols:\n",
    "            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "        \n",
    "        # Drop any NaN rows\n",
    "        result_df = result_df.dropna()\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def scan_single_market(self, pair, ohlcv_data):\n",
    "        \"\"\"Scan a single market for Confluence pattern in the specified bar\"\"\"\n",
    "        try:\n",
    "            # Prepare data for confluence analysis\n",
    "            df = self.prepare_sf_data(ohlcv_data)\n",
    "            \n",
    "            if df is None or len(df) < 50:  # Need enough data for confluence\n",
    "                return None\n",
    "            \n",
    "            # Calculate which bar to check based on offset\n",
    "            check_bar = -(self.offset + 1)  # offset=0 means current bar (-1), offset=1 means last closed (-2), etc.\n",
    "            \n",
    "            # Run confluence detection\n",
    "            detected, result = detect_confluence(df, check_bar=check_bar)\n",
    "            \n",
    "            if detected:\n",
    "                # Calculate volume in USD for the target bar\n",
    "                target_close = df['close'].iloc[check_bar]\n",
    "                target_volume = df['volume'].iloc[check_bar]\n",
    "                volume_usd = float(target_close) * float(target_volume)\n",
    "                \n",
    "                confluence_result = {\n",
    "                    'symbol': f\"{pair['Token']}{pair['Quote']}\",\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'close': float(target_close),\n",
    "                    'volume': float(target_volume),\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                    'bar_range': result['bar_range']\n",
    "                }\n",
    "                return confluence_result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pair['Token']}{pair['Quote']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    async def scan_all_markets(self):\n",
    "        \"\"\"Scan all markets for Confluence pattern\"\"\"\n",
    "        await self.init_telegram()\n",
    "        try:\n",
    "            # Define volume thresholds\n",
    "            volume_thresholds = {\n",
    "                \"1w\": 300000,\n",
    "                \"2d\": 100000,\n",
    "                \"1d\": 50000\n",
    "            }\n",
    "            min_volume = volume_thresholds.get(self.timeframe.lower(), 50000)\n",
    "            \n",
    "            # Create offset description\n",
    "            if self.offset == 0:\n",
    "                offset_desc = \"current candle\"\n",
    "            elif self.offset == 1:\n",
    "                offset_desc = \"last closed candle\"\n",
    "            else:\n",
    "                offset_desc = f\"{self.offset} candles ago\"\n",
    "            \n",
    "            print(f\"Scanning for Confluence patterns in {offset_desc}...\")\n",
    "            print(f\"Minimum volume threshold: ${min_volume:,.0f}\")\n",
    "            \n",
    "            # Get all pairs from SF service\n",
    "            pairs = self.sf_service.get_pairs_of_exchange(self.exchange)\n",
    "            print(f\"Found {len(pairs)} markets to scan...\")\n",
    "            \n",
    "            # Process all pairs with progress bar\n",
    "            all_results = []\n",
    "            with tqdm(total=len(pairs), desc=\"Scanning markets\") as pbar:\n",
    "                for pair in pairs:\n",
    "                    try:\n",
    "                        # Get OHLCV data from SF service\n",
    "                        ohlcv_data = self.sf_service.get_ohlcv_for_pair(\n",
    "                            pair['Token'], \n",
    "                            pair['Quote'], \n",
    "                            self.exchange, \n",
    "                            self.timeframe, \n",
    "                            100  # Get more data for confluence analysis\n",
    "                        )\n",
    "                        \n",
    "                        if ohlcv_data is None or len(ohlcv_data) == 0:\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        df = pd.DataFrame(ohlcv_data)\n",
    "                        \n",
    "                        # Check if we have enough data\n",
    "                        if len(df) >= 50:  # Need enough for confluence analysis\n",
    "                            target_idx = -(self.offset + 1)  # Adjust index based on offset\n",
    "                            \n",
    "                            # Check volume threshold for the target candle\n",
    "                            try:\n",
    "                                target_candle_volume = float(df['close'].iloc[target_idx]) * float(df['volume'].iloc[target_idx])\n",
    "                                \n",
    "                                # Only process if volume meets threshold\n",
    "                                if target_candle_volume >= min_volume:\n",
    "                                    result = self.scan_single_market(pair, ohlcv_data)\n",
    "                                    if result:\n",
    "                                        all_results.append(result)\n",
    "                                        print(f\"Found Confluence: {pair['Token']}{pair['Quote']} 🎯\")\n",
    "                            except (IndexError, ValueError):\n",
    "                                pass  # Skip if we can't calculate volume\n",
    "                                    \n",
    "                    except Exception as e:\n",
    "                        if \"500\" not in str(e):  # Don't log 500 errors\n",
    "                            logging.error(f\"Error processing {pair['Token']}{pair['Quote']}: {str(e)}\")\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            \n",
    "            # Sort by volume\n",
    "            all_results.sort(key=lambda x: x['volume_usd'], reverse=True)\n",
    "            \n",
    "            # Send Telegram alert if we found any patterns\n",
    "            if all_results:\n",
    "                await self.send_telegram_alert(all_results)\n",
    "            \n",
    "            return all_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scanning markets: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "async def run_confluence_scanner(exchange, timeframe, offset=1):\n",
    "    \"\"\"\n",
    "    Run the Confluence scanner\n",
    "    \n",
    "    Parameters:\n",
    "    exchange (str): Exchange name (Kucoin, Mexc, Binance)\n",
    "    timeframe (str): Time period (1d, 2d, 1w)\n",
    "    offset (int): Bar offset (0=current, 1=last closed, 2=two bars ago, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if offset == 0:\n",
    "        offset_desc = \"current candle\"\n",
    "    elif offset == 1:\n",
    "        offset_desc = \"last closed candle\"\n",
    "    else:\n",
    "        offset_desc = f\"{offset} candles ago\"\n",
    "    \n",
    "    print(f\"Starting Confluence scan for {offset_desc} on {exchange} {timeframe}...\")\n",
    "    \n",
    "    # Use the confluence telegram token from your big project config\n",
    "    # You should replace this with the actual token from utils/config.py TELEGRAM_TOKENS[\"confluence\"]\n",
    "    telegram_token = \"8066329517:AAHVr6kufZWe8UqCKPfmsRhSPleNlt_7G-g\"  # Replace with confluence token\n",
    "    telegram_chat_id = \"375812423\"  # Your chat ID\n",
    "    \n",
    "    scanner = ConfluenceScanner(telegram_token, telegram_chat_id, exchange, timeframe, offset)\n",
    "    results = await scanner.scan_all_markets()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} Confluence patterns:\")\n",
    "        \n",
    "        # Convert results to DataFrame for console display\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Round numeric columns\n",
    "        df_results['volume_usd'] = df_results['volume_usd'].round(2)\n",
    "        df_results['close'] = df_results['close'].round(8)\n",
    "        df_results['volume'] = df_results['volume'].round(2)\n",
    "        df_results['volume_ratio'] = df_results['volume_ratio'].round(2)\n",
    "        df_results['close_off_low'] = df_results['close_off_low'].round(1)\n",
    "        df_results['momentum_score'] = df_results['momentum_score'].round(4)\n",
    "        \n",
    "        # Reorder columns for better display\n",
    "        display_cols = ['symbol', 'close', 'volume_usd', 'volume_ratio', 'close_off_low', \n",
    "                       'momentum_score', 'high_volume', 'spread_breakout', 'momentum_breakout']\n",
    "        available_cols = [col for col in display_cols if col in df_results.columns]\n",
    "        \n",
    "        # Display the results\n",
    "        print(df_results[available_cols])\n",
    "        \n",
    "        # Show component analysis\n",
    "        print(f\"\\n🔧 COMPONENT ANALYSIS:\")\n",
    "        vol_count = df_results['high_volume'].sum()\n",
    "        spread_count = df_results['spread_breakout'].sum()\n",
    "        momentum_count = df_results['momentum_breakout'].sum()\n",
    "        \n",
    "        print(f\"High Volume signals: {vol_count}/{len(results)} ({vol_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Spread Breakout signals: {spread_count}/{len(results)} ({spread_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Momentum Breakout signals: {momentum_count}/{len(results)} ({momentum_count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo Confluence patterns found in {offset_desc}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Apply nest_asyncio to allow async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage functions\n",
    "async def scan_current_confluence():\n",
    "    \"\"\"Scan current candle for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=0)\n",
    "\n",
    "async def scan_closed_confluence():\n",
    "    \"\"\"Scan last closed candle for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=1)\n",
    "\n",
    "async def scan_previous_confluence():\n",
    "    \"\"\"Scan two candles ago for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=2)\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution - modify parameters here\n",
    "    \"\"\"\n",
    "    exchange = \"Mexc\"  # Binance, Kucoin, Mexc\n",
    "    timeframe = \"1w\"     # 1d, 2d, 1w\n",
    "    offset = 0           # 0 = current candle, 1 = last closed candle, 2 = two candles ago\n",
    "    \n",
    "    await run_confluence_scanner(exchange, timeframe, offset)\n",
    "\n",
    "# Run the async main function\n",
    "print(\"🔍 CONFLUENCE SCANNER\")\n",
    "print(\"=\" * 30)\n",
    "print(\"Available functions:\")\n",
    "print(\"• await main() - Run with default settings\")\n",
    "print(\"• await scan_current_confluence() - Scan current candle\")\n",
    "print(\"• await scan_closed_confluence() - Scan last closed candle\")\n",
    "print(\"• await scan_previous_confluence() - Scan two candles ago\")\n",
    "print(\"• await run_confluence_scanner('Exchange', 'timeframe', offset) - Custom scan\")\n",
    "print(\"\\nExample: await main()\")\n",
    "\n",
    "# Uncomment to auto-run:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0c836-afa7-4fba-94d7-0dfafef0f8c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Standalone HLC Bar Chart Plotter\n",
    "\"\"\"\n",
    "Standalone HLC Bar Chart Plotter\n",
    "A reusable function for plotting HLC bars with optional pattern highlighting\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_hlc_bars(data, highlighted_bars=None, title=\"HLC Chart\", symbol=\"SYMBOL\", \n",
    "                  interval=\"1d\", semilog=False, highlight_color=\"fuchsia\", \n",
    "                  highlight_label=\"Pattern\", figsize=(14, 10), show_volume=True):\n",
    "    \"\"\"\n",
    "    Plot HLC bar chart with optional pattern highlighting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame with columns: ['datetime', 'high', 'low', 'close', 'volume']\n",
    "        - datetime: timestamp column (will be used for x-axis)\n",
    "        - high: high prices\n",
    "        - low: low prices  \n",
    "        - close: close prices\n",
    "        - volume: volume data (optional if show_volume=False)\n",
    "        \n",
    "    highlighted_bars : pandas.Series or list/array, optional\n",
    "        Boolean series or array indicating which bars to highlight\n",
    "        Length must match data length\n",
    "        \n",
    "    title : str, default \"HLC Chart\"\n",
    "        Chart title\n",
    "        \n",
    "    symbol : str, default \"SYMBOL\" \n",
    "        Symbol name for display\n",
    "        \n",
    "    interval : str, default \"1d\"\n",
    "        Time interval for date formatting (1m, 5m, 15m, 30m, 1h, 4h, 1d, 3d, 1w, 1M)\n",
    "        \n",
    "    semilog : bool, default False\n",
    "        Use logarithmic scale for price chart\n",
    "        \n",
    "    highlight_color : str, default \"fuchsia\"\n",
    "        Color for highlighted bars\n",
    "        \n",
    "    highlight_label : str, default \"Pattern\"\n",
    "        Label for highlighted bars in legend\n",
    "        \n",
    "    figsize : tuple, default (14, 10)\n",
    "        Figure size (width, height)\n",
    "        \n",
    "    show_volume : bool, default True\n",
    "        Whether to show volume subplot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The created figure object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise ValueError(\"data must be a pandas DataFrame\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['datetime', 'high', 'low', 'close']\n",
    "    if show_volume:\n",
    "        required_cols.append('volume')\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        raise ValueError(\"data DataFrame is empty\")\n",
    "    \n",
    "    # Validate highlighted_bars\n",
    "    if highlighted_bars is not None:\n",
    "        if len(highlighted_bars) != len(data):\n",
    "            raise ValueError(\"highlighted_bars length must match data length\")\n",
    "        # Convert to boolean array\n",
    "        highlighted_bars = np.array(highlighted_bars, dtype=bool)\n",
    "    else:\n",
    "        highlighted_bars = np.zeros(len(data), dtype=bool)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    if show_volume:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, \n",
    "                                       gridspec_kw={'height_ratios': [3, 1]})\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "        ax2 = None\n",
    "    \n",
    "    # Convert dates to numbers for plotting\n",
    "    dates = mdates.date2num(data['datetime'])\n",
    "    \n",
    "    # Calculate margins and tick length\n",
    "    date_range = dates[-1] - dates[0] if len(dates) > 1 else 1\n",
    "    margin = date_range * 0.05\n",
    "    \n",
    "    # Calculate actual bar spacing for consistent tick length\n",
    "    if len(dates) > 1:\n",
    "        avg_bar_spacing = date_range / (len(dates) - 1)\n",
    "        tick_length = avg_bar_spacing * 0.4  # 40% of bar spacing\n",
    "        volume_bar_width = avg_bar_spacing * 0.8\n",
    "    else:\n",
    "        tick_length = date_range * 0.01\n",
    "        volume_bar_width = date_range * 0.02\n",
    "    \n",
    "    # Draw HLC bars\n",
    "    for i, (date, high, low, close) in enumerate(zip(dates, data['high'], data['low'], data['close'])):\n",
    "        is_highlighted = highlighted_bars[i]\n",
    "        color = highlight_color if is_highlighted else 'black'\n",
    "        line_width = 1.2\n",
    "        \n",
    "        # Vertical line from low to high\n",
    "        ax1.plot([date, date], [low, high], color=color, linewidth=line_width, solid_capstyle='butt')\n",
    "        \n",
    "        # Horizontal tick mark for close (on the right side)\n",
    "        ax1.plot([date, date + tick_length], [close, close], color=color, \n",
    "                linewidth=line_width+0.5, solid_capstyle='butt')\n",
    "    \n",
    "    # Configure price chart\n",
    "    if semilog:\n",
    "        ax1.set_yscale('log')\n",
    "        scale_text = \"Semilog Scale\"\n",
    "    else:\n",
    "        scale_text = \"Linear Scale\"\n",
    "    \n",
    "    ax1.set_title(f'{symbol} {title} - {scale_text}', fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel('Price', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis based on timeframe\n",
    "    _format_datetime_axis(ax1, interval)\n",
    "    ax1.set_xlim(dates[0] - margin, dates[-1] + margin)\n",
    "    \n",
    "    # Volume chart\n",
    "    if show_volume and ax2 is not None:\n",
    "        volume_colors = [highlight_color if highlighted_bars[i] else 'orange' for i in range(len(dates))]\n",
    "        volume_edges = ['darkmagenta' if highlighted_bars[i] else 'darkorange' for i in range(len(dates))]\n",
    "        \n",
    "        ax2.bar(dates, data['volume'], width=volume_bar_width, alpha=0.6, \n",
    "                color=volume_colors, edgecolor=volume_edges)\n",
    "        ax2.set_ylabel('Volume', fontsize=12)\n",
    "        ax2.set_xlabel('Date', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        _format_datetime_axis(ax2, interval)\n",
    "        ax2.set_xlim(dates[0] - margin, dates[-1] + margin)\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "    else:\n",
    "        ax1.set_xlabel('Date', fontsize=12)\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='black', linewidth=2, label='High-Low Range'),\n",
    "        Line2D([0], [0], color='black', linewidth=3, label='Close Price (right tick)')\n",
    "    ]\n",
    "    \n",
    "    # Add highlighted bars to legend if any exist\n",
    "    if highlighted_bars.any():\n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0], color=highlight_color, linewidth=3, label=highlight_label)\n",
    "        )\n",
    "        highlight_count = highlighted_bars.sum()\n",
    "    else:\n",
    "        highlight_count = 0\n",
    "    \n",
    "    ax1.legend(handles=legend_elements, loc='upper left', title=scale_text)\n",
    "    \n",
    "    # Add pattern count if patterns exist\n",
    "    if highlight_count > 0:\n",
    "        ax1.text(0.99, 0.95, f'{highlight_label}: {highlight_count}', \n",
    "                transform=ax1.transAxes, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=10)\n",
    "    \n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def _format_datetime_axis(ax, interval):\n",
    "    \"\"\"Helper function to format datetime axis based on interval\"\"\"\n",
    "    if interval in ['1m', '5m', '15m', '30m']:\n",
    "        ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
    "    elif interval in ['1h', '2h', '4h', '6h', '12h']:\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    elif interval in ['1d', '3d']:\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    else:  # weekly, monthly\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Example usage and test function\n",
    "def example_usage():\n",
    "    \"\"\"Example showing how to use the plot_hlc_bars function\"\"\"\n",
    "    \n",
    "    # Create sample data\n",
    "    import datetime\n",
    "    dates = pd.date_range('2023-01-01', periods=100, freq='D')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate realistic OHLC data\n",
    "    closes = 100 + np.cumsum(np.random.randn(100) * 0.02)\n",
    "    highs = closes + np.random.rand(100) * 5\n",
    "    lows = closes - np.random.rand(100) * 5\n",
    "    volumes = np.random.rand(100) * 1000000\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'high': highs,\n",
    "        'low': lows, \n",
    "        'close': closes,\n",
    "        'volume': volumes\n",
    "    })\n",
    "    \n",
    "    # Create some random pattern detections\n",
    "    pattern_detected = np.random.choice([True, False], size=100, p=[0.1, 0.9])\n",
    "    \n",
    "    # Plot with pattern highlighting\n",
    "    fig = plot_hlc_bars(\n",
    "        data=data,\n",
    "        highlighted_bars=pattern_detected,\n",
    "        title=\"Daily Chart with Pattern Detection\",\n",
    "        symbol=\"EXAMPLE\",\n",
    "        interval=\"1d\",\n",
    "        semilog=False,\n",
    "        highlight_color=\"red\",\n",
    "        highlight_label=\"Detected Pattern\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Scanner integration example\n",
    "def scanner_integration_example():\n",
    "    \"\"\"Example of how to integrate with a scanner function\"\"\"\n",
    "    \n",
    "    def my_pattern_scanner(data):\n",
    "        \"\"\"\n",
    "        Example scanner function - replace with your actual scanner logic\n",
    "        Returns boolean array indicating pattern detection\n",
    "        \"\"\"\n",
    "        # Example: detect when close > 20-period moving average\n",
    "        ma20 = data['close'].rolling(20).mean()\n",
    "        pattern = (data['close'] > ma20) & (data['volume'] > data['volume'].rolling(10).mean())\n",
    "        return pattern.fillna(False)\n",
    "    \n",
    "    # Your data loading logic here\n",
    "    # data = load_your_data()  # Replace with actual data loading\n",
    "    \n",
    "    # For demo, create sample data\n",
    "    dates = pd.date_range('2023-01-01', periods=200, freq='D')\n",
    "    np.random.seed(42)\n",
    "    closes = 100 + np.cumsum(np.random.randn(200) * 0.02)\n",
    "    highs = closes + np.random.rand(200) * 3\n",
    "    lows = closes - np.random.rand(200) * 3\n",
    "    volumes = np.random.rand(200) * 1000000\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'high': highs,\n",
    "        'low': lows,\n",
    "        'close': closes,\n",
    "        'volume': volumes\n",
    "    })\n",
    "    \n",
    "    # Run your scanner\n",
    "    detected_patterns = my_pattern_scanner(data)\n",
    "    \n",
    "    # Plot only if patterns are detected\n",
    "    if detected_patterns.any():\n",
    "        print(f\"Patterns detected! Found {detected_patterns.sum()} occurrences\")\n",
    "        fig = plot_hlc_bars(\n",
    "            data=data,\n",
    "            highlighted_bars=detected_patterns,\n",
    "            title=\"Scanner Results\",\n",
    "            symbol=\"SCANNED_SYMBOL\",\n",
    "            interval=\"1d\",\n",
    "            highlight_color=\"lime\",\n",
    "            highlight_label=\"Scanner Hit\"\n",
    "        )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No patterns detected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run example\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff2c22-5671-487f-971c-fbd0c3d6a64b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dashboard\n",
    "from pyngrok import ngrok\n",
    "import os, sys, subprocess, time\n",
    "\n",
    "# --- ngrok auth (DO NOT commit your token) ---\n",
    "ngrok.set_auth_token(\"31mFDQNYuBuJw7mTKNxyDZLbZag_4Q1mV2EEggMBGYecRRZyF\")\n",
    "\n",
    "# Clean old tunnels\n",
    "try:\n",
    "    ngrok.kill()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Pick a port\n",
    "port = 8501\n",
    "\n",
    "# Launch Streamlit dashboard in the background\n",
    "# Ensure working directory contains dashboard.py\n",
    "cmd = [\"streamlit\", \"run\", \"dashboard.py\", \"--server.address\", \"0.0.0.0\", \"--server.port\", str(port)]\n",
    "proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Give Streamlit a moment to boot\n",
    "time.sleep(3)\n",
    "\n",
    "# Create public tunnel\n",
    "public_url = ngrok.connect(addr=port)\n",
    "print(\"Streamlit is running! Access it at:\", public_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f15de0-44e1-4090-bfa5-b10adc1dee19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Simple Historical Signal Scanner - Scan a specific pair or all pairs for historical signals\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our scanner module\n",
    "from direct_test import (\n",
    "    HistoricalSignalScanner, \n",
    "    save_signals_to_csv,\n",
    "    analyze_signals,\n",
    "    filter_signals\n",
    ")\n",
    "\n",
    "print(\"Historical Signal Scanner Ready!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN SCANNING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "async def scan_historical_signals(symbol=None, exchange=\"binance_futures\", timeframe=\"1d\", \n",
    "                                 strategies=None, max_symbols=None):\n",
    "    \"\"\"\n",
    "    Scan for historical signals\n",
    "    \n",
    "    Args:\n",
    "        symbol: Specific symbol to scan (e.g., \"BTCUSDT\") or None to scan all\n",
    "        exchange: Exchange to use (default: \"binance_futures\")\n",
    "        timeframe: Timeframe to analyze (default: \"1d\")\n",
    "        strategies: List of strategies or None for all available\n",
    "        max_symbols: Max number of symbols to scan when symbol=None (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: All detected signals\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default strategies if none provided\n",
    "    if strategies is None:\n",
    "        strategies = ['hbs_breakout', 'confluence', 'wedge_breakout', 'channel_breakout', \n",
    "                     'consolidation_breakout', 'sma50_breakout']\n",
    "    \n",
    "    # Default max symbols\n",
    "    if max_symbols is None:\n",
    "        max_symbols = 50\n",
    "    \n",
    "    scanner = HistoricalSignalScanner(exchange, timeframe)\n",
    "    \n",
    "    try:\n",
    "        await scanner.initialize()\n",
    "        print(f\"Initialized {exchange} scanner for {timeframe} timeframe\")\n",
    "        \n",
    "        if symbol:\n",
    "            # Scan specific symbol\n",
    "            print(f\"Scanning {symbol}...\")\n",
    "            results = await scanner.scan_symbol_historical(symbol, strategies)\n",
    "            df = scanner.create_signals_dataframe(results)\n",
    "            \n",
    "            if not df.empty:\n",
    "                filename = f\"{symbol}_{exchange}_{timeframe}_signals\"\n",
    "                save_signals_to_csv(df, filename)\n",
    "                print(f\"\\nFound {len(df)} signals for {symbol}\")\n",
    "                analyze_signals(df)\n",
    "            else:\n",
    "                print(f\"No signals found for {symbol}\")\n",
    "                \n",
    "        else:\n",
    "            # Scan all symbols\n",
    "            print(f\"Getting symbols from {exchange}...\")\n",
    "            symbols = await scanner.get_all_symbols(limit=max_symbols)\n",
    "            print(f\"Scanning {len(symbols)} symbols...\")\n",
    "            \n",
    "            results = await scanner.scan_multiple_symbols(symbols, strategies)\n",
    "            df = scanner.create_signals_dataframe(results)\n",
    "            \n",
    "            if not df.empty:\n",
    "                filename = f\"all_symbols_{exchange}_{timeframe}_{len(symbols)}pairs_signals\"\n",
    "                save_signals_to_csv(df, filename)\n",
    "                print(f\"\\nFound {len(df)} total signals across {len(symbols)} symbols\")\n",
    "                analyze_signals(df)\n",
    "                \n",
    "                # Show top symbols by signal count\n",
    "                print(f\"\\nTop symbols by signal count:\")\n",
    "                symbol_counts = df['symbol'].value_counts().head(10)\n",
    "                for sym, count in symbol_counts.items():\n",
    "                    print(f\"  {sym}: {count} signals\")\n",
    "            else:\n",
    "                print(f\"No signals found across {len(symbols)} symbols\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    finally:\n",
    "        await scanner.close()\n",
    "\n",
    "# =============================================================================\n",
    "# QUICK COMMANDS\n",
    "# =============================================================================\n",
    "\n",
    "async def scan_pair(symbol, exchange=\"binance_futures\", timeframe=\"1d\"):\n",
    "    \"\"\"Quick scan for a specific trading pair\"\"\"\n",
    "    return await scan_historical_signals(symbol=symbol, exchange=exchange, timeframe=timeframe)\n",
    "\n",
    "async def scan_all_pairs(exchange=\"binance_futures\", timeframe=\"1d\", max_pairs=50):\n",
    "    \"\"\"Quick scan for all trading pairs\"\"\"\n",
    "    return await scan_historical_signals(symbol=None, exchange=exchange, \n",
    "                                        timeframe=timeframe, max_symbols=max_pairs)\n",
    "\n",
    "\n",
    "df = await scan_historical_signals(exchange=\"mexc_spot\", timeframe=\"2d\",\n",
    "    symbol=\"NOS_USDT\",\n",
    "    strategies=['hbs_breakout']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1af8c6-534b-4a1e-9a0c-448cef9f1bd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#ASCII CHART + Testing mexc 1w candles\n",
    "\n",
    "import requests\n",
    "import datetime as dt\n",
    "\n",
    "MEXC_URL = \"https://api.mexc.com/api/v3/klines\"\n",
    "SYMBOL = \"STBLUSDT\"\n",
    "INTERVAL = \"1W\"         # MEXC uses uppercase W for weekly klines\n",
    "LIMIT = 50              # ask for more than 5 so we can always slice down\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"console-fetch/1.0 (+https://example.com)\"}\n",
    "\n",
    "def fetch_mexc_weekly(symbol=SYMBOL, limit=LIMIT):\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"interval\": INTERVAL,\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    r = requests.get(MEXC_URL, params=params, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if not isinstance(data, list) or not data:\n",
    "        raise RuntimeError(f\"Unexpected response: {data}\")\n",
    "\n",
    "    # Each kline: [openTime, open, high, low, close, volume, closeTime, ...]\n",
    "    candles = []\n",
    "    for row in data:\n",
    "        open_time_ms = int(row[0])\n",
    "        candles.append({\n",
    "            \"time\": dt.datetime.utcfromtimestamp(open_time_ms / 1000.0),\n",
    "            \"open\": float(row[1]),\n",
    "            \"high\": float(row[2]),\n",
    "            \"low\":  float(row[3]),\n",
    "            \"close\": float(row[4]),\n",
    "            \"volume\": float(row[5]),\n",
    "        })\n",
    "    return candles\n",
    "\n",
    "def ascii_candles(candles, width=40):\n",
    "    \"\"\"\n",
    "    Render simple ASCII candles for a small set of bars.\n",
    "    For each candle, draw a range [low..high] as a line, and mark:\n",
    "      O = open, C = close. Up candles also show '+' at close; down show '-'.\n",
    "    The scale is shared across the provided candles.\n",
    "    \"\"\"\n",
    "    if not candles:\n",
    "        return\n",
    "\n",
    "    lo = min(c[\"low\"] for c in candles)\n",
    "    hi = max(c[\"high\"] for c in candles)\n",
    "    if hi == lo:\n",
    "        hi = lo + 1e-9  # avoid zero range\n",
    "\n",
    "    def pos(price):\n",
    "        # map a price into [0..width-1]\n",
    "        return int(round((price - lo) / (hi - lo) * (width - 1)))\n",
    "\n",
    "    lines = []\n",
    "    for c in candles:\n",
    "        line = [\" \"] * width\n",
    "        low_p, high_p = pos(c[\"low\"]), pos(c[\"high\"])\n",
    "        open_p, close_p = pos(c[\"open\"]), pos(c[\"close\"])\n",
    "        # draw range\n",
    "        for i in range(low_p, high_p + 1):\n",
    "            line[i] = \"─\"\n",
    "        # mark open/close\n",
    "        line[open_p] = \"O\"\n",
    "        if close_p == open_p:\n",
    "            # nudge to show both if equal\n",
    "            close_p = min(width - 1, close_p + 1)\n",
    "        line[close_p] = \"C\"\n",
    "        # up/down marker at close\n",
    "        line[close_p] = \"+\" if c[\"close\"] >= c[\"open\"] else \"-\"\n",
    "        lines.append(\"\".join(line))\n",
    "    return lines, lo, hi\n",
    "\n",
    "def fmt_num(x):\n",
    "    # compact formatting for prices\n",
    "    if x >= 1000:\n",
    "        return f\"{x:,.0f}\"\n",
    "    return f\"{x:,.2f}\"\n",
    "\n",
    "def main():\n",
    "    candles = fetch_mexc_weekly()\n",
    "    last14 = candles[-14:]  # last 5 weekly bars\n",
    "\n",
    "    # Print a small info table\n",
    "    print(\"Last 5 weekly candles (MEXC spot, BTCUSDT, 1W):\")\n",
    "    print(\"Date (UTC)     |     Open       High        Low       Close        Vol\")\n",
    "    print(\"-\" * 74)\n",
    "    for c in last14:\n",
    "        print(f\"{c['time'].date()} | {fmt_num(c['open']).rjust(10)}  {fmt_num(c['high']).rjust(10)}  \"\n",
    "              f\"{fmt_num(c['low']).rjust(10)}  {fmt_num(c['close']).rjust(10)}  {fmt_num(c['volume']).rjust(10)}\")\n",
    "\n",
    "    print(\"\\nASCII chart (shared scale across the 14 bars):\")\n",
    "    chart, lo, hi = ascii_candles(last14, width=48)\n",
    "    # Put labels and lines together\n",
    "    for i, (c, line) in enumerate(zip(last14, chart)):\n",
    "        label = c[\"time\"].strftime(\"%Y-%m-%d\")\n",
    "        print(f\"{label} | {line} | O={fmt_num(c['open'])} C={fmt_num(c['close'])}\")\n",
    "\n",
    "    print(f\"\\nScale: low={fmt_num(lo)}  high={fmt_num(hi)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a30e9-779a-4d4c-9740-70f078638bd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Backtest a strategy\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "VS Wakeup Strategy Backtest - Binance Spot (Multiple Pairs)\n",
    "===========================================================\n",
    "\n",
    "Backtests the VS Wakeup composed strategy using actual project implementations\n",
    "on Binance spot data for the last 1000 bars across multiple trading pairs.\n",
    "\n",
    "Analyzes:\n",
    "- Highest price after signal (pivot high with 3-bar lookback)\n",
    "- Maximum drawdown before reaching highest price\n",
    "- Quick reversal analysis (max high in next 3 bars)\n",
    "- Performance metrics across all pairs\n",
    "\n",
    "Author: Market Scanner v2.10\n",
    "Date: 2024\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project path to import custom strategies\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"VS Wakeup Strategy Backtest - Binance Spot (Multi-Pair)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Import Custom Strategies from Project\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "try:\n",
    "    from custom_strategies import detect_consolidation, detect_confluence\n",
    "    print(\"✓ Successfully imported custom strategies from project\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import custom strategies: {e}\")\n",
    "    print(\"Please ensure you're running this notebook from the project directory\")\n",
    "    print(\"or adjust the project_dir path above\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Configuration\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Backtest Configuration\n",
    "TIMEFRAME = '1d'  # Binance supports: 1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d, 3d, 1w, 1M\n",
    "BARS_TO_FETCH = 1000  # Maximum bars to fetch for backtest\n",
    "LOOKBACK_BARS = 3  # Bars to look back for pivot high detection\n",
    "QUICK_REVERSAL_BARS = 3  # Bars to check for quick reversal after signal\n",
    "\n",
    "# Symbol Selection Options\n",
    "SYMBOL_MODE = 'top_volume'  # Options: 'top_volume', 'selected', 'all_major'\n",
    "\n",
    "# Pre-selected high-quality pairs (if using 'selected' mode)\n",
    "SELECTED_SYMBOLS = [\n",
    "    'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT',\n",
    "    'XRPUSDT', 'DOTUSDT', 'LINKUSDT', 'LTCUSDT', 'MATICUSDT',\n",
    "    'AVAXUSDT', 'ATOMUSDT', 'NEARUSDT', 'FTMUSDT', 'SANDUSDT'\n",
    "]\n",
    "\n",
    "# Major pairs for comprehensive testing (if using 'all_major' mode)  \n",
    "MAJOR_SYMBOLS = [\n",
    "    'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT', 'XRPUSDT', 'DOTUSDT',\n",
    "    'LINKUSDT', 'LTCUSDT', 'MATICUSDT', 'AVAXUSDT', 'ATOMUSDT', 'NEARUSDT', \n",
    "    'FTMUSDT', 'SANDUSDT', 'MANAUSDT', 'ALGOUSDT', 'VETUSDT', 'ICPUSDT', 'THETAUSDT',\n",
    "    'FILUSDT', 'TRXUSDT', 'EOSUSDT', 'XLMUSDT', 'AAVEUSDT', 'MKRUSDT', 'COMPUSDT',\n",
    "    'SUSHIUSDT', 'YFIUSDT', 'SNXUSDT', 'CRVUSDT', 'UNIUSDT', 'RENUSDT', 'ENJUSDT',\n",
    "    'CHZUSDT', 'BATUSDT', 'ZRXUSDT', 'STORJUSDT', 'OCEAUSDT', 'SKLUSDT'\n",
    "]\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Timeframe: {TIMEFRAME}\")\n",
    "print(f\"  Bars to fetch: {BARS_TO_FETCH}\")\n",
    "print(f\"  Symbol mode: {SYMBOL_MODE}\")\n",
    "print(f\"  Pivot lookback: {LOOKBACK_BARS} bars\")\n",
    "print(f\"  Quick reversal check: {QUICK_REVERSAL_BARS} bars\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Binance Data Client\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class BinanceDataClient:\n",
    "    \"\"\"Enhanced Binance client for backtesting with multiple pairs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://api.binance.com\"\n",
    "        self.session = None\n",
    "    \n",
    "    async def init_session(self):\n",
    "        \"\"\"Initialize aiohttp session\"\"\"\n",
    "        if self.session is None:\n",
    "            connector = aiohttp.TCPConnector(limit=100, limit_per_host=30)\n",
    "            timeout = aiohttp.ClientTimeout(total=30)\n",
    "            self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)\n",
    "    \n",
    "    async def close_session(self):\n",
    "        \"\"\"Close aiohttp session\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "            self.session = None\n",
    "    \n",
    "    async def get_exchange_info(self):\n",
    "        \"\"\"Get exchange information and active symbols\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        url = f\"{self.base_url}/api/v3/exchangeInfo\"\n",
    "        try:\n",
    "            async with self.session.get(url) as response:\n",
    "                data = await response.json()\n",
    "                \n",
    "                symbols = [\n",
    "                    item['symbol'] for item in data['symbols']\n",
    "                    if item['symbol'].endswith('USDT') and item['status'] == 'TRADING'\n",
    "                ]\n",
    "                \n",
    "                logger.info(f\"Found {len(symbols)} active USDT trading pairs\")\n",
    "                return symbols\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching exchange info: {e}\")\n",
    "            return []\n",
    "    \n",
    "    async def get_top_volume_symbols(self, limit=20):\n",
    "        \"\"\"Get top volume USDT pairs\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        url = f\"{self.base_url}/api/v3/ticker/24hr\"\n",
    "        try:\n",
    "            async with self.session.get(url) as response:\n",
    "                data = await response.json()\n",
    "                \n",
    "                # Filter USDT pairs with good volume\n",
    "                usdt_pairs = [\n",
    "                    {\n",
    "                        'symbol': item['symbol'],\n",
    "                        'volume': float(item['quoteVolume']),\n",
    "                        'count': int(item['count'])\n",
    "                    }\n",
    "                    for item in data \n",
    "                    if (item['symbol'].endswith('USDT') and \n",
    "                        item['status'] == 'TRADING' and\n",
    "                        float(item['quoteVolume']) > 5000000 and  # Min $5M volume\n",
    "                        int(item['count']) > 10000)  # Min trade count\n",
    "                ]\n",
    "                \n",
    "                # Sort by volume\n",
    "                usdt_pairs.sort(key=lambda x: x['volume'], reverse=True)\n",
    "                \n",
    "                symbols = [pair['symbol'] for pair in usdt_pairs[:limit]]\n",
    "                volumes = [pair['volume'] for pair in usdt_pairs[:limit]]\n",
    "                \n",
    "                logger.info(f\"Selected {len(symbols)} top volume pairs\")\n",
    "                logger.info(f\"Volume range: ${volumes[-1]:,.0f} - ${volumes[0]:,.0f}\")\n",
    "                \n",
    "                return symbols\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching volume data: {e}\")\n",
    "            return SELECTED_SYMBOLS[:limit]  # Fallback\n",
    "    \n",
    "    async def fetch_klines(self, symbol, interval=TIMEFRAME, limit=BARS_TO_FETCH):\n",
    "        \"\"\"Fetch historical kline data for a symbol\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        url = f\"{self.base_url}/api/v3/klines\"\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'limit': limit\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            async with self.session.get(url, params=params) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    \n",
    "                    if isinstance(data, list) and len(data) > 0:\n",
    "                        # Convert to DataFrame\n",
    "                        columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', \n",
    "                                  'close_time', 'quote_asset_volume', 'number_of_trades', \n",
    "                                  'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n",
    "                        \n",
    "                        df = pd.DataFrame(data, columns=columns)\n",
    "                        \n",
    "                        # Convert numeric columns\n",
    "                        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "                            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                        \n",
    "                        # Convert timestamp\n",
    "                        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "                        df.set_index('timestamp', inplace=True)\n",
    "                        \n",
    "                        # Keep only OHLCV\n",
    "                        df = df[['open', 'high', 'low', 'close', 'volume']].copy()\n",
    "                        df = df.sort_index()\n",
    "                        \n",
    "                        # Basic data validation\n",
    "                        if len(df) < 100:  # Need minimum data\n",
    "                            logger.warning(f\"Insufficient data for {symbol}: {len(df)} bars\")\n",
    "                            return None\n",
    "                        \n",
    "                        # Check for invalid prices\n",
    "                        if df['close'].isna().any() or (df['close'] <= 0).any():\n",
    "                            logger.warning(f\"Invalid price data for {symbol}\")\n",
    "                            return None\n",
    "                        \n",
    "                        logger.info(f\"✓ {symbol}: {len(df)} bars ({df.index[0].date()} to {df.index[-1].date()})\")\n",
    "                        return df\n",
    "                        \n",
    "                else:\n",
    "                    logger.error(f\"HTTP {response.status} for {symbol}\")\n",
    "                    return None\n",
    "                    \n",
    "        except asyncio.TimeoutError:\n",
    "            logger.error(f\"Timeout fetching {symbol}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching {symbol}: {e}\")\n",
    "            return None\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# VS Wakeup Strategy Implementation\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def detect_vs_wakeup_signal(df, check_bar=-1):\n",
    "    \"\"\"\n",
    "    VS Wakeup strategy using actual project implementations\n",
    "    \n",
    "    Args:\n",
    "        df: OHLCV DataFrame\n",
    "        check_bar: Bar index to check (-1 for last bar)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (detected, result_dict)\n",
    "    \"\"\"\n",
    "    if len(df) < 50:  # Need sufficient data\n",
    "        return False, {}\n",
    "    \n",
    "    try:\n",
    "        # Check consolidation (ongoing pattern, no breakout)\n",
    "        cons_detected, cons_result = detect_consolidation(df, check_bar=check_bar)\n",
    "        \n",
    "        if not cons_detected:\n",
    "            return False, {'reason': 'no_consolidation'}\n",
    "        \n",
    "        # Ensure it's not a breakout\n",
    "        if cons_result.get('breakout', False):\n",
    "            return False, {'reason': 'consolidation_breakout'}\n",
    "        \n",
    "        # Check confluence (wakeup signal within consolidation)\n",
    "        conf_detected, conf_result = detect_confluence(df, check_bar=check_bar, only_wakeup=True)\n",
    "        \n",
    "        if not conf_detected:\n",
    "            return False, {'reason': 'no_confluence'}\n",
    "        \n",
    "        # Combine results for VS Wakeup signal\n",
    "        idx = check_bar if check_bar >= 0 else len(df) + check_bar\n",
    "        \n",
    "        result = {\n",
    "            'timestamp': df.index[idx],\n",
    "            'signal_type': 'vs_wakeup',\n",
    "            'symbol': None,  # Will be set by caller\n",
    "            'close_price': df.iloc[idx]['close'],\n",
    "            'volume_usd': df.iloc[idx]['volume'] * df.iloc[idx]['close'],\n",
    "            'direction': conf_result.get('direction', 'Up'),\n",
    "            \n",
    "            # Consolidation info\n",
    "            'box_age': cons_result.get('box_age', 0),\n",
    "            'consolidation_height_pct': cons_result.get('height_pct', 0),\n",
    "            \n",
    "            # Confluence info  \n",
    "            'volume_ratio': conf_result.get('volume_ratio', 1.0),\n",
    "            'momentum_score': conf_result.get('momentum_score', 0),\n",
    "            'close_off_low': conf_result.get('close_off_low', 50.0),\n",
    "            'extreme_volume': conf_result.get('extreme_volume', False),\n",
    "            'extreme_spread': conf_result.get('extreme_spread', False),\n",
    "            \n",
    "            # Technical details\n",
    "            'bar_index': idx,\n",
    "            'current_bar': (idx == len(df) - 1)\n",
    "        }\n",
    "        \n",
    "        return True, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in VS Wakeup detection: {e}\")\n",
    "        return False, {'reason': f'error: {str(e)}'}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Performance Analysis Functions\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def find_pivot_high(df, start_idx, lookback=LOOKBACK_BARS, min_bars_ahead=10):\n",
    "    \"\"\"\n",
    "    Find the highest pivot high after a signal with lookback confirmation\n",
    "    \n",
    "    Args:\n",
    "        df: Price DataFrame\n",
    "        start_idx: Index where signal occurred  \n",
    "        lookback: Bars to look back for pivot confirmation\n",
    "        min_bars_ahead: Minimum bars to look ahead\n",
    "    \n",
    "    Returns:\n",
    "        dict: Pivot high analysis results\n",
    "    \"\"\"\n",
    "    if start_idx >= len(df) - min_bars_ahead:\n",
    "        return {\n",
    "            'pivot_high_price': np.nan,\n",
    "            'pivot_high_index': np.nan,\n",
    "            'pivot_high_date': None,\n",
    "            'bars_to_pivot': np.nan,\n",
    "            'gain_to_pivot_pct': np.nan\n",
    "        }\n",
    "    \n",
    "    signal_price = df.iloc[start_idx]['close']\n",
    "    search_start = start_idx + 1\n",
    "    search_end = len(df)\n",
    "    \n",
    "    best_pivot_high = signal_price\n",
    "    best_pivot_idx = start_idx\n",
    "    \n",
    "    # Search for pivot highs\n",
    "    for i in range(search_start + lookback, search_end - lookback):\n",
    "        current_high = df.iloc[i]['high']\n",
    "        \n",
    "        # Check if this is a pivot high (higher than lookback bars on both sides)\n",
    "        is_pivot = True\n",
    "        \n",
    "        # Check left side (lookback bars)\n",
    "        for j in range(i - lookback, i):\n",
    "            if df.iloc[j]['high'] >= current_high:\n",
    "                is_pivot = False\n",
    "                break\n",
    "        \n",
    "        # Check right side (lookback bars)\n",
    "        if is_pivot:\n",
    "            for j in range(i + 1, min(i + lookback + 1, search_end)):\n",
    "                if df.iloc[j]['high'] >= current_high:\n",
    "                    is_pivot = False\n",
    "                    break\n",
    "        \n",
    "        # Update best pivot if this is higher\n",
    "        if is_pivot and current_high > best_pivot_high:\n",
    "            best_pivot_high = current_high\n",
    "            best_pivot_idx = i\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bars_to_pivot = best_pivot_idx - start_idx\n",
    "    gain_pct = ((best_pivot_high - signal_price) / signal_price * 100) if signal_price > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'pivot_high_price': best_pivot_high,\n",
    "        'pivot_high_index': best_pivot_idx,\n",
    "        'pivot_high_date': df.index[best_pivot_idx] if best_pivot_idx < len(df) else None,\n",
    "        'bars_to_pivot': bars_to_pivot,\n",
    "        'gain_to_pivot_pct': gain_pct\n",
    "    }\n",
    "\n",
    "def calculate_max_drawdown(df, start_idx, end_idx):\n",
    "    \"\"\"\n",
    "    Calculate maximum drawdown from signal to pivot high, and highest gain before drawdown\n",
    "    \n",
    "    Args:\n",
    "        df: Price DataFrame\n",
    "        start_idx: Signal bar index\n",
    "        end_idx: Target bar index (pivot high)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Enhanced drawdown analysis\n",
    "    \"\"\"\n",
    "    if start_idx >= end_idx or end_idx >= len(df):\n",
    "        return {\n",
    "            'max_drawdown_pct': 0.0,\n",
    "            'max_drawdown_price': df.iloc[start_idx]['close'],\n",
    "            'max_drawdown_index': start_idx,\n",
    "            'max_drawdown_date': df.index[start_idx],\n",
    "            'bars_to_max_dd': 0,\n",
    "            'highest_gain_before_dd_pct': 0.0,\n",
    "            'highest_gain_before_dd_price': df.iloc[start_idx]['close'],\n",
    "            'highest_gain_before_dd_date': df.index[start_idx],\n",
    "            'stopped_before_target': False\n",
    "        }\n",
    "    \n",
    "    signal_price = df.iloc[start_idx]['close']\n",
    "    \n",
    "    max_drawdown_pct = 0.0\n",
    "    max_drawdown_price = signal_price\n",
    "    max_drawdown_idx = start_idx\n",
    "    \n",
    "    highest_price_before_dd = signal_price\n",
    "    highest_gain_before_dd_pct = 0.0\n",
    "    highest_gain_before_dd_idx = start_idx\n",
    "    \n",
    "    # Track running high and drawdown\n",
    "    running_high = signal_price\n",
    "    \n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        current_high = df.iloc[i]['high']\n",
    "        current_low = df.iloc[i]['low']\n",
    "        \n",
    "        # Update running high\n",
    "        if current_high > running_high:\n",
    "            running_high = current_high\n",
    "        \n",
    "        # Calculate drawdown from running high (not just signal price)\n",
    "        drawdown_from_high = ((running_high - current_low) / running_high * 100) if running_high > 0 else 0\n",
    "        drawdown_from_signal = ((signal_price - current_low) / signal_price * 100) if signal_price > 0 else 0\n",
    "        \n",
    "        # Track maximum drawdown from signal price\n",
    "        if drawdown_from_signal > max_drawdown_pct:\n",
    "            # Before updating max drawdown, capture the highest gain up to this point\n",
    "            highest_price_before_dd = running_high\n",
    "            highest_gain_before_dd_pct = ((running_high - signal_price) / signal_price * 100) if signal_price > 0 else 0\n",
    "            highest_gain_before_dd_idx = i\n",
    "            \n",
    "            # Update max drawdown\n",
    "            max_drawdown_pct = drawdown_from_signal\n",
    "            max_drawdown_price = current_low\n",
    "            max_drawdown_idx = i\n",
    "    \n",
    "    # Check if stopped before reaching target (assuming 10% drawdown stops you out)\n",
    "    stop_loss_threshold = 10.0\n",
    "    stopped_before_target = max_drawdown_pct >= stop_loss_threshold\n",
    "    \n",
    "    return {\n",
    "        'max_drawdown_pct': max_drawdown_pct,\n",
    "        'max_drawdown_price': max_drawdown_price,\n",
    "        'max_drawdown_index': max_drawdown_idx,\n",
    "        'max_drawdown_date': df.index[max_drawdown_idx],\n",
    "        'bars_to_max_dd': max_drawdown_idx - start_idx,\n",
    "        'highest_gain_before_dd_pct': highest_gain_before_dd_pct,\n",
    "        'highest_gain_before_dd_price': highest_price_before_dd,\n",
    "        'highest_gain_before_dd_date': df.index[highest_gain_before_dd_idx],\n",
    "        'stopped_before_target': stopped_before_target,\n",
    "        'stop_loss_threshold_used': stop_loss_threshold\n",
    "    }\n",
    "\n",
    "def analyze_quick_reversal(df, start_idx, bars_ahead=QUICK_REVERSAL_BARS):\n",
    "    \"\"\"\n",
    "    Analyze quick reversal potential in next N bars\n",
    "    \n",
    "    Args:\n",
    "        df: Price DataFrame\n",
    "        start_idx: Signal bar index\n",
    "        bars_ahead: Number of bars to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Quick reversal analysis\n",
    "    \"\"\"\n",
    "    if start_idx >= len(df) - bars_ahead:\n",
    "        return {\n",
    "            'quick_reversal_max_high': np.nan,\n",
    "            'quick_reversal_gain_pct': np.nan,\n",
    "            'quick_reversal_occurred': False\n",
    "        }\n",
    "    \n",
    "    signal_price = df.iloc[start_idx]['close']\n",
    "    \n",
    "    max_high = signal_price\n",
    "    for i in range(start_idx + 1, min(start_idx + bars_ahead + 1, len(df))):\n",
    "        max_high = max(max_high, df.iloc[i]['high'])\n",
    "    \n",
    "    gain_pct = ((max_high - signal_price) / signal_price * 100) if signal_price > 0 else 0\n",
    "    quick_reversal = gain_pct >= 2.0  # 2% threshold for quick reversal\n",
    "    \n",
    "    return {\n",
    "        'quick_reversal_max_high': max_high,\n",
    "        'quick_reversal_gain_pct': gain_pct,\n",
    "        'quick_reversal_occurred': quick_reversal\n",
    "    }\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Backtesting Engine\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "async def backtest_symbol(client, symbol):\n",
    "    \"\"\"\n",
    "    Run backtest on a single symbol\n",
    "    \n",
    "    Args:\n",
    "        client: BinanceDataClient instance\n",
    "        symbol: Trading pair symbol\n",
    "    \n",
    "    Returns:\n",
    "        list: List of signal results\n",
    "    \"\"\"\n",
    "    logger.info(f\"Backtesting {symbol}...\")\n",
    "    \n",
    "    # Fetch data\n",
    "    df = await client.fetch_klines(symbol, TIMEFRAME, BARS_TO_FETCH)\n",
    "    if df is None or len(df) < 100:\n",
    "        logger.warning(f\"Skipping {symbol} - insufficient data\")\n",
    "        return []\n",
    "    \n",
    "    signals = []\n",
    "    \n",
    "    # Scan for VS Wakeup signals (leave last 20 bars for forward analysis)\n",
    "    scan_end = len(df) - 20\n",
    "    \n",
    "    for i in range(50, scan_end):  # Start after 50 bars for indicator warmup\n",
    "        detected, result = detect_vs_wakeup_signal(df, check_bar=i)\n",
    "        \n",
    "        if detected:\n",
    "            result['symbol'] = symbol\n",
    "            \n",
    "            # Performance analysis\n",
    "            pivot_analysis = find_pivot_high(df, i, LOOKBACK_BARS)\n",
    "            drawdown_analysis = calculate_max_drawdown(\n",
    "                df, i, pivot_analysis['pivot_high_index']\n",
    "            )\n",
    "            quick_reversal = analyze_quick_reversal(df, i, QUICK_REVERSAL_BARS)\n",
    "            \n",
    "            # Combine all analysis\n",
    "            signal_result = {\n",
    "                **result,\n",
    "                **pivot_analysis,\n",
    "                **drawdown_analysis,\n",
    "                **quick_reversal\n",
    "            }\n",
    "            \n",
    "            signals.append(signal_result)\n",
    "            \n",
    "            logger.info(f\"  Signal {len(signals)}: {result['timestamp'].date()} \"\n",
    "                       f\"Gain: {pivot_analysis['gain_to_pivot_pct']:.2f}% \"\n",
    "                       f\"DD: -{drawdown_analysis['max_drawdown_pct']:.2f}%\")\n",
    "    \n",
    "    logger.info(f\"✓ {symbol}: {len(signals)} signals found\")\n",
    "    return signals\n",
    "\n",
    "async def run_backtest():\n",
    "    \"\"\"\n",
    "    Main backtest runner for multiple symbols\n",
    "    \"\"\"\n",
    "    print(\"\\nInitializing backtest...\")\n",
    "    \n",
    "    client = BinanceDataClient()\n",
    "    await client.init_session()\n",
    "    \n",
    "    try:\n",
    "        # Select symbols based on mode\n",
    "        if SYMBOL_MODE == 'top_volume':\n",
    "            symbols = await client.get_top_volume_symbols(20)\n",
    "        elif SYMBOL_MODE == 'selected':\n",
    "            symbols = SELECTED_SYMBOLS\n",
    "        elif SYMBOL_MODE == 'all_major':\n",
    "            symbols = MAJOR_SYMBOLS\n",
    "        else:\n",
    "            symbols = SELECTED_SYMBOLS\n",
    "        \n",
    "        print(f\"\\nSelected {len(symbols)} symbols for backtesting:\")\n",
    "        print(f\"{', '.join(symbols)}\")\n",
    "        print(f\"Estimated time: ~{len(symbols) * 2} seconds\\n\")\n",
    "        \n",
    "        # Run backtests\n",
    "        all_signals = []\n",
    "        \n",
    "        for i, symbol in enumerate(symbols, 1):\n",
    "            print(f\"[{i}/{len(symbols)}] Processing {symbol}...\")\n",
    "            \n",
    "            try:\n",
    "                signals = await backtest_symbol(client, symbol)\n",
    "                all_signals.extend(signals)\n",
    "                \n",
    "                # Brief pause to respect API limits\n",
    "                await asyncio.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {symbol}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return all_signals\n",
    "        \n",
    "    finally:\n",
    "        await client.close_session()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Results Analysis and Export\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def analyze_results(signals):\n",
    "    \"\"\"\n",
    "    Analyze backtest results and generate summary statistics\n",
    "    \n",
    "    Args:\n",
    "        signals: List of signal dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis summary\n",
    "    \"\"\"\n",
    "    if not signals:\n",
    "        return {\n",
    "            'total_signals': 0,\n",
    "            'summary': 'No signals found'\n",
    "        }\n",
    "    \n",
    "    df_signals = pd.DataFrame(signals)\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_signals = len(df_signals)\n",
    "    symbols_with_signals = df_signals['symbol'].nunique()\n",
    "    \n",
    "    # Performance metrics\n",
    "    avg_gain = df_signals['gain_to_pivot_pct'].mean()\n",
    "    median_gain = df_signals['gain_to_pivot_pct'].median()\n",
    "    max_gain = df_signals['gain_to_pivot_pct'].max()\n",
    "    min_gain = df_signals['gain_to_pivot_pct'].min()\n",
    "    \n",
    "    # Drawdown metrics\n",
    "    avg_drawdown = df_signals['max_drawdown_pct'].mean()\n",
    "    max_drawdown = df_signals['max_drawdown_pct'].max()\n",
    "    \n",
    "    # Time metrics\n",
    "    avg_bars_to_pivot = df_signals['bars_to_pivot'].mean()\n",
    "    \n",
    "    # Success metrics\n",
    "    profitable_signals = (df_signals['gain_to_pivot_pct'] > 0).sum()\n",
    "    success_rate = profitable_signals / total_signals * 100\n",
    "    \n",
    "    # Quick reversal analysis\n",
    "    quick_reversals = df_signals['quick_reversal_occurred'].sum()\n",
    "    quick_reversal_rate = quick_reversals / total_signals * 100\n",
    "    \n",
    "    return {\n",
    "        'total_signals': total_signals,\n",
    "        'symbols_with_signals': symbols_with_signals,\n",
    "        'success_rate': success_rate,\n",
    "        'avg_gain_pct': avg_gain,\n",
    "        'median_gain_pct': median_gain,\n",
    "        'max_gain_pct': max_gain,\n",
    "        'min_gain_pct': min_gain,\n",
    "        'avg_drawdown_pct': avg_drawdown,\n",
    "        'max_drawdown_pct': max_drawdown,\n",
    "        'avg_bars_to_pivot': avg_bars_to_pivot,\n",
    "        'quick_reversal_count': quick_reversals,\n",
    "        'quick_reversal_rate': quick_reversal_rate,\n",
    "        'df_signals': df_signals\n",
    "    }\n",
    "\n",
    "def save_results_to_csv(signals, filename=None):\n",
    "    \"\"\"\n",
    "    Save backtest results to CSV file\n",
    "    \n",
    "    Args:\n",
    "        signals: List of signal dictionaries\n",
    "        filename: CSV filename (auto-generated if None)\n",
    "    \n",
    "    Returns:\n",
    "        str: Filename of saved file\n",
    "    \"\"\"\n",
    "    if not signals:\n",
    "        print(\"No signals to save\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(signals)\n",
    "    \n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"vs_wakeup_backtest_{TIMEFRAME}_{timestamp}.csv\"\n",
    "    \n",
    "    # Select key columns for CSV export\n",
    "    export_columns = [\n",
    "        'symbol', 'timestamp', 'signal_type', 'direction',\n",
    "        'close_price', 'volume_usd', 'box_age',\n",
    "        'pivot_high_price', 'pivot_high_date', 'bars_to_pivot', 'gain_to_pivot_pct',\n",
    "        'max_drawdown_pct', 'max_drawdown_date', 'bars_to_max_dd',\n",
    "        'highest_gain_before_dd_pct', 'highest_gain_before_dd_price', 'highest_gain_before_dd_date',\n",
    "        'stopped_before_target', 'stop_loss_threshold_used',\n",
    "        'quick_reversal_max_high', 'quick_reversal_gain_pct', 'quick_reversal_occurred',\n",
    "        'volume_ratio', 'momentum_score', 'close_off_low', 'extreme_volume', 'extreme_spread'\n",
    "    ]\n",
    "    \n",
    "    # Filter columns that exist\n",
    "    available_columns = [col for col in export_columns if col in df.columns]\n",
    "    df_export = df[available_columns].copy()\n",
    "    \n",
    "    # Format timestamp columns\n",
    "    for col in ['timestamp', 'pivot_high_date', 'max_drawdown_date']:\n",
    "        if col in df_export.columns:\n",
    "            df_export[col] = pd.to_datetime(df_export[col]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Round numeric columns\n",
    "    numeric_columns = df_export.select_dtypes(include=[np.number]).columns\n",
    "    df_export[numeric_columns] = df_export[numeric_columns].round(4)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df_export.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"✓ Results saved to: {filename}\")\n",
    "    print(f\"  Total signals: {len(df_export)}\")\n",
    "    print(f\"  Columns: {len(df_export.columns)}\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Main Execution\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(f\"\\nStarting VS Wakeup backtest...\")\n",
    "    print(f\"Timeframe: {TIMEFRAME} | Bars: {BARS_TO_FETCH} | Mode: {SYMBOL_MODE}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Run backtest\n",
    "        signals = await run_backtest()\n",
    "        \n",
    "        # Analyze results\n",
    "        analysis = analyze_results(signals)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"BACKTEST RESULTS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if analysis['total_signals'] > 0:\n",
    "            print(f\"Total signals found: {analysis['total_signals']}\")\n",
    "            print(f\"Symbols with signals: {analysis['symbols_with_signals']}\")\n",
    "            print(f\"Success rate: {analysis['success_rate']:.1f}%\")\n",
    "            print(f\"Average gain: {analysis['avg_gain_pct']:.2f}%\")\n",
    "            print(f\"Median gain: {analysis['median_gain_pct']:.2f}%\")\n",
    "            print(f\"Best gain: {analysis['max_gain_pct']:.2f}%\")\n",
    "            print(f\"Worst result: {analysis['min_gain_pct']:.2f}%\")\n",
    "            print(f\"Average drawdown: {analysis['avg_drawdown_pct']:.2f}%\")\n",
    "            print(f\"Max drawdown: {analysis['max_drawdown_pct']:.2f}%\")\n",
    "            print(f\"Avg bars to pivot: {analysis['avg_bars_to_pivot']:.1f}\")\n",
    "            print(f\"Quick reversals: {analysis['quick_reversal_count']} ({analysis['quick_reversal_rate']:.1f}%)\")\n",
    "            \n",
    "            # Save to CSV\n",
    "            csv_file = save_results_to_csv(signals)\n",
    "            \n",
    "            # Top performers - show more comprehensive metrics\n",
    "            df = analysis['df_signals']\n",
    "            top_signals = df.nlargest(5, 'gain_to_pivot_pct')[\n",
    "                ['symbol', 'timestamp', 'gain_to_pivot_pct', 'max_drawdown_pct', \n",
    "                 'highest_gain_before_dd_pct', 'stopped_before_target', 'bars_to_pivot']\n",
    "            ]\n",
    "            print(f\"\\nTop 5 performing signals:\")\n",
    "            print(\"Symbol    Date        Total_Gain%  Max_DD%  Gain_Before_DD%  Stopped  Days_to_Peak\")\n",
    "            print(\"-\" * 80)\n",
    "            for _, row in top_signals.iterrows():\n",
    "                stopped_status = \"YES\" if row.get('stopped_before_target', False) else \"NO\"\n",
    "                print(f\"{row['symbol']:<8} {row['timestamp'].date()!s:<11} \"\n",
    "                      f\"{row['gain_to_pivot_pct']:>8.1f}%  {row['max_drawdown_pct']:>6.1f}%  \"\n",
    "                      f\"{row.get('highest_gain_before_dd_pct', 0):>12.1f}%   {stopped_status:<7} \"\n",
    "                      f\"{row.get('bars_to_pivot', 0):>6.0f}\")\n",
    "            \n",
    "            # Risk analysis\n",
    "            stopped_count = df['stopped_before_target'].sum() if 'stopped_before_target' in df.columns else 0\n",
    "            stopped_rate = (stopped_count / len(df) * 100) if len(df) > 0 else 0\n",
    "            avg_gain_before_dd = df['highest_gain_before_dd_pct'].mean() if 'highest_gain_before_dd_pct' in df.columns else 0\n",
    "            \n",
    "            print(f\"\\nRisk Analysis:\")\n",
    "            print(f\"Signals stopped out (10% DD): {stopped_count} ({stopped_rate:.1f}%)\")\n",
    "            print(f\"Average gain before max DD: {avg_gain_before_dd:.2f}%\")\n",
    "            \n",
    "            # Symbol performance summary\n",
    "            symbol_summary = df.groupby('symbol').agg({\n",
    "                'gain_to_pivot_pct': ['count', 'mean', 'max'],\n",
    "                'max_drawdown_pct': 'mean',\n",
    "                'highest_gain_before_dd_pct': 'mean',\n",
    "                'stopped_before_target': 'sum'\n",
    "            }).round(2)\n",
    "            symbol_summary.columns = ['Signal_Count', 'Avg_Total_Gain_%', 'Best_Total_Gain_%', \n",
    "                                    'Avg_DD_%', 'Avg_Gain_Before_DD_%', 'Stopped_Count']\n",
    "            \n",
    "            print(f\"\\nSymbol Performance Summary:\")\n",
    "            print(symbol_summary.sort_values('Avg_Gain_%', ascending=False).head(10).to_string())\n",
    "            \n",
    "        else:\n",
    "            print(\"No VS Wakeup signals found in the backtested data\")\n",
    "            print(\"This could indicate:\")\n",
    "            print(\"- Strategy parameters may be too strict\")\n",
    "            print(\"- Selected timeframe may not be optimal for this strategy\")  \n",
    "            print(\"- Market conditions in the backtested period may not favor this pattern\")\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\nBacktest completed in {duration.total_seconds():.1f} seconds\")\n",
    "        \n",
    "        return signals\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Backtest failed: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Execution Cell\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the backtest\n",
    "    signals = await main()\n",
    "    \n",
    "    print(f\"\\nBacktest complete! Found {len(signals)} signals.\")\n",
    "    \n",
    "    if signals:\n",
    "        print(f\"\\nTo analyze further:\")\n",
    "        print(f\"- CSV file contains all signal details\")\n",
    "        print(f\"- Check 'gain_to_pivot_pct' for profitability\")\n",
    "        print(f\"- Review 'max_drawdown_pct' for risk assessment\")\n",
    "        print(f\"- Examine 'quick_reversal_occurred' for short-term opportunities\")\n",
    "        \n",
    "        # Quick data access\n",
    "        df_results = pd.DataFrame(signals)\n",
    "        \n",
    "        print(f\"\\nResults DataFrame available as 'df_results'\")\n",
    "        print(f\"Shape: {df_results.shape}\")\n",
    "        print(f\"Columns: {list(df_results.columns)}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nSample results:\")\n",
    "        sample_cols = ['symbol', 'timestamp', 'direction', 'gain_to_pivot_pct', 'max_drawdown_pct', 'quick_reversal_occurred']\n",
    "        available_cols = [col for col in sample_cols if col in df_results.columns]\n",
    "        print(df_results[available_cols].head().to_string(index=False))\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nNo signals found. Consider adjusting:\")\n",
    "        print(f\"- TIMEFRAME (try '4h' or '1h' for more frequent signals)\")\n",
    "        print(f\"- SYMBOL_MODE (try 'all_major' for broader coverage)\")  \n",
    "        print(f\"- Strategy parameters in detect_consolidation/detect_confluence\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Additional Analysis Functions (Optional Usage)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def plot_signal_distribution(df_signals):\n",
    "    \"\"\"\n",
    "    Plot distribution of signal performance (requires matplotlib)\n",
    "    \n",
    "    Usage: plot_signal_distribution(df_results)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Gain distribution\n",
    "        axes[0,0].hist(df_signals['gain_to_pivot_pct'], bins=20, alpha=0.7, color='green')\n",
    "        axes[0,0].axvline(df_signals['gain_to_pivot_pct'].mean(), color='red', linestyle='--', label='Mean')\n",
    "        axes[0,0].set_title('Gain to Pivot Distribution')\n",
    "        axes[0,0].set_xlabel('Gain %')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Drawdown distribution\n",
    "        axes[0,1].hist(df_signals['max_drawdown_pct'], bins=20, alpha=0.7, color='red')\n",
    "        axes[0,1].axvline(df_signals['max_drawdown_pct'].mean(), color='blue', linestyle='--', label='Mean')\n",
    "        axes[0,1].set_title('Max Drawdown Distribution')\n",
    "        axes[0,1].set_xlabel('Drawdown %')\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Bars to pivot\n",
    "        axes[1,0].hist(df_signals['bars_to_pivot'], bins=20, alpha=0.7, color='blue')\n",
    "        axes[1,0].set_title('Bars to Pivot High Distribution')\n",
    "        axes[1,0].set_xlabel('Bars')\n",
    "        \n",
    "        # Gain vs Drawdown scatter\n",
    "        axes[1,1].scatter(df_signals['max_drawdown_pct'], df_signals['gain_to_pivot_pct'], alpha=0.6)\n",
    "        axes[1,1].set_xlabel('Max Drawdown %')\n",
    "        axes[1,1].set_ylabel('Gain to Pivot %')\n",
    "        axes[1,1].set_title('Risk vs Reward')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"matplotlib not available. Install with: pip install matplotlib\")\n",
    "\n",
    "def analyze_by_symbol(df_signals):\n",
    "    \"\"\"\n",
    "    Detailed analysis by symbol\n",
    "    \n",
    "    Usage: analyze_by_symbol(df_results)\n",
    "    \"\"\"\n",
    "    symbol_analysis = df_signals.groupby('symbol').agg({\n",
    "        'gain_to_pivot_pct': ['count', 'mean', 'median', 'std', 'min', 'max'],\n",
    "        'max_drawdown_pct': ['mean', 'max'],\n",
    "        'bars_to_pivot': 'mean',\n",
    "        'quick_reversal_occurred': ['sum', 'mean']\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    symbol_analysis.columns = ['_'.join(col).strip() for col in symbol_analysis.columns]\n",
    "    \n",
    "    # Add success rate\n",
    "    symbol_analysis['success_rate'] = (\n",
    "        df_signals.groupby('symbol')['gain_to_pivot_pct'].apply(lambda x: (x > 0).mean() * 100)\n",
    "    ).round(1)\n",
    "    \n",
    "    # Sort by average gain\n",
    "    symbol_analysis = symbol_analysis.sort_values('gain_to_pivot_pct_mean', ascending=False)\n",
    "    \n",
    "    print(\"Detailed Symbol Analysis:\")\n",
    "    print(symbol_analysis.to_string())\n",
    "    \n",
    "    return symbol_analysis\n",
    "\n",
    "def export_for_tradingview(df_signals, symbol='BTCUSDT'):\n",
    "    \"\"\"\n",
    "    Export signals for a specific symbol in TradingView format\n",
    "    \n",
    "    Usage: export_for_tradingview(df_results, 'BTCUSDT')\n",
    "    \"\"\"\n",
    "    symbol_signals = df_signals[df_signals['symbol'] == symbol].copy()\n",
    "    \n",
    "    if len(symbol_signals) == 0:\n",
    "        print(f\"No signals found for {symbol}\")\n",
    "        return\n",
    "    \n",
    "    # Format for TradingView\n",
    "    tv_format = symbol_signals[['timestamp', 'close_price', 'direction', 'gain_to_pivot_pct']].copy()\n",
    "    tv_format['timestamp'] = tv_format['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    filename = f\"vs_wakeup_{symbol}_tradingview.csv\"\n",
    "    tv_format.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"TradingView format exported to: {filename}\")\n",
    "    print(f\"Signals for {symbol}: {len(tv_format)}\")\n",
    "    \n",
    "    return tv_format\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Usage Instructions\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(f\"\"\"\n",
    "Usage Instructions:\n",
    "==================\n",
    "\n",
    "1. Run the main backtest:\n",
    "   signals = await main()\n",
    "\n",
    "2. Analyze results:\n",
    "   df_results = pd.DataFrame(signals)\n",
    "   \n",
    "3. Optional analysis functions:\n",
    "   - plot_signal_distribution(df_results)  # Requires matplotlib\n",
    "   - analyze_by_symbol(df_results)         # Detailed symbol breakdown\n",
    "   - export_for_tradingview(df_results, 'BTCUSDT')  # Export for TV\n",
    "\n",
    "4. Configuration options (modify at top):\n",
    "   - TIMEFRAME: '1d', '4h', '1h' (Binance supported intervals)\n",
    "   - SYMBOL_MODE: 'top_volume', 'selected', 'all_major'  \n",
    "   - BARS_TO_FETCH: Number of historical bars (max 1000)\n",
    "\n",
    "5. CSV output contains:\n",
    "   - Signal details (timestamp, symbol, direction)\n",
    "   - Performance metrics (gain to pivot, max drawdown)\n",
    "   - Technical indicators (volume ratio, momentum score)\n",
    "   - Quick reversal analysis (3-bar forward look)\n",
    "\n",
    "Note: Ensure you're in the project directory with custom_strategies available.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3249d-404c-40d2-9c2a-a413f4f1b0ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TEST ANY PAIR ON ANY EXCHANGE ON ANY TIMEFRAME\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Real-Time Strategy Checker - All Strategies Scanner\n",
    "===================================================\n",
    "\n",
    "Interactive notebook to check all native and composed strategies against any trading pair\n",
    "from your project workspace. Tests both current and last closed bars.\n",
    "\n",
    "Strategies Tested:\n",
    "- Native: confluence, consolidation_breakout, channel_breakout, loaded_bar, \n",
    "         trend_breakout, pin_up, sma50_breakout\n",
    "- Composed: hbs_breakout, vs_wakeup\n",
    "- Futures-only: reversal_bar, pin_down\n",
    "\n",
    "Usage:\n",
    "1. Set SYMBOL, EXCHANGE, TIMEFRAME variables\n",
    "2. Run the scan\n",
    "3. Get ✅❌ results for all strategies\n",
    "\n",
    "Author: Market Scanner v2.10\n",
    "Date: 2024\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project path\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "print(\"Real-Time Strategy Checker - All Strategies Scanner\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Configuration - MODIFY THESE VALUES\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Trading pair to analyze\n",
    "SYMBOL = \"XAIUSDT\"\n",
    "\n",
    "# Exchange to use\n",
    "EXCHANGE = \"binance_spot\"  # Options: binance_spot, binance_futures, bybit_spot, etc.\n",
    "\n",
    "# Timeframe to analyze  \n",
    "TIMEFRAME = \"1w\"  # Options: 1h, 4h, 1d, 2d, 3d, 4d, 1w\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Import Project Components\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "try:\n",
    "    # Import exchange clients\n",
    "    from exchanges import (\n",
    "        BinanceSpotClient, BinanceFuturesClient, BybitSpotClient, BybitFuturesClient,\n",
    "        GateioSpotClient, GateioFuturesClient, KucoinSpotClient, MexcSpotClient, MexcFuturesClient\n",
    "    )\n",
    "    \n",
    "    # Import all available custom strategies from your project\n",
    "    from custom_strategies import (\n",
    "        detect_volume_surge, detect_weak_uptrend, detect_pin_down, detect_confluence,\n",
    "        detect_consolidation, detect_channel, detect_consolidation_breakout,\n",
    "        detect_channel_breakout, detect_wedge_breakout, detect_sma50_breakout,\n",
    "        detect_trend_breakout, detect_pin_up\n",
    "    )\n",
    "    \n",
    "    # Import VSA strategies\n",
    "    vsa_available = []\n",
    "    try:\n",
    "        from breakout_vsa.strategies.loaded_bar import get_params as get_loaded_bar_params\n",
    "        from breakout_vsa.strategies.reversal_bar import get_params as get_reversal_bar_params\n",
    "        from breakout_vsa.core import vsa_detector\n",
    "        vsa_available.extend(['loaded_bar', 'reversal_bar'])\n",
    "    except ImportError:\n",
    "        print(\"Note: VSA strategies not available\")\n",
    "    \n",
    "    print(f\"✓ Successfully imported all project components\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Strategy Definitions based on your actual project structure\n",
    "NATIVE_STRATEGIES = [\n",
    "    'confluence',\n",
    "    'consolidation_breakout', \n",
    "    'channel_breakout',\n",
    "    'loaded_bar',\n",
    "    'trend_breakout',\n",
    "    'pin_up',\n",
    "    'sma50_breakout'\n",
    "]\n",
    "\n",
    "COMPOSED_STRATEGIES = [\n",
    "    'hbs_breakout',\n",
    "    'vs_wakeup'\n",
    "]\n",
    "\n",
    "FUTURES_ONLY_STRATEGIES = [\n",
    "    'reversal_bar',\n",
    "    'pin_down'\n",
    "]\n",
    "\n",
    "ALL_STRATEGIES = NATIVE_STRATEGIES + COMPOSED_STRATEGIES + FUTURES_ONLY_STRATEGIES\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Data Fetching\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "async def fetch_market_data(symbol, exchange, timeframe):\n",
    "    \"\"\"\n",
    "    Fetch market data using project exchange clients\n",
    "    \n",
    "    Args:\n",
    "        symbol: Trading pair symbol\n",
    "        exchange: Exchange name\n",
    "        timeframe: Timeframe string\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: OHLCV data or None if error\n",
    "    \"\"\"\n",
    "    if exchange not in EXCHANGE_CLIENTS:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Initialize exchange client\n",
    "        ClientClass = EXCHANGE_CLIENTS[exchange]\n",
    "        client = ClientClass(timeframe=timeframe)\n",
    "        \n",
    "        await client.init_session()\n",
    "        df = await client.fetch_klines(symbol)\n",
    "        await client.close_session()\n",
    "        \n",
    "        if df is not None and len(df) > 50:\n",
    "            return df\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Strategy Testing Functions\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_native_strategy(strategy_name, df, check_bar):\n",
    "    \"\"\"Test a native strategy\"\"\"\n",
    "    try:\n",
    "        if strategy_name == 'confluence':\n",
    "            detected, result = detect_confluence(df, check_bar=check_bar)\n",
    "        elif strategy_name == 'consolidation_breakout':\n",
    "            detected, result = detect_consolidation_breakout(df, check_bar=check_bar)\n",
    "        elif strategy_name == 'channel_breakout':\n",
    "            detected, result = detect_channel_breakout(df, check_bar=check_bar)\n",
    "        elif strategy_name == 'trend_breakout':\n",
    "            detected, result = detect_trend_breakout(df, check_bar=check_bar)\n",
    "        elif strategy_name == 'pin_up':\n",
    "            detected, result = detect_pin_up(df, check_bar=check_bar)\n",
    "        elif strategy_name == 'sma50_breakout':\n",
    "            detected, result = detect_sma50_breakout(df, check_bar=check_bar)\n",
    "        elif strategy_name == 'loaded_bar':\n",
    "            # VSA loaded bar\n",
    "            params = get_loaded_bar_params()\n",
    "            condition, vsa_result = vsa_detector(df, params)\n",
    "            detected = condition.iloc[check_bar] if len(condition) > abs(check_bar) else False\n",
    "            result = {'vsa_type': 'loaded_bar'} if detected else {}\n",
    "        else:\n",
    "            return False, {'error': f'Unknown native strategy: {strategy_name}'}\n",
    "        \n",
    "        return detected, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, {'error': str(e)}\n",
    "\n",
    "def test_composed_strategy(strategy_name, df, check_bar):\n",
    "    \"\"\"Test a composed strategy\"\"\"\n",
    "    try:\n",
    "        if strategy_name == 'hbs_breakout':\n",
    "            # HBS = (Consolidation Breakout OR Channel Breakout) + Confluence\n",
    "            cb_detected, cb_result = detect_consolidation_breakout(df, check_bar=check_bar)\n",
    "            chb_detected, chb_result = detect_channel_breakout(df, check_bar=check_bar)\n",
    "            conf_detected, conf_result = detect_confluence(df, check_bar=check_bar)\n",
    "            \n",
    "            breakout_detected = cb_detected or chb_detected\n",
    "            hbs_detected = breakout_detected and conf_detected\n",
    "            \n",
    "            if hbs_detected:\n",
    "                result = {\n",
    "                    'hbs_type': 'consolidation_breakout' if cb_detected else 'channel_breakout',\n",
    "                    'has_confluence': True,\n",
    "                    'consolidation_detected': cb_detected,\n",
    "                    'channel_breakout_detected': chb_detected\n",
    "                }\n",
    "            else:\n",
    "                result = {\n",
    "                    'consolidation_detected': cb_detected,\n",
    "                    'channel_breakout_detected': chb_detected,\n",
    "                    'confluence_detected': conf_detected,\n",
    "                    'reason': 'missing_component'\n",
    "                }\n",
    "            \n",
    "            return hbs_detected, result\n",
    "            \n",
    "        elif strategy_name == 'vs_wakeup':\n",
    "            \n",
    "            # VS Wakeup = Consolidation (ongoing) + Confluence\n",
    "            cons_detected, cons_result = detect_consolidation(df, check_bar=check_bar)\n",
    "            conf_detected, conf_result = detect_confluence(df, check_bar=check_bar, only_wakeup=True)\n",
    "            \n",
    "            # Must be consolidation without breakout\n",
    "            is_consolidation = cons_detected and not cons_result.get('breakout', False)\n",
    "            vs_detected = is_consolidation and conf_detected\n",
    "            \n",
    "            result = {\n",
    "                'consolidation_detected': cons_detected,\n",
    "                'consolidation_breakout': cons_result.get('breakout', False),\n",
    "                'confluence_detected': conf_detected,\n",
    "                'vs_wakeup_valid': vs_detected\n",
    "            }\n",
    "            \n",
    "            return vs_detected, result\n",
    "            \n",
    "        else:\n",
    "            return False, {'error': f'Unknown composed strategy: {strategy_name}'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return False, {'error': str(e)}\n",
    "\n",
    "def test_futures_strategy(strategy_name, df, check_bar):\n",
    "    \"\"\"Test a futures-only strategy\"\"\"\n",
    "    try:\n",
    "        if strategy_name == 'reversal_bar':\n",
    "            # VSA reversal bar\n",
    "            params = get_reversal_bar_params()\n",
    "            condition, vsa_result = vsa_detector(df, params)\n",
    "            detected = condition.iloc[check_bar] if len(condition) > abs(check_bar) else False\n",
    "            result = {'vsa_type': 'reversal_bar'} if detected else {}\n",
    "            \n",
    "        elif strategy_name == 'pin_down':\n",
    "            # Pin down pattern\n",
    "            from custom_strategies import detect_pin_down\n",
    "            detected, result = detect_pin_down(df, check_bar=check_bar)\n",
    "            \n",
    "        else:\n",
    "            return False, {'error': f'Unknown futures strategy: {strategy_name}'}\n",
    "        \n",
    "        return detected, result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, {'error': str(e)}\n",
    "\n",
    "def test_single_strategy(strategy_name, df, check_bar):\n",
    "    \"\"\"Test any strategy by name\"\"\"\n",
    "    if strategy_name in NATIVE_STRATEGIES:\n",
    "        return test_native_strategy(strategy_name, df, check_bar)\n",
    "    elif strategy_name in COMPOSED_STRATEGIES:\n",
    "        return test_composed_strategy(strategy_name, df, check_bar)\n",
    "    elif strategy_name in FUTURES_ONLY_STRATEGIES:\n",
    "        return test_futures_strategy(strategy_name, df, check_bar)\n",
    "    else:\n",
    "        return False, {'error': f'Unknown strategy: {strategy_name}'}\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Main Scanner Function\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "async def scan_all_strategies(symbol, exchange, timeframe):\n",
    "    \"\"\"\n",
    "    Scan all strategies for a given symbol/exchange/timeframe\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results for current and last closed bars\n",
    "    \"\"\"\n",
    "    # Fetch data\n",
    "    df = await fetch_market_data(symbol, exchange, timeframe)\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Check if futures strategies should be included\n",
    "    is_futures = 'futures' in exchange.lower()\n",
    "    strategies_to_test = ALL_STRATEGIES if is_futures else NATIVE_STRATEGIES + COMPOSED_STRATEGIES\n",
    "    \n",
    "    results = {\n",
    "        'symbol': symbol,\n",
    "        'exchange': exchange,\n",
    "        'timeframe': timeframe,\n",
    "        'scan_time': datetime.now(),\n",
    "        'latest_price': df['close'].iloc[-1],\n",
    "        'current_bar': {},\n",
    "        'last_closed_bar': {},\n",
    "        'summary': {}\n",
    "    }\n",
    "    \n",
    "    # Test current bar (-1)\n",
    "    current_signals = 0\n",
    "    for strategy in strategies_to_test:\n",
    "        detected, result = test_single_strategy(strategy, df, check_bar=-1)\n",
    "        results['current_bar'][strategy] = {\n",
    "            'detected': detected,\n",
    "            'result': result\n",
    "        }\n",
    "        if detected:\n",
    "            current_signals += 1\n",
    "    \n",
    "    # Test last closed bar (-2)\n",
    "    closed_signals = 0\n",
    "    for strategy in strategies_to_test:\n",
    "        detected, result = test_single_strategy(strategy, df, check_bar=-2)\n",
    "        results['last_closed_bar'][strategy] = {\n",
    "            'detected': detected,\n",
    "            'result': result\n",
    "        }\n",
    "        if detected:\n",
    "            closed_signals += 1\n",
    "    \n",
    "    # Summary\n",
    "    results['summary'] = {\n",
    "        'total_strategies_tested': len(strategies_to_test),\n",
    "        'current_bar_signals': current_signals,\n",
    "        'last_closed_bar_signals': closed_signals,\n",
    "        'any_signals': current_signals > 0 or closed_signals > 0\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Results Display Functions\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def display_results_summary(results):\n",
    "    \"\"\"Display formatted results summary\"\"\"\n",
    "    if results is None:\n",
    "        print(\"No results to display\")\n",
    "        return\n",
    "    \n",
    "    # Calculate separate counts for native and composed strategies\n",
    "    native_current = sum(1 for s in NATIVE_STRATEGIES if results['current_bar'][s]['detected'])\n",
    "    native_closed = sum(1 for s in NATIVE_STRATEGIES if results['last_closed_bar'][s]['detected'])\n",
    "    composed_current = sum(1 for s in COMPOSED_STRATEGIES if results['current_bar'][s]['detected'])\n",
    "    composed_closed = sum(1 for s in COMPOSED_STRATEGIES if results['last_closed_bar'][s]['detected'])\n",
    "    \n",
    "    print(f\"{results['symbol']} | {results['timeframe']} | {results['exchange']} | Latest Price: ${results['latest_price']:,.2f}\")\n",
    "    print()\n",
    "    print(f\"{'Strategy':<22} {'Current Bar (' + str(native_current) + ')':<18} {'Last Closed (' + str(native_closed) + ')':<18} {'Category':<12}\")\n",
    "    print(\"-\" * 72)\n",
    "    \n",
    "    # Native strategies first\n",
    "    for strategy in NATIVE_STRATEGIES:\n",
    "        if strategy in results['current_bar']:\n",
    "            current = \"✅\" if results['current_bar'][strategy]['detected'] else \"❌\"\n",
    "            closed = \"✅\" if results['last_closed_bar'][strategy]['detected'] else \"❌\"\n",
    "            print(f\"{strategy:<22} {current:<18} {closed:<18} {'Native':<12}\")\n",
    "    \n",
    "    # Add separator line and header for composed strategies\n",
    "    print(\"-\" * 72)\n",
    "    print(f\"{'Strategy':<22} {'Current Bar (' + str(composed_current) + ')':<18} {'Last Closed (' + str(composed_closed) + ')':<18} {'Category':<12}\")\n",
    "    print(\"-\" * 72)\n",
    "    \n",
    "    # Composed strategies\n",
    "    for strategy in COMPOSED_STRATEGIES:\n",
    "        if strategy in results['current_bar']:\n",
    "            current = \"✅\" if results['current_bar'][strategy]['detected'] else \"❌\"\n",
    "            closed = \"✅\" if results['last_closed_bar'][strategy]['detected'] else \"❌\"\n",
    "            print(f\"{strategy:<22} {current:<18} {closed:<18} {'Composed':<12}\")\n",
    "\n",
    "def display_detailed_results(results):\n",
    "    \"\"\"This function is now merged with display_results_summary\"\"\"\n",
    "    pass\n",
    "\n",
    "def display_signal_details(results):\n",
    "    \"\"\"Display details for detected signals - REMOVED\"\"\"\n",
    "    pass\n",
    "\n",
    "# Main Execution\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Run the scan\n",
    "        results = await scan_all_strategies(SYMBOL, EXCHANGE, TIMEFRAME)\n",
    "        \n",
    "        if results:\n",
    "            # Display results - single clean table\n",
    "            display_results_summary(results)\n",
    "            \n",
    "            # Save results to variable for further analysis\n",
    "            global scan_results\n",
    "            scan_results = results\n",
    "            \n",
    "        else:\n",
    "            print(\"Scan failed - no results generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Scan error: {e}\")\n",
    "        logging.error(f\"Main execution error: {e}\")\n",
    "\n",
    "# Quick Configuration Functions\n",
    "async def quick_scan(symbol, exchange=\"binance_spot\", timeframe=\"1d\"):\n",
    "    \"\"\"\n",
    "    Quick scan function for easy testing\n",
    "    \n",
    "    Usage:\n",
    "        await quick_scan(\"ETHUSDT\", \"binance_spot\", \"4h\")\n",
    "    \"\"\"\n",
    "    global SYMBOL, EXCHANGE, TIMEFRAME\n",
    "    SYMBOL = symbol\n",
    "    EXCHANGE = exchange  \n",
    "    TIMEFRAME = timeframe\n",
    "    \n",
    "    print(f\"Quick scan: {symbol} on {exchange} ({timeframe})\")\n",
    "    await main()\n",
    "\n",
    "def list_available_exchanges():\n",
    "    \"\"\"List all available exchanges\"\"\"\n",
    "    print(\"Available exchanges:\")\n",
    "    for exchange in sorted(EXCHANGE_CLIENTS.keys()):\n",
    "        print(f\"  • {exchange}\")\n",
    "\n",
    "def list_all_strategies():\n",
    "    \"\"\"List all available strategies by category\"\"\"\n",
    "    print(\"Available strategies:\")\n",
    "    print(f\"\\nNative Strategies ({len(NATIVE_STRATEGIES)}):\")\n",
    "    for strategy in NATIVE_STRATEGIES:\n",
    "        print(f\"  • {strategy}\")\n",
    "    \n",
    "    print(f\"\\nComposed Strategies ({len(COMPOSED_STRATEGIES)}):\")\n",
    "    for strategy in COMPOSED_STRATEGIES:\n",
    "        print(f\"  • {strategy}\")\n",
    "    \n",
    "    print(f\"\\nFutures-Only Strategies ({len(FUTURES_ONLY_STRATEGIES)}):\")\n",
    "    for strategy in FUTURES_ONLY_STRATEGIES:\n",
    "        print(f\"  • {strategy}\")\n",
    "\n",
    "# Run the main scan\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b606c-a450-4c10-ad34-b30ca10e03e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
