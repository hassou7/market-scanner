{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab56392-9bd5-48da-aef6-5dddab880329",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f493cf4-15f9-4d57-b16c-cb3ee2b33977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Simple Parallel Scan for Test Bar\n",
    "\n",
    "This script runs the test bar scan across multiple exchanges in parallel\n",
    "using a simplified parallel scanning approach that avoids console output issues.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Add project directory to path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(os.getcwd())\n",
    "print(f\"✓ Added {os.getcwd()} to sys.path\")\n",
    "\n",
    "# Import the simple parallel scanner\n",
    "from run_parallel_scanner import run_parallel_exchanges, run_parallel_multi_timeframes_all_exchanges\n",
    "from scanner.main import kline_cache\n",
    "\n",
    "# Define exchanges\n",
    "futures_exchanges = [\"binance_futures\", \"bybit_futures\", \"mexc_futures\", \"gateio_futures\"]\n",
    "spot_exchanges = [\"binance_spot\", \"bybit_spot\", \"kucoin_spot\", \"mexc_spot\", \"gateio_spot\"]\n",
    "spot_exchanges_1w = [\"binance_spot\", \"bybit_spot\", \"gateio_spot\"]\n",
    "\n",
    "async def main():\n",
    "    # Clear cache for fresh data\n",
    "    kline_cache.clear()\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Run parallel scan for test bar strategy on spot exchanges\n",
    "    result = await run_parallel_exchanges(\n",
    "        timeframe=\"4h\",                    # Example timeframe\n",
    "        strategies=[\"confluence\", \"test_bar\", \"consolidation\"],\n",
    "        # strategies=[\"reversal_bar\"],       \n",
    "        exchanges=spot_exchanges,          # Spot exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for Telegram notifications\n",
    "        send_telegram=True,                # Enable Telegram notifications\n",
    "        min_volume_usd=None                # Use default volume threshold\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Run multi-timeframe parallel scan\n",
    "    result = await run_parallel_multi_timeframes_all_exchanges(\n",
    "        timeframes=[\"1d\", \"2d\", \"3d\", \"4d\", \"1w\"],     # Multiple timeframes\n",
    "        strategies=[\"hbs_breakout\"],        # Strategies to scan\n",
    "        exchanges=[\"binance_spot\", \"binance_futures\", \"bybit_spot\", \"kucoin_spot\", \"mexc_spot\", \"gateio_spot\"],          # Exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for notifications\n",
    "        send_telegram=True,                # Enable notifications\n",
    "        min_volume_usd=None                # Use default volume threshold\n",
    "    )\n",
    "     #\"\"\"\n",
    "    \n",
    "    print(\"Scan completed!\")\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6740f2-da50-4ba9-9c41-0bcc9be58173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added /home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project/Project to sys.path\n",
      "✓ Added /home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 8 threads.\n",
      "\n",
      "================================================================================\n",
      "  RUNNING PARALLEL MULTI-TIMEFRAME SCAN ON ALL EXCHANGES\n",
      "================================================================================\n",
      "\n",
      "• Exchanges: binance_spot\n",
      "• Timeframes: 1d\n",
      "• Strategies: hbs_breakout\n",
      "• Notifications: Enabled\n",
      "• Recipients: default\n",
      "• Start time: 20:10:31\n",
      "\n",
      "Fetching market data...\n",
      "\n",
      "Processing 1d timeframe\n",
      "[20:10:31] Starting scan on binance_spot for 1d timeframe...\n",
      "Found 410 markets on Binance Spot for 1d timeframe\n",
      "Processing 410 symbols for Binance Spot...\n",
      "hbs_breakout detected for KERNELUSDT (current bar)\n",
      "hbs_breakout detected for KERNELUSDT\n",
      "hbs_breakout detected for MEMEUSDT (current bar)\n",
      "hbs_breakout detected for MEMEUSDT\n",
      "hbs_breakout detected for TUTUSDT (current bar)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:129\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 129\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:360\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:205\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    203\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/work/Crypto/sevenfigures-bot/hbs_2025/Project/scanner/main.py:564\u001b[0m, in \u001b[0;36mUnifiedScanner.scan_market\u001b[0;34m(self, symbol)\u001b[0m\n\u001b[1;32m    562\u001b[0m cb_detected_prev, cb_result_prev \u001b[38;5;241m=\u001b[39m detect_consolidation_breakout(df, check_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    563\u001b[0m cb_detected_curr, cb_result_curr \u001b[38;5;241m=\u001b[39m detect_consolidation_breakout(df, check_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 564\u001b[0m cf_detected_prev, cf_result_prev \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_confluence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m cf_detected_curr, cf_result_curr \u001b[38;5;241m=\u001b[39m detect_confluence(df, check_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    566\u001b[0m chb_detected_prev, chb_result_prev \u001b[38;5;241m=\u001b[39m detect_channel_breakout(df, check_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/work/Crypto/sevenfigures-bot/hbs_2025/Project/custom_strategies/confluence.py:230\u001b[0m, in \u001b[0;36mdetect_confluence\u001b[0;34m(df, doji_threshold, ctx_len, range_floor, len_fast, len_mid, len_slow, check_bar)\u001b[0m\n\u001b[1;32m    228\u001b[0m     range_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, idx \u001b[38;5;241m-\u001b[39m ctx_len \u001b[38;5;241m+\u001b[39m highest_range_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    229\u001b[0m     ctx_hi \u001b[38;5;241m=\u001b[39m curr_high\u001b[38;5;241m.\u001b[39miloc[range_start:idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m--> 230\u001b[0m     ctx_lo \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_low\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrange_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m    232\u001b[0m ctxHi\u001b[38;5;241m.\u001b[39miloc[idx] \u001b[38;5;241m=\u001b[39m ctx_hi\n\u001b[1;32m    233\u001b[0m ctxLo\u001b[38;5;241m.\u001b[39miloc[idx] \u001b[38;5;241m=\u001b[39m ctx_lo\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1691\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m   1686\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using .loc for automatic alignment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1688\u001b[0m     )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m-> 1691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   1694\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexing.py:1727\u001b[0m, in \u001b[0;36m_iLocIndexer._get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1725\u001b[0m labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1726\u001b[0m labels\u001b[38;5;241m.\u001b[39m_validate_positional_slice(slice_obj)\n\u001b[0;32m-> 1727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/series.py:1012\u001b[0m, in \u001b[0;36mSeries._slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slice\u001b[39m(\u001b[38;5;28mself\u001b[39m, slobj: \u001b[38;5;28mslice\u001b[39m, axis: AxisInt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# axis kwarg is retained for compat with NDFrame method\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m#  _slice is *always* positional\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(mgr, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:1942\u001b[0m, in \u001b[0;36mSingleBlockManager.get_slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# TODO this method is only used in groupby SeriesSplitter at the moment,\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;66;03m# so passing refs is not yet covered by the tests\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(blk)(array, placement\u001b[38;5;241m=\u001b[39mbp, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, refs\u001b[38;5;241m=\u001b[39mblk\u001b[38;5;241m.\u001b[39mrefs)\n\u001b[0;32m-> 1942\u001b[0m new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(block, new_index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:5395\u001b[0m, in \u001b[0;36mIndex._getitem_slice\u001b[0;34m(self, slobj)\u001b[0m\n\u001b[1;32m   5391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_slice\u001b[39m(\u001b[38;5;28mself\u001b[39m, slobj: \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   5392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5393\u001b[0m \u001b[38;5;124;03m    Fastpath for __getitem__ when we know we have a slice.\u001b[39;00m\n\u001b[1;32m   5394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5395\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslobj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   5396\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_simple_new(res, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, refs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_references)\n\u001b[1;32m   5397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_engine\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:382\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# At this point we know the result is an array.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     result \u001b[38;5;241m=\u001b[39m cast(Self, result)\n\u001b[0;32m--> 382\u001b[0m result\u001b[38;5;241m.\u001b[39m_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_getitem_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:398\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._get_getitem_freq\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    396\u001b[0m freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m         freq \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/arrays/datetimelike.py:1991\u001b[0m, in \u001b[0;36mTimelikeOps.freq\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_dtype\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values, dtype):\n\u001b[1;32m   1989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AbstractMethodError(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m-> 1991\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreq\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;124;03m    Return the frequency object if it is set, otherwise None.\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_freq\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run Simple Parallel Scan for Test Bar\n",
    "\n",
    "This script runs the test bar scan across multiple exchanges in parallel\n",
    "using a simplified parallel scanning approach that avoids console output issues.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Add project directory to path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(os.getcwd())\n",
    "print(f\"✓ Added {os.getcwd()} to sys.path\")\n",
    "\n",
    "# Import the simple parallel scanner\n",
    "from run_parallel_scanner import run_parallel_exchanges, run_parallel_multi_timeframes_all_exchanges\n",
    "from scanner.main import kline_cache\n",
    "\n",
    "# Define exchanges\n",
    "futures_exchanges = [\"binance_futures\", \"bybit_futures\", \"mexc_futures\", \"gateio_futures\"]\n",
    "spot_exchanges = [\"binance_spot\", \"bybit_spot\", \"kucoin_spot\", \"mexc_spot\", \"gateio_spot\"]\n",
    "spot_exchanges_1w = [\"binance_spot\", \"bybit_spot\", \"gateio_spot\"]\n",
    "\n",
    "async def main():\n",
    "    # Clear cache for fresh data\n",
    "    kline_cache.clear()\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Run parallel scan for test bar strategy on spot exchanges\n",
    "    result = await run_parallel_exchanges(\n",
    "        timeframe=\"4h\",                    # Example timeframe\n",
    "        strategies=[\"confluence\", \"test_bar\", \"consolidation\"],\n",
    "        # strategies=[\"reversal_bar\"],       \n",
    "        exchanges=spot_exchanges,          # Spot exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for Telegram notifications\n",
    "        send_telegram=True,                # Enable Telegram notifications\n",
    "        min_volume_usd=None                # Use default volume threshold\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Run multi-timeframe parallel scan\n",
    "    result = await run_parallel_multi_timeframes_all_exchanges(\n",
    "        timeframes=[\"1d\"],     # Multiple timeframes\n",
    "        strategies=[\"hbs_breakout\"],        # Strategies to scan\n",
    "        exchanges=[\"binance_spot\"],          # Exchanges to scan\n",
    "        users=[\"default\"],                 # Recipients for notifications\n",
    "        send_telegram=True,                # Enable notifications\n",
    "        min_volume_usd=None                # Use default volume threshold\n",
    "    )\n",
    "     #\"\"\"\n",
    "    \n",
    "    print(\"Scan completed!\")\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c86d73-1174-45d1-b215-842f2609b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly scan on SF Mexc and Kucoin: Confluence, consolidation breakout, channel breakout, and HBS\n",
    "\"\"\"\n",
    "Seven Figures Weekly Strategy Scanner for KuCoin & MEXC\n",
    "\n",
    "This notebook runs weekly strategy detection using SF server data for KuCoin and MEXC exchanges.\n",
    "You can choose between: hbs_breakout, consolidation_breakout, channel_breakout, and confluence strategies.\n",
    "Both exchanges are scanned simultaneously with parallel processing.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm.asyncio import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "# Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add project directory to path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "\n",
    "# Import strategies and SF service\n",
    "from exchanges.sf_pairs_service import SFPairsService\n",
    "from custom_strategies import detect_consolidation_breakout, detect_confluence, detect_consolidation, detect_channel_breakout\n",
    "\n",
    "class SFWeeklyStrategyScanner:\n",
    "    \"\"\"\n",
    "    Weekly strategy scanner using Seven Figures server data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sf_service = SFPairsService()\n",
    "        self.strategy_functions = {\n",
    "            'consolidation_breakout': self.detect_consolidation_breakout,\n",
    "            'channel_breakout': self.detect_channel_breakout,  # NEW\n",
    "            'confluence': self.detect_confluence,\n",
    "            'consolidation': self.detect_consolidation,\n",
    "            'hbs_breakout': self.detect_hbs_breakout\n",
    "        }\n",
    "        \n",
    "    def prepare_sf_data(self, raw_df):\n",
    "        \"\"\"Convert SF data to strategy-compatible format\"\"\"\n",
    "        if raw_df is None or len(raw_df) == 0:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(raw_df)\n",
    "        \n",
    "        # Convert datetime column to pandas datetime and set as index\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df = df.set_index('datetime')\n",
    "        elif 'time' in df.columns:\n",
    "            df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "            df = df.set_index('time')\n",
    "        \n",
    "        # Select only OHLCV columns needed\n",
    "        required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        available_cols = [col for col in required_cols if col in df.columns]\n",
    "        \n",
    "        if len(available_cols) != 5:\n",
    "            return None\n",
    "        \n",
    "        # Select and clean data\n",
    "        result_df = df[required_cols].copy()\n",
    "        \n",
    "        # Ensure numeric types\n",
    "        for col in required_cols:\n",
    "            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "        \n",
    "        # Drop any NaN rows\n",
    "        result_df = result_df.dropna()\n",
    "        \n",
    "        # Sort by index (oldest first)\n",
    "        result_df = result_df.sort_index()\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def detect_consolidation_breakout(self, df, symbol, exchange):\n",
    "        \"\"\"Detect consolidation breakout signals\"\"\"\n",
    "        signals = []\n",
    "        \n",
    "        # Check last closed bar\n",
    "        if len(df) > 1:\n",
    "            detected, result = detect_consolidation_breakout(df, check_bar=-2)\n",
    "            if detected:\n",
    "                # Calculate volume info\n",
    "                volume_usd = df['volume'].iloc[-2] * df['close'].iloc[-2]\n",
    "                volume_mean = df['volume'].rolling(7).mean().iloc[-2]\n",
    "                volume_ratio = df['volume'].iloc[-2] / volume_mean if volume_mean > 0 else 0\n",
    "                \n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'consolidation_breakout',\n",
    "                    'direction': result.get('direction'),\n",
    "                    'timestamp': result.get('timestamp', df.index[-2]),\n",
    "                    'close': df['close'].iloc[-2],\n",
    "                    'current_bar': False,\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'volume_ratio': volume_ratio,\n",
    "                    'box_hi': result.get('box_hi'),\n",
    "                    'box_lo': result.get('box_lo'),\n",
    "                    'box_age': result.get('box_age'),\n",
    "                    'bars_inside': result.get('bars_inside'),\n",
    "                    'height_pct': result.get('height_pct'),\n",
    "                    'max_height_pct_req': result.get('max_height_pct_req'),\n",
    "                })\n",
    "        \n",
    "        # Check current bar\n",
    "        if len(df) > 2:\n",
    "            detected, result = detect_consolidation_breakout(df, check_bar=-1)\n",
    "            if detected:\n",
    "                # Calculate volume info\n",
    "                volume_usd = df['volume'].iloc[-1] * df['close'].iloc[-1]\n",
    "                volume_mean = df['volume'].rolling(7).mean().iloc[-1]\n",
    "                volume_ratio = df['volume'].iloc[-1] / volume_mean if volume_mean > 0 else 0\n",
    "                \n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'consolidation_breakout',\n",
    "                    'direction': result.get('direction'),\n",
    "                    'timestamp': result.get('timestamp', df.index[-1]),\n",
    "                    'close': df['close'].iloc[-1],\n",
    "                    'current_bar': True,\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'volume_ratio': volume_ratio,\n",
    "                    'box_hi': result.get('box_hi'),\n",
    "                    'box_lo': result.get('box_lo'),\n",
    "                    'box_age': result.get('box_age'),\n",
    "                    'bars_inside': result.get('bars_inside'),\n",
    "                    'height_pct': result.get('height_pct'),\n",
    "                    'max_height_pct_req': result.get('max_height_pct_req'),\n",
    "                })\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    def detect_channel_breakout(self, df, symbol, exchange):\n",
    "        \"\"\"Detect channel breakout signals (NEW)\"\"\"\n",
    "        signals = []\n",
    "        \n",
    "        # Check if we have enough data (minimum 23 bars required)\n",
    "        if len(df) < 24:\n",
    "            return signals\n",
    "        \n",
    "        # Check last closed bar\n",
    "        if len(df) > 23:\n",
    "            detected, result = detect_channel_breakout(df, check_bar=-2)\n",
    "            if detected:\n",
    "                # Calculate volume info\n",
    "                volume_usd = df['volume'].iloc[-2] * df['close'].iloc[-2]\n",
    "                volume_mean = df['volume'].rolling(7).mean().iloc[-2]\n",
    "                volume_ratio = df['volume'].iloc[-2] / volume_mean if volume_mean > 0 else 0\n",
    "                \n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'channel_breakout',\n",
    "                    'direction': result.get('direction'),\n",
    "                    'timestamp': result.get('timestamp', df.index[-2]),\n",
    "                    'close': df['close'].iloc[-2],\n",
    "                    'current_bar': False,\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'volume_ratio': volume_ratio,\n",
    "                    # Channel-specific information\n",
    "                    'channel_age': result.get('channel_age'),\n",
    "                    'channel_direction': result.get('channel_direction'),\n",
    "                    'channel_slope': result.get('channel_slope'),\n",
    "                    'channel_offset': result.get('channel_offset'),\n",
    "                    'bars_inside': result.get('bars_inside'),\n",
    "                    'min_bars_inside_req': result.get('min_bars_inside_req'),\n",
    "                    'height_pct': result.get('height_pct'),\n",
    "                    'max_height_pct_req': result.get('max_height_pct_req'),\n",
    "                    'atr_ok': result.get('atr_ok'),\n",
    "                    'window_size': result.get('window_size'),\n",
    "                })\n",
    "        \n",
    "        # Check current bar\n",
    "        if len(df) > 24:\n",
    "            detected, result = detect_channel_breakout(df, check_bar=-1)\n",
    "            if detected:\n",
    "                # Calculate volume info\n",
    "                volume_usd = df['volume'].iloc[-1] * df['close'].iloc[-1]\n",
    "                volume_mean = df['volume'].rolling(7).mean().iloc[-1]\n",
    "                volume_ratio = df['volume'].iloc[-1] / volume_mean if volume_mean > 0 else 0\n",
    "                \n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'channel_breakout',\n",
    "                    'direction': result.get('direction'),\n",
    "                    'timestamp': result.get('timestamp', df.index[-1]),\n",
    "                    'close': df['close'].iloc[-1],\n",
    "                    'current_bar': True,\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'volume_ratio': volume_ratio,\n",
    "                    # Channel-specific information\n",
    "                    'channel_age': result.get('channel_age'),\n",
    "                    'channel_direction': result.get('channel_direction'),\n",
    "                    'channel_slope': result.get('channel_slope'),\n",
    "                    'channel_offset': result.get('channel_offset'),\n",
    "                    'bars_inside': result.get('bars_inside'),\n",
    "                    'min_bars_inside_req': result.get('min_bars_inside_req'),\n",
    "                    'height_pct': result.get('height_pct'),\n",
    "                    'max_height_pct_req': result.get('max_height_pct_req'),\n",
    "                    'atr_ok': result.get('atr_ok'),\n",
    "                    'window_size': result.get('window_size'),\n",
    "                })\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    def detect_confluence(self, df, symbol, exchange):\n",
    "        \"\"\"Detect confluence signals\"\"\"\n",
    "        signals = []\n",
    "        \n",
    "        # Check last closed bar\n",
    "        if len(df) > 1:\n",
    "            detected, result = detect_confluence(df, check_bar=-2)\n",
    "            if detected:\n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'confluence',\n",
    "                    'timestamp': result['timestamp'],\n",
    "                    'close': result['close_price'],\n",
    "                    'current_bar': False,\n",
    "                    'volume': result['volume'],\n",
    "                    'volume_usd': result['volume_usd'],\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                })\n",
    "        \n",
    "        # Check current bar\n",
    "        if len(df) > 2:\n",
    "            detected, result = detect_confluence(df, check_bar=-1)\n",
    "            if detected:\n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'confluence',\n",
    "                    'timestamp': result['timestamp'],\n",
    "                    'close': result['close_price'],\n",
    "                    'current_bar': True,\n",
    "                    'volume': result['volume'],\n",
    "                    'volume_usd': result['volume_usd'],\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                })\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    def detect_consolidation(self, df, symbol, exchange):\n",
    "        \"\"\"Detect consolidation patterns (no breakout)\"\"\"\n",
    "        signals = []\n",
    "        \n",
    "        # Check last closed bar\n",
    "        if len(df) > 1:\n",
    "            detected, result = detect_consolidation(df, check_bar=-2)\n",
    "            if detected:\n",
    "                # Calculate volume info\n",
    "                volume_usd = df['volume'].iloc[-2] * df['close'].iloc[-2]\n",
    "                volume_mean = df['volume'].rolling(7).mean().iloc[-2]\n",
    "                volume_ratio = df['volume'].iloc[-2] / volume_mean if volume_mean > 0 else 0\n",
    "                \n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'consolidation',\n",
    "                    'timestamp': result.get('timestamp', df.index[-2]),\n",
    "                    'close': df['close'].iloc[-2],\n",
    "                    'current_bar': False,\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'volume_ratio': volume_ratio,\n",
    "                    'box_hi': result.get('box_hi'),\n",
    "                    'box_lo': result.get('box_lo'),\n",
    "                    'box_age': result.get('box_age'),\n",
    "                    'bars_inside': result.get('bars_inside'),\n",
    "                    'min_bars_inside_req': result.get('min_bars_inside_req'),\n",
    "                    'height_pct': result.get('height_pct'),\n",
    "                })\n",
    "        \n",
    "        # Check current bar\n",
    "        if len(df) > 2:\n",
    "            detected, result = detect_consolidation(df, check_bar=-1)\n",
    "            if detected:\n",
    "                # Calculate volume info\n",
    "                volume_usd = df['volume'].iloc[-1] * df['close'].iloc[-1]\n",
    "                volume_mean = df['volume'].rolling(7).mean().iloc[-1]\n",
    "                volume_ratio = df['volume'].iloc[-1] / volume_mean if volume_mean > 0 else 0\n",
    "                \n",
    "                signals.append({\n",
    "                    'symbol': symbol,\n",
    "                    'exchange': exchange,\n",
    "                    'strategy': 'consolidation',\n",
    "                    'timestamp': result.get('timestamp', df.index[-1]),\n",
    "                    'close': df['close'].iloc[-1],\n",
    "                    'current_bar': True,\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'volume_ratio': volume_ratio,\n",
    "                    'box_hi': result.get('box_hi'),\n",
    "                    'box_lo': result.get('box_lo'),\n",
    "                    'box_age': result.get('box_age'),\n",
    "                    'bars_inside': result.get('bars_inside'),\n",
    "                    'min_bars_inside_req': result.get('min_bars_inside_req'),\n",
    "                    'height_pct': result.get('height_pct'),\n",
    "                })\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    def detect_hbs_breakout(self, df, symbol, exchange):\n",
    "        \"\"\"Detect HBS breakout signals - Updated to match main.py logic\"\"\"\n",
    "        signals = []\n",
    "        \n",
    "        # Check consolidation breakout\n",
    "        cb_detected_prev, cb_result_prev = detect_consolidation_breakout(df, check_bar=-2)\n",
    "        cb_detected_curr, cb_result_curr = detect_consolidation_breakout(df, check_bar=-1)\n",
    "        \n",
    "        # Check channel breakout (if enough data)\n",
    "        chb_detected_prev, chb_result_prev = (False, {})\n",
    "        chb_detected_curr, chb_result_curr = (False, {})\n",
    "        if len(df) > 23:\n",
    "            chb_detected_prev, chb_result_prev = detect_channel_breakout(df, check_bar=-2)\n",
    "        if len(df) > 24:\n",
    "            chb_detected_curr, chb_result_curr = detect_channel_breakout(df, check_bar=-1)\n",
    "        \n",
    "        # Check confluence\n",
    "        cf_detected_prev, cf_result_prev = detect_confluence(df, check_bar=-2)\n",
    "        cf_detected_curr, cf_result_curr = detect_confluence(df, check_bar=-1)\n",
    "        \n",
    "        # HBS logic: confluence + any breakout (consolidation OR channel)\n",
    "        # Current bar check\n",
    "        if cf_detected_curr and (cb_detected_curr or chb_detected_curr):\n",
    "            # Prefer channel breakout if both occur\n",
    "            if cb_detected_curr and chb_detected_curr:\n",
    "                breakout_result = chb_result_curr\n",
    "                breakout_type = \"channel_breakout\"\n",
    "            elif cb_detected_curr:\n",
    "                breakout_result = cb_result_curr\n",
    "                breakout_type = \"consolidation_breakout\"\n",
    "            else:\n",
    "                breakout_result = chb_result_curr\n",
    "                breakout_type = \"channel_breakout\"\n",
    "            \n",
    "            # Calculate volume info\n",
    "            volume_usd = df['volume'].iloc[-1] * df['close'].iloc[-1]\n",
    "            volume_mean = df['volume'].rolling(7).mean().iloc[-1]\n",
    "            volume_ratio = df['volume'].iloc[-1] / volume_mean if volume_mean > 0 else 0\n",
    "            \n",
    "            signals.append({\n",
    "                'symbol': symbol,\n",
    "                'exchange': exchange,\n",
    "                'strategy': 'hbs_breakout',\n",
    "                'breakout_type': breakout_type,\n",
    "                'direction': breakout_result.get('direction'),\n",
    "                'timestamp': breakout_result.get('timestamp', df.index[-1]),\n",
    "                'close': df['close'].iloc[-1],\n",
    "                'current_bar': True,\n",
    "                'volume_usd': volume_usd,\n",
    "                'volume_ratio': volume_ratio,\n",
    "                'bars_inside': breakout_result.get('bars_inside'),\n",
    "                'min_bars_inside_req': breakout_result.get('min_bars_inside_req'),\n",
    "                'height_pct': breakout_result.get('height_pct'),\n",
    "                'max_height_pct_req': breakout_result.get('max_height_pct_req'),\n",
    "                # HBS flags\n",
    "                'consolidation_fired': cb_detected_curr,\n",
    "                'channel_fired': chb_detected_curr,\n",
    "                'confluence_fired': cf_detected_curr,\n",
    "                # Add channel-specific data if channel breakout\n",
    "                'channel_age': breakout_result.get('channel_age') if breakout_type == \"channel_breakout\" else None,\n",
    "                'channel_direction': breakout_result.get('channel_direction') if breakout_type == \"channel_breakout\" else None,\n",
    "                'channel_slope': breakout_result.get('channel_slope') if breakout_type == \"channel_breakout\" else None,\n",
    "            })\n",
    "        \n",
    "        # Previous bar check\n",
    "        elif cf_detected_prev and (cb_detected_prev or chb_detected_prev):\n",
    "            # Prefer channel breakout if both occur\n",
    "            if cb_detected_prev and chb_detected_prev:\n",
    "                breakout_result = chb_result_prev\n",
    "                breakout_type = \"channel_breakout\"\n",
    "            elif cb_detected_prev:\n",
    "                breakout_result = cb_result_prev\n",
    "                breakout_type = \"consolidation_breakout\"\n",
    "            else:\n",
    "                breakout_result = chb_result_prev\n",
    "                breakout_type = \"channel_breakout\"\n",
    "            \n",
    "            # Calculate volume info\n",
    "            volume_usd = df['volume'].iloc[-2] * df['close'].iloc[-2]\n",
    "            volume_mean = df['volume'].rolling(7).mean().iloc[-2]\n",
    "            volume_ratio = df['volume'].iloc[-2] / volume_mean if volume_mean > 0 else 0\n",
    "            \n",
    "            signals.append({\n",
    "                'symbol': symbol,\n",
    "                'exchange': exchange,\n",
    "                'strategy': 'hbs_breakout',\n",
    "                'breakout_type': breakout_type,\n",
    "                'direction': breakout_result.get('direction'),\n",
    "                'timestamp': breakout_result.get('timestamp', df.index[-2]),\n",
    "                'close': df['close'].iloc[-2],\n",
    "                'current_bar': False,\n",
    "                'volume_usd': volume_usd,\n",
    "                'volume_ratio': volume_ratio,\n",
    "                'bars_inside': breakout_result.get('bars_inside'),\n",
    "                'min_bars_inside_req': breakout_result.get('min_bars_inside_req'),\n",
    "                'height_pct': breakout_result.get('height_pct'),\n",
    "                'max_height_pct_req': breakout_result.get('max_height_pct_req'),\n",
    "                # HBS flags\n",
    "                'consolidation_fired': cb_detected_prev,\n",
    "                'channel_fired': chb_detected_prev,\n",
    "                'confluence_fired': cf_detected_prev,\n",
    "                # Add channel-specific data if channel breakout\n",
    "                'channel_age': breakout_result.get('channel_age') if breakout_type == \"channel_breakout\" else None,\n",
    "                'channel_direction': breakout_result.get('channel_direction') if breakout_type == \"channel_breakout\" else None,\n",
    "                'channel_slope': breakout_result.get('channel_slope') if breakout_type == \"channel_breakout\" else None,\n",
    "            })\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    async def scan_exchange_pairs(self, exchange, strategy, volume_threshold=300000, max_pairs=None):\n",
    "        \"\"\"Scan all pairs for an exchange with the selected strategy\"\"\"\n",
    "        print(f\"\\n🔍 Scanning {exchange} for {strategy} signals...\")\n",
    "        print(f\"📊 Volume threshold: ${volume_threshold:,}\")\n",
    "        \n",
    "        # Get all pairs from exchange\n",
    "        all_pairs = self.sf_service.get_pairs_of_exchange(exchange)\n",
    "        \n",
    "        # Filter for USDT pairs\n",
    "        usdt_pairs = [\n",
    "            pair for pair in all_pairs \n",
    "            if 'Quote' in pair and pair['Quote'].upper() == \"USDT\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"📋 Found {len(usdt_pairs)} USDT pairs on {exchange}\")\n",
    "        \n",
    "        # Use all pairs unless max_pairs is specified\n",
    "        if max_pairs is None:\n",
    "            pairs_to_scan = usdt_pairs\n",
    "            print(f\"🔄 Scanning ALL {len(pairs_to_scan)} pairs...\")\n",
    "        else:\n",
    "            pairs_to_scan = usdt_pairs[:max_pairs]\n",
    "            print(f\"🔄 Scanning {len(pairs_to_scan)} pairs (limited by max_pairs={max_pairs})...\")\n",
    "        \n",
    "        all_signals = []\n",
    "        \n",
    "        # Use tqdm for progress tracking\n",
    "        for pair in tqdm(pairs_to_scan, desc=f\"{exchange} {strategy}\"):\n",
    "            try:\n",
    "                token = pair['Token']\n",
    "                \n",
    "                # Skip stablecoins\n",
    "                if token.upper() in ['USDT', 'USDC', 'BUSD', 'DAI', 'TUSD']:\n",
    "                    continue\n",
    "                \n",
    "                # Get weekly OHLCV data (50 weeks for good analysis)\n",
    "                raw_data = self.sf_service.get_ohlcv_for_pair(\n",
    "                    token, 'USDT', exchange, '1w', 50\n",
    "                )\n",
    "                \n",
    "                if raw_data is None or len(raw_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Prepare data\n",
    "                df = self.prepare_sf_data(raw_data)\n",
    "                if df is None:\n",
    "                    continue\n",
    "                \n",
    "                # Different minimum data requirements for different strategies\n",
    "                min_data_req = 25 if strategy == 'channel_breakout' else 25\n",
    "                if len(df) < min_data_req:\n",
    "                    continue\n",
    "                \n",
    "                # Check volume threshold on recent bars (both current and last closed)\n",
    "                current_volume_usd = df['close'].iloc[-1] * df['volume'].iloc[-1]\n",
    "                last_closed_volume_usd = df['close'].iloc[-2] * df['volume'].iloc[-2]\n",
    "                \n",
    "                # Accept if either current or last closed bar meets volume threshold\n",
    "                if current_volume_usd < volume_threshold and last_closed_volume_usd < volume_threshold:\n",
    "                    continue\n",
    "                \n",
    "                # Run the selected strategy\n",
    "                strategy_func = self.strategy_functions.get(strategy)\n",
    "                if strategy_func:\n",
    "                    signals = strategy_func(df, f\"{token}USDT\", exchange)\n",
    "                    if signals:\n",
    "                        all_signals.extend(signals)\n",
    "                        for signal in signals:\n",
    "                            bar_type = \"current\" if signal.get('current_bar') else \"last closed\"\n",
    "                            direction = signal.get('direction', 'N/A')\n",
    "                            breakout_type = signal.get('breakout_type', '')\n",
    "                            if breakout_type:\n",
    "                                print(f\"🎯 {strategy} signal found: {token}USDT on {exchange} ({bar_type} bar, {direction}, {breakout_type})\")\n",
    "                            else:\n",
    "                                print(f\"🎯 {strategy} signal found: {token}USDT on {exchange} ({bar_type} bar, {direction})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Skip errors silently to avoid spam\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ {exchange} scan complete: {len(all_signals)} {strategy} signals found\")\n",
    "        return all_signals\n",
    "    \n",
    "    async def run_parallel_scan(self, strategy='consolidation_breakout', volume_threshold=300000, max_pairs=None):\n",
    "        \"\"\"Run parallel scan on both KuCoin and MEXC\"\"\"\n",
    "        \n",
    "        exchanges = ['Kucoin', 'Mexc']\n",
    "        strategy_name = strategy.replace('_', ' ').title()\n",
    "        \n",
    "        print(f\"🚀 STARTING PARALLEL SF WEEKLY SCAN\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"📊 Strategy: {strategy_name}\")\n",
    "        print(f\"🏢 Exchanges: {', '.join(exchanges)}\")\n",
    "        print(f\"📅 Timeframe: Weekly (1w)\")\n",
    "        print(f\"💰 Min Volume: ${volume_threshold:,}\")\n",
    "        if max_pairs:\n",
    "            print(f\"🎯 Max Pairs per Exchange: {max_pairs}\")\n",
    "        else:\n",
    "            print(f\"🎯 Scanning ALL pairs on each exchange\")\n",
    "        print(f\"📊 Checking: Current bar AND last closed bar\")\n",
    "        print(f\"🕐 Start Time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Create tasks for both exchanges\n",
    "        tasks = [\n",
    "            self.scan_exchange_pairs(exchange, strategy, volume_threshold, max_pairs)\n",
    "            for exchange in exchanges\n",
    "        ]\n",
    "        \n",
    "        # Run both exchanges in parallel\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Combine results\n",
    "        all_signals = []\n",
    "        for exchange_signals in results:\n",
    "            all_signals.extend(exchange_signals)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"📊 SCAN RESULTS - {strategy_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"🕐 Duration: {str(duration).split('.')[0]}\")\n",
    "        print(f\"🎯 Total Signals: {len(all_signals)}\")\n",
    "        \n",
    "        if all_signals:\n",
    "            # Convert to DataFrame for better display\n",
    "            df_results = pd.DataFrame(all_signals)\n",
    "            \n",
    "            # Group by exchange\n",
    "            for exchange in exchanges:\n",
    "                exchange_signals = [s for s in all_signals if s['exchange'] == exchange]\n",
    "                print(f\"\\n📈 {exchange}: {len(exchange_signals)} signals\")\n",
    "            \n",
    "            # Sort by volume for display\n",
    "            df_results = df_results.sort_values('volume_usd', ascending=False)\n",
    "            \n",
    "            # Display key columns based on strategy\n",
    "            if strategy == 'consolidation_breakout':\n",
    "                display_cols = ['symbol', 'exchange', 'direction', 'close', 'volume_usd', \n",
    "                               'volume_ratio', 'box_age', 'bars_inside', 'height_pct']\n",
    "            elif strategy == 'channel_breakout':\n",
    "                display_cols = ['symbol', 'exchange', 'direction', 'close', 'volume_usd', \n",
    "                               'volume_ratio', 'channel_age', 'channel_direction', 'channel_slope', 'bars_inside']\n",
    "            elif strategy == 'confluence':\n",
    "                display_cols = ['symbol', 'exchange', 'close', 'volume_usd', 'volume_ratio', \n",
    "                               'momentum_score', 'high_volume', 'spread_breakout', 'momentum_breakout']\n",
    "            elif strategy == 'hbs_breakout':\n",
    "                display_cols = ['symbol', 'exchange', 'breakout_type', 'direction', 'close', 'volume_usd', \n",
    "                               'volume_ratio', 'consolidation_fired', 'channel_fired', 'confluence_fired']\n",
    "            elif strategy == 'consolidation':\n",
    "                display_cols = ['symbol', 'exchange', 'close', 'volume_usd', 'volume_ratio', \n",
    "                               'box_age', 'bars_inside', 'height_pct']\n",
    "            \n",
    "            # Filter available columns\n",
    "            available_cols = [col for col in display_cols if col in df_results.columns]\n",
    "            \n",
    "            print(f\"\\n📋 DETAILED RESULTS:\")\n",
    "            print(\"-\" * 120)\n",
    "            print(df_results[available_cols].to_string(index=False))\n",
    "            \n",
    "            # Show some statistics\n",
    "            if 'direction' in df_results.columns:\n",
    "                direction_counts = df_results['direction'].value_counts()\n",
    "                print(f\"\\n📊 Direction Breakdown:\")\n",
    "                for direction, count in direction_counts.items():\n",
    "                    print(f\"  {direction}: {count} signals\")\n",
    "            \n",
    "            if 'breakout_type' in df_results.columns:\n",
    "                breakout_counts = df_results['breakout_type'].value_counts()\n",
    "                print(f\"\\n📊 Breakout Type Breakdown:\")\n",
    "                for breakout_type, count in breakout_counts.items():\n",
    "                    print(f\"  {breakout_type}: {count} signals\")\n",
    "            \n",
    "            print(f\"\\n💰 Volume Statistics:\")\n",
    "            print(f\"  Average Volume: ${df_results['volume_usd'].mean():,.0f}\")\n",
    "            print(f\"  Max Volume: ${df_results['volume_usd'].max():,.0f}\")\n",
    "            print(f\"  Min Volume: ${df_results['volume_usd'].min():,.0f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n❌ No {strategy_name} signals found\")\n",
    "            print(\"💡 Try:\")\n",
    "            print(\"   • Lowering volume threshold\")\n",
    "            print(\"   • Increasing max_pairs\")\n",
    "            print(\"   • Different strategy\")\n",
    "        \n",
    "        return all_signals\n",
    "\n",
    "# Create scanner instance\n",
    "scanner = SFWeeklyStrategyScanner()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "async def scan_consolidation_breakout(volume_threshold=300000, max_pairs=None):\n",
    "    \"\"\"Scan for consolidation breakout signals\"\"\"\n",
    "    return await scanner.run_parallel_scan(\n",
    "        strategy='consolidation_breakout',\n",
    "        volume_threshold=volume_threshold,\n",
    "        max_pairs=max_pairs\n",
    "    )\n",
    "\n",
    "async def scan_channel_breakout(volume_threshold=300000, max_pairs=None):\n",
    "    \"\"\"Scan for channel breakout signals (NEW)\"\"\"\n",
    "    return await scanner.run_parallel_scan(\n",
    "        strategy='channel_breakout',\n",
    "        volume_threshold=volume_threshold,\n",
    "        max_pairs=max_pairs\n",
    "    )\n",
    "\n",
    "async def scan_confluence(volume_threshold=300000, max_pairs=None):\n",
    "    \"\"\"Scan for confluence signals\"\"\"\n",
    "    return await scanner.run_parallel_scan(\n",
    "        strategy='confluence',\n",
    "        volume_threshold=volume_threshold,\n",
    "        max_pairs=max_pairs\n",
    "    )\n",
    "\n",
    "async def scan_hbs_breakout(volume_threshold=300000, max_pairs=None):\n",
    "    \"\"\"Scan for HBS (hybrid) breakout signals - Updated with channel breakout support\"\"\"\n",
    "    return await scanner.run_parallel_scan(\n",
    "        strategy='hbs_breakout',\n",
    "        volume_threshold=volume_threshold,\n",
    "        max_pairs=max_pairs\n",
    "    )\n",
    "\n",
    "async def scan_consolidation(volume_threshold=300000, max_pairs=None):\n",
    "    \"\"\"Scan for consolidation patterns\"\"\"\n",
    "    return await scanner.run_parallel_scan(\n",
    "        strategy='consolidation',\n",
    "        volume_threshold=volume_threshold,\n",
    "        max_pairs=max_pairs\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# EASY-TO-USE INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🎯 SF WEEKLY STRATEGY SCANNER FOR KUCOIN & MEXC\")\n",
    "print(\"=\"*60)\n",
    "print(\"Available strategies:\")\n",
    "print(\"1. await scan_consolidation_breakout()  # Consolidation breakout detection\")\n",
    "print(\"2. await scan_channel_breakout()        # Channel breakout detection (NEW)\")\n",
    "print(\"3. await scan_confluence()              # Multi-factor confluence signals\")  \n",
    "print(\"4. await scan_hbs_breakout()            # Hybrid breakout (consolidation/channel + confluence)\")\n",
    "print(\"5. await scan_consolidation()           # Consolidation pattern detection\")\n",
    "print()\n",
    "print(\"Optional parameters:\")\n",
    "print(\"• volume_threshold: Minimum USD volume (default: 300,000)\")\n",
    "print(\"• max_pairs: Max pairs per exchange (default: None = ALL pairs)\")\n",
    "print()\n",
    "print(\"Examples:\")\n",
    "print(\"• await scan_consolidation_breakout()                    # Scan ALL pairs\")\n",
    "print(\"• await scan_channel_breakout()                          # NEW: Channel breakouts\")\n",
    "print(\"• await scan_hbs_breakout(volume_threshold=500000)       # Higher volume threshold\")\n",
    "print(\"• await scan_confluence(max_pairs=100)                   # Limit to 100 pairs per exchange\")\n",
    "print(\"• await scan_hbs_breakout(volume_threshold=200000)       # Lower volume threshold\")\n",
    "print()\n",
    "print(\"🆕 NEW FEATURES:\")\n",
    "print(\"• Channel Breakout: Detects diagonal trending channel breakouts\")\n",
    "print(\"• Enhanced HBS: Now includes both consolidation AND channel breakouts + confluence\")\n",
    "print(\"• Breakout Type Tracking: Shows which type of breakout triggered HBS signals\")\n",
    "print(\"• Channel Metrics: Age, direction, slope analysis for channel breakouts\")\n",
    "\n",
    "# Uncomment one of these to auto-run:\n",
    "# await scan_consolidation()\n",
    "# await scan_consolidation_breakout()\n",
    "# await scan_channel_breakout()\n",
    "# await scan_confluence() \n",
    "await scan_hbs_breakout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac56a-df34-4974-8cd6-8fb33f8d477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binance BTC dominated pairs - Confluence Scanner with Direct API\n",
    "\n",
    "from telegram.ext import Application\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import os\n",
    "import html\n",
    "import aiohttp\n",
    "import time\n",
    "\n",
    "# Add project path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from custom_strategies import detect_confluence\n",
    "\n",
    "class BinanceBTCConfluenceScanner:\n",
    "    def __init__(self, telegram_token, telegram_chat_id, timeframe, offset=1):\n",
    "        self.telegram_token = telegram_token\n",
    "        self.telegram_chat_id = telegram_chat_id\n",
    "        self.telegram_app = None\n",
    "        self.exchange = \"Binance\"\n",
    "        self.timeframe = timeframe\n",
    "        self.quote_currency = \"BTC\"\n",
    "        self.offset = offset\n",
    "        self.base_url = \"https://api.binance.com\"\n",
    "        self.session = None\n",
    "        \n",
    "        # Binance timeframe mapping - All available intervals\n",
    "        self.timeframe_map = {\n",
    "            # Minutes\n",
    "            \"1m\": \"1m\",\n",
    "            \"3m\": \"3m\", \n",
    "            \"5m\": \"5m\",\n",
    "            \"15m\": \"15m\",\n",
    "            \"30m\": \"30m\",\n",
    "            # Hours\n",
    "            \"1h\": \"1h\",\n",
    "            \"2h\": \"2h\",\n",
    "            \"4h\": \"4h\",\n",
    "            \"6h\": \"6h\",\n",
    "            \"8h\": \"8h\",\n",
    "            \"12h\": \"12h\",\n",
    "            # Days\n",
    "            \"1d\": \"1d\",\n",
    "            \"2d\": \"2d\",\n",
    "            \"3d\": \"3d\",\n",
    "            # Weeks/Months\n",
    "            \"1w\": \"1w\",\n",
    "            \"1M\": \"1M\"\n",
    "        }\n",
    "        \n",
    "    async def init_session(self):\n",
    "        \"\"\"Initialize aiohttp session\"\"\"\n",
    "        if self.session is None:\n",
    "            self.session = aiohttp.ClientSession()\n",
    "            \n",
    "    async def close_session(self):\n",
    "        \"\"\"Close aiohttp session\"\"\"\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "            self.session = None\n",
    "        \n",
    "    async def init_telegram(self):\n",
    "        if self.telegram_app is None:\n",
    "            self.telegram_app = Application.builder().token(self.telegram_token).build()\n",
    "\n",
    "    async def get_btc_pairs(self):\n",
    "        \"\"\"Get all BTC trading pairs from Binance\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.base_url}/api/v3/exchangeInfo\"\n",
    "            async with self.session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    \n",
    "                    # Filter for BTC pairs that are actively trading\n",
    "                    btc_pairs = []\n",
    "                    for symbol_info in data['symbols']:\n",
    "                        if (symbol_info['quoteAsset'] == 'BTC' and \n",
    "                            symbol_info['status'] == 'TRADING' and\n",
    "                            symbol_info['isSpotTradingAllowed']):\n",
    "                            \n",
    "                            btc_pairs.append({\n",
    "                                'symbol': symbol_info['symbol'],\n",
    "                                'baseAsset': symbol_info['baseAsset'],\n",
    "                                'quoteAsset': symbol_info['quoteAsset']\n",
    "                            })\n",
    "                    \n",
    "                    return btc_pairs\n",
    "                else:\n",
    "                    logging.error(f\"Error fetching exchange info: {response.status}\")\n",
    "                    return []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching BTC pairs: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    async def get_klines(self, symbol, interval, limit=100):\n",
    "        \"\"\"Get kline/candlestick data from Binance\"\"\"\n",
    "        await self.init_session()\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.base_url}/api/v3/klines\"\n",
    "            params = {\n",
    "                'symbol': symbol,\n",
    "                'interval': interval,\n",
    "                'limit': limit\n",
    "            }\n",
    "            \n",
    "            async with self.session.get(url, params=params) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    \n",
    "                    # Convert to DataFrame\n",
    "                    df = pd.DataFrame(data, columns=[\n",
    "                        'open_time', 'open', 'high', 'low', 'close', 'volume',\n",
    "                        'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "                        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "                    ])\n",
    "                    \n",
    "                    # Convert timestamps to datetime\n",
    "                    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "                    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')\n",
    "                    \n",
    "                    # Convert OHLCV to numeric\n",
    "                    for col in ['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume']:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    \n",
    "                    # Set datetime index\n",
    "                    df.set_index('open_time', inplace=True)\n",
    "                    \n",
    "                    # Select only OHLCV columns needed for confluence\n",
    "                    # Note: Using quote_asset_volume as it's the volume in BTC\n",
    "                    result_df = df[['open', 'high', 'low', 'close', 'quote_asset_volume']].copy()\n",
    "                    result_df.rename(columns={'quote_asset_volume': 'volume'}, inplace=True)\n",
    "                    \n",
    "                    return result_df\n",
    "                    \n",
    "                elif response.status == 429:\n",
    "                    # Rate limit hit, wait a bit\n",
    "                    await asyncio.sleep(1)\n",
    "                    return None\n",
    "                else:\n",
    "                    return None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching klines for {symbol}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    async def send_telegram_alert(self, results):\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            message = f\"🚨 Confluence Detection - Binance BTC Pairs {self.timeframe}\\n\\n\"\n",
    "            \n",
    "            # Map timeframe to TradingView format - Extended mapping\n",
    "            tv_timeframe_map = {\n",
    "                # Minutes\n",
    "                \"1m\": \"1\", \"3m\": \"3\", \"5m\": \"5\", \"15m\": \"15\", \"30m\": \"30\",\n",
    "                # Hours  \n",
    "                \"1h\": \"60\", \"2h\": \"120\", \"4h\": \"240\", \"6h\": \"360\", \"8h\": \"480\", \"12h\": \"720\",\n",
    "                # Days\n",
    "                \"1d\": \"1D\", \"2d\": \"2D\", \"3d\": \"3D\",\n",
    "                # Weeks/Months\n",
    "                \"1w\": \"1W\", \"1M\": \"1M\"\n",
    "            }\n",
    "            tv_timeframe = tv_timeframe_map.get(self.timeframe.lower(), self.timeframe)\n",
    "            \n",
    "            for result in results:\n",
    "                formatted_symbol = result['symbol']\n",
    "                tv_link = f\"https://www.tradingview.com/chart/?symbol=BINANCE:{formatted_symbol}&interval={tv_timeframe}\"\n",
    "                \n",
    "                # Escape HTML entities in the URL\n",
    "                escaped_link = html.escape(tv_link)\n",
    "                \n",
    "                # Format according to BTC specifications\n",
    "                time_str = \"\"\n",
    "                if result.get('timestamp') is not None:\n",
    "                    time_str = f\"Time: {result['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "                \n",
    "                message += (\n",
    "                    f\"Symbol: {result['symbol']}\\n\"\n",
    "                    f\"{time_str}\"\n",
    "                    f\"Volume BTC: ₿{result['volume_btc']:,.4f}\\n\"\n",
    "                    f\"Close: <a href='{escaped_link}'>₿{result['close']:.8f}</a>\\n\"\n",
    "                    f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                    f\"Close Off Low: {result['close_off_low']:.1f}%\\n\"\n",
    "                    f\"Momentum: {result['momentum_score']:.4f}\\n\"\n",
    "                    f\"{'='*30}\\n\"\n",
    "                )\n",
    "            \n",
    "            # Split message more carefully to avoid breaking HTML tags\n",
    "            max_length = 4000\n",
    "            \n",
    "            if len(message) > max_length:\n",
    "                # Split at natural breaks (between results) to avoid breaking HTML\n",
    "                sections = message.split('='*30 + '\\n')\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for section in sections:\n",
    "                    if len(current_chunk + section + '='*30 + '\\n') > max_length:\n",
    "                        if current_chunk:\n",
    "                            await self.telegram_app.bot.send_message(\n",
    "                                chat_id=self.telegram_chat_id,\n",
    "                                text=current_chunk.strip(),\n",
    "                                parse_mode='HTML',\n",
    "                                disable_web_page_preview=True\n",
    "                            )\n",
    "                        current_chunk = section + '\\n'\n",
    "                    else:\n",
    "                        current_chunk += section + '='*30 + '\\n'\n",
    "                \n",
    "                # Send remaining chunk\n",
    "                if current_chunk.strip():\n",
    "                    await self.telegram_app.bot.send_message(\n",
    "                        chat_id=self.telegram_chat_id,\n",
    "                        text=current_chunk.strip(),\n",
    "                        parse_mode='HTML',\n",
    "                        disable_web_page_preview=True\n",
    "                    )\n",
    "            else:\n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=message,\n",
    "                    parse_mode='HTML',\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sending Telegram alert: {str(e)}\")\n",
    "            \n",
    "            # Fallback: send without HTML formatting\n",
    "            try:\n",
    "                simple_message = f\"🚨 Confluence Detection - Binance BTC Pairs {self.timeframe}\\n\\n\"\n",
    "                for result in results:\n",
    "                    simple_message += (\n",
    "                        f\"Symbol: {result['symbol']}\\n\"\n",
    "                        f\"Volume BTC: ₿{result['volume_btc']:,.4f}\\n\"\n",
    "                        f\"Close: ₿{result['close']:.8f}\\n\"\n",
    "                        f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                        f\"Components: Vol={result['high_volume']}, Spread={result['spread_breakout']}, Mom={result['momentum_breakout']}\\n\\n\"\n",
    "                    )\n",
    "                \n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=simple_message,\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "            except Exception as fallback_error:\n",
    "                logging.error(f\"Fallback Telegram send also failed: {str(fallback_error)}\")\n",
    "\n",
    "    def scan_single_market(self, pair, df):\n",
    "        \"\"\"Scan a single market for Confluence pattern in the specified bar\"\"\"\n",
    "        try:\n",
    "            if df is None or len(df) < 50:  # Need enough data for confluence\n",
    "                return None\n",
    "            \n",
    "            # Calculate which bar to check based on offset\n",
    "            check_bar = -(self.offset + 1)  # offset=0 means current bar (-1), offset=1 means last closed (-2), etc.\n",
    "            \n",
    "            # Run confluence detection\n",
    "            detected, result = detect_confluence(df, check_bar=check_bar)\n",
    "            \n",
    "            if detected:\n",
    "                # Get the target bar values\n",
    "                target_close = df['close'].iloc[check_bar]\n",
    "                target_volume = df['volume'].iloc[check_bar]  # This is already in BTC\n",
    "                \n",
    "                confluence_result = {\n",
    "                    'symbol': pair['symbol'],\n",
    "                    'volume_btc': float(target_volume),\n",
    "                    'close': float(target_close),\n",
    "                    'volume': float(target_volume),\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                    'bar_range': result['bar_range'],\n",
    "                    'timestamp': df.index[check_bar] if hasattr(df.index, '__getitem__') else None\n",
    "                }\n",
    "                return confluence_result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pair['symbol']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    async def scan_all_markets(self):\n",
    "        \"\"\"Scan all BTC markets for Confluence pattern\"\"\"\n",
    "        await self.init_telegram()\n",
    "        \n",
    "        try:\n",
    "            # Define volume thresholds for BTC pairs based on timeframe\n",
    "            volume_thresholds = {\n",
    "                # Minutes - higher volume needed for shorter timeframes\n",
    "                \"1m\": 0.01,   \"3m\": 0.02,   \"5m\": 0.03,\n",
    "                \"15m\": 0.05,  \"30m\": 0.08,\n",
    "                # Hours\n",
    "                \"1h\": 0.1,    \"2h\": 0.15,   \"4h\": 0.2,\n",
    "                \"6h\": 0.25,   \"8h\": 0.3,    \"12h\": 0.35,\n",
    "                # Days\n",
    "                \"1d\": 0.4,    \"2d\": 0.8,    \"3d\": 1.2,\n",
    "                # Weeks/Months\n",
    "                \"1w\": 2.0,    \"1M\": 8.0\n",
    "            }\n",
    "            min_volume = volume_thresholds.get(self.timeframe.lower(), 0.1)  # Default 0.1 BTC\n",
    "            \n",
    "            # Create offset description\n",
    "            if self.offset == 0:\n",
    "                offset_desc = \"current candle\"\n",
    "            elif self.offset == 1:\n",
    "                offset_desc = \"last closed candle\"\n",
    "            else:\n",
    "                offset_desc = f\"{self.offset} candles ago\"\n",
    "            \n",
    "            print(f\"Scanning Binance BTC pairs for Confluence patterns in {offset_desc}...\")\n",
    "            print(f\"Timeframe: {self.timeframe}\")\n",
    "            print(f\"Minimum volume threshold: ₿{min_volume:.2f}\")\n",
    "            \n",
    "            # Get all BTC pairs from Binance\n",
    "            print(\"Fetching BTC pairs from Binance...\")\n",
    "            btc_pairs = await self.get_btc_pairs()\n",
    "            \n",
    "            if not btc_pairs:\n",
    "                print(\"No BTC pairs found or error fetching pairs\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"Found {len(btc_pairs)} BTC pairs to scan\")\n",
    "            \n",
    "            # Filter out stablecoins and obvious non-trading pairs\n",
    "            filtered_pairs = []\n",
    "            skip_tokens = ['USDT', 'USDC', 'BUSD', 'DAI', 'TUSD', 'USDD', 'FDUSD']\n",
    "            \n",
    "            for pair in btc_pairs:\n",
    "                if pair['baseAsset'] not in skip_tokens:\n",
    "                    filtered_pairs.append(pair)\n",
    "            \n",
    "            print(f\"After filtering: {len(filtered_pairs)} pairs to scan\")\n",
    "            \n",
    "            # Get Binance timeframe - fallback to 1d if not found\n",
    "            binance_interval = self.timeframe_map.get(self.timeframe.lower(), \"1d\")\n",
    "            \n",
    "            # Process all pairs with progress bar\n",
    "            all_results = []\n",
    "            successful_scans = 0\n",
    "            \n",
    "            with tqdm(total=len(filtered_pairs), desc=\"Scanning markets\") as pbar:\n",
    "                for pair in filtered_pairs:\n",
    "                    try:\n",
    "                        # Get OHLCV data from Binance\n",
    "                        df = await self.get_klines(pair['symbol'], binance_interval, 100)\n",
    "                        \n",
    "                        if df is None or len(df) < 50:\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        successful_scans += 1\n",
    "                        target_idx = -(self.offset + 1)\n",
    "                        \n",
    "                        # Update progress bar with current symbol\n",
    "                        pbar.set_description(f\"Scanning: {pair['symbol']} ({len(df)} candles)\")\n",
    "                        \n",
    "                        # Check volume threshold for the target candle\n",
    "                        try:\n",
    "                            target_candle_volume = float(df['volume'].iloc[target_idx])  # Already in BTC\n",
    "                            \n",
    "                            # Only process if volume meets threshold\n",
    "                            if target_candle_volume >= min_volume:\n",
    "                                result = self.scan_single_market(pair, df)\n",
    "                                if result:\n",
    "                                    all_results.append(result)\n",
    "                                    print(f\"Found Confluence: {pair['symbol']} 🎯\")\n",
    "                        except (IndexError, ValueError):\n",
    "                            pass  # Skip if we can't calculate volume\n",
    "                        \n",
    "                        # Add small delay to respect rate limits\n",
    "                        await asyncio.sleep(0.1)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        if \"429\" in str(e):\n",
    "                            # Rate limit - add longer delay\n",
    "                            await asyncio.sleep(2)\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            \n",
    "            print(f\"Successfully scanned {successful_scans}/{len(filtered_pairs)} pairs\")\n",
    "            \n",
    "            # Sort by volume\n",
    "            all_results.sort(key=lambda x: x['volume_btc'], reverse=True)\n",
    "            \n",
    "            # Send Telegram alert if we found any patterns\n",
    "            if all_results:\n",
    "                await self.send_telegram_alert(all_results)\n",
    "            \n",
    "            return all_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scanning markets: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            await self.close_session()\n",
    "\n",
    "async def run_binance_btc_confluence_scanner(timeframe, offset=1):\n",
    "    \"\"\"\n",
    "    Run the Binance BTC Confluence scanner\n",
    "    \n",
    "    Parameters:\n",
    "    timeframe (str): Time period - Available options:\n",
    "                    Minutes: 1m, 3m, 5m, 15m, 30m\n",
    "                    Hours: 1h, 2h, 4h, 6h, 8h, 12h  \n",
    "                    Days: 1d, 2d, 3d\n",
    "                    Weeks/Months: 1w, 1M\n",
    "    offset (int): Bar offset (0=current, 1=last closed, 2=two bars ago, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if offset == 0:\n",
    "        offset_desc = \"current candle\"\n",
    "    elif offset == 1:\n",
    "        offset_desc = \"last closed candle\"\n",
    "    else:\n",
    "        offset_desc = f\"{offset} candles ago\"\n",
    "    \n",
    "    print(f\"Starting Binance BTC Confluence scan for {offset_desc} on {timeframe}...\")\n",
    "    \n",
    "    # Use the confluence telegram token\n",
    "    telegram_token = \"8066329517:AAHVr6kufZWe8UqCKPfmsRhSPleNlt_7G-g\"\n",
    "    telegram_chat_id = \"375812423\"\n",
    "    \n",
    "    scanner = BinanceBTCConfluenceScanner(telegram_token, telegram_chat_id, timeframe, offset)\n",
    "    results = await scanner.scan_all_markets()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} Confluence patterns:\")\n",
    "        \n",
    "        # Convert results to DataFrame for console display\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Round numeric columns for BTC precision\n",
    "        df_results['volume_btc'] = df_results['volume_btc'].round(4)\n",
    "        df_results['close'] = df_results['close'].round(8)\n",
    "        df_results['volume'] = df_results['volume'].round(4)\n",
    "        df_results['volume_ratio'] = df_results['volume_ratio'].round(2)\n",
    "        df_results['close_off_low'] = df_results['close_off_low'].round(1)\n",
    "        df_results['momentum_score'] = df_results['momentum_score'].round(4)\n",
    "        \n",
    "        # Reorder columns for better display\n",
    "        display_cols = ['symbol', 'close', 'volume_btc', 'volume_ratio', 'close_off_low', \n",
    "                       'momentum_score', 'high_volume', 'spread_breakout', 'momentum_breakout']\n",
    "        available_cols = [col for col in display_cols if col in df_results.columns]\n",
    "        \n",
    "        # Display the results\n",
    "        print(df_results[available_cols])\n",
    "        \n",
    "        # Show component analysis\n",
    "        print(f\"\\n🔧 COMPONENT ANALYSIS:\")\n",
    "        vol_count = df_results['high_volume'].sum()\n",
    "        spread_count = df_results['spread_breakout'].sum()\n",
    "        momentum_count = df_results['momentum_breakout'].sum()\n",
    "        \n",
    "        print(f\"High Volume signals: {vol_count}/{len(results)} ({vol_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Spread Breakout signals: {spread_count}/{len(results)} ({spread_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Momentum Breakout signals: {momentum_count}/{len(results)} ({momentum_count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo Confluence patterns found in {offset_desc}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Apply nest_asyncio to allow async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage functions\n",
    "async def scan_binance_btc_current():\n",
    "    \"\"\"Scan current candle for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=0)\n",
    "\n",
    "async def scan_binance_btc_closed():\n",
    "    \"\"\"Scan last closed candle for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=1)\n",
    "\n",
    "async def scan_binance_btc_previous():\n",
    "    \"\"\"Scan two candles ago for confluence - Binance BTC pairs\"\"\"\n",
    "    await run_binance_btc_confluence_scanner(\"1w\", offset=2)\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution - modify parameters here\n",
    "    \"\"\"\n",
    "    timeframe = \"1w\"  # 1d, 2d, 3d, 1w\n",
    "    offset = 0        # 0 = current candle, 1 = last closed candle, 2 = two candles ago\n",
    "    \n",
    "    await run_binance_btc_confluence_scanner(timeframe, offset)\n",
    "\n",
    "# Run the async main function\n",
    "print(\"🔍 BINANCE BTC CONFLUENCE SCANNER\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Available timeframes:\")\n",
    "print(\"• Minutes: 1m, 3m, 5m, 15m, 30m\")\n",
    "print(\"• Hours: 1h, 2h, 4h, 6h, 8h, 12h\") \n",
    "print(\"• Days: 1d, 2d, 3d\")\n",
    "print(\"• Weeks/Months: 1w, 1M\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"• await main() - Run with default settings\")\n",
    "print(\"• await scan_binance_btc_current() - Scan current candle\")\n",
    "print(\"• await scan_binance_btc_closed() - Scan last closed candle\")\n",
    "print(\"• await scan_binance_btc_previous() - Scan two candles ago\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('timeframe', offset) - Custom scan\")\n",
    "print(\"\\nExamples:\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('4h', 1)  # 4-hour last closed\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('15m', 0) # 15-min current\")\n",
    "print(\"• await run_binance_btc_confluence_scanner('1M', 1)  # Monthly last closed\")\n",
    "print(\"This scanner uses REAL Binance BTC pair volumes!\")\n",
    "\n",
    "# Uncomment to auto-run:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804bead-b40b-4676-9951-8d4e30e29ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug ohlcv data of any pair\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "\n",
    "from exchanges import BybitFuturesClient  # Ensure this matches your exchanges/__init__.py\n",
    "\n",
    "async def test_fetch():\n",
    "    client = BybitFuturesClient(timeframe=\"2d\")\n",
    "    await client.init_session()\n",
    "    df = await client.fetch_klines(\"L3USDT\")\n",
    "    await client.close_session()\n",
    "    if df is not None:\n",
    "        print(\"2d Candles for L3:\")\n",
    "        print(df.tail(5))  # Last 5 weeks\n",
    "        last_row = df.iloc[-1]\n",
    "        volume_usd = last_row['volume'] * last_row['close']\n",
    "        print(f\"Last Week: volume_usd={volume_usd:.2f}, close={last_row['close']}, volume={last_row['volume']:.2f}\")\n",
    "\n",
    "# Run the async function directly in the notebook\n",
    "await test_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b52f4-ca65-425b-a41a-42b70a57aefa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Direct strategy debug of any pair on any exchange\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "from exchanges import MexcSpotClient, BybitSpotClient, GateioSpotClient, KucoinSpotClient, BinanceSpotClient, BinanceFuturesClient, BybitFuturesClient\n",
    "from custom_strategies import detect_volume_surge, detect_weak_uptrend, detect_pin_down\n",
    "from breakout_vsa import vsa_detector, breakout_bar_vsa, stop_bar_vsa, reversal_bar_vsa, start_bar_vsa, loaded_bar_vsa, test_bar_vsa\n",
    "\n",
    "async def test_strategy(exchange_client_class, timeframe, symbol, strategy_name):\n",
    "    client = exchange_client_class(timeframe=timeframe)\n",
    "    await client.init_session()\n",
    "    df = await client.fetch_klines(symbol)\n",
    "    await client.close_session()\n",
    "    \n",
    "    if df is None or len(df) < 10:\n",
    "        print(f\"No data fetched for {symbol} or insufficient data (< 10 bars)\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{timeframe} Candles for {symbol}:\")\n",
    "    print(df.tail(5))\n",
    "    last_row = df.iloc[-1]\n",
    "    volume_usd = last_row['volume'] * last_row['close']\n",
    "    print(f\"Last Bar: volume_usd={volume_usd:.2f}, close={last_row['close']}, volume={last_row['volume']:.2f}\")\n",
    "    \n",
    "    # Different handling based on strategy type\n",
    "    if strategy_name == \"volume_surge\":\n",
    "        # Use detect_volume_surge directly\n",
    "        detected, result = detect_volume_surge(df)\n",
    "        \n",
    "        print(f\"\\nVolume Surge Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nVolume Surge Details:\")\n",
    "            print(f\"  Date: {result['timestamp']}\")\n",
    "            print(f\"  Close: ${result['close_price']:,.8f}\")\n",
    "            print(f\"  Volume: {result['volume']:,.2f}\")\n",
    "            print(f\"  Volume USD: ${result['volume_usd']:,.2f}\")\n",
    "            print(f\"  Volume Ratio: {result['volume_ratio']:,.2f}x\")\n",
    "            print(f\"  Score: {result['score']:,.2f}\")\n",
    "            print(f\"  Price Extreme: {result['price_extreme']}\")\n",
    "    \n",
    "    elif strategy_name == \"pin_down\":\n",
    "        from custom_strategies import detect_pin_down\n",
    "        detected, result = detect_pin_down(df)\n",
    "        \n",
    "        print(f\"\\nPin Down Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nPin Down Details:\")\n",
    "            for key, value in result.items():\n",
    "                if key != 'symbol':  # Skip symbol as we already know it\n",
    "                    print(f\"  {key}: {value}\")\n",
    "    \n",
    "    elif strategy_name == \"weak_uptrend\":\n",
    "        from custom_strategies import detect_weak_uptrend\n",
    "        detected, result = detect_weak_uptrend(df)\n",
    "        \n",
    "        print(f\"\\nWeak Uptrend Detection Results:\")\n",
    "        print(f\"Detected: {detected}\")\n",
    "        \n",
    "        if detected:\n",
    "            print(f\"\\nWeak Uptrend Details:\")\n",
    "            for key, value in result.items():\n",
    "                if key != 'symbol':  # Skip symbol as we already know it\n",
    "                    print(f\"  {key}: {value}\")\n",
    "    \n",
    "    else:\n",
    "        # For VSA strategies, import the appropriate get_params\n",
    "        if strategy_name == \"reversal_bar\":\n",
    "            from breakout_vsa.strategies.reversal_bar import get_params\n",
    "        elif strategy_name == \"breakout_bar\":\n",
    "            from breakout_vsa.strategies.breakout_bar import get_params\n",
    "        elif strategy_name == \"loaded_bar\":\n",
    "            from breakout_vsa.strategies.loaded_bar import get_params\n",
    "        elif strategy_name == \"stop_bar\":\n",
    "            from breakout_vsa.strategies.stop_bar import get_params\n",
    "        elif strategy_name == \"start_bar\":\n",
    "            from breakout_vsa.strategies.start_bar import get_params\n",
    "        else:\n",
    "            print(f\"Unknown strategy: {strategy_name}\")\n",
    "            return\n",
    "        \n",
    "        # Use vsa_detector with strategy-specific params\n",
    "        params = get_params()\n",
    "        condition, result = vsa_detector(df, params)\n",
    "        \n",
    "        strategy_display_name = strategy_name.replace('_vsa', '').replace('_', ' ').title()\n",
    "        print(f\"\\n{strategy_display_name} Detection Results:\")\n",
    "        print(f\"Current Bar (index -1): {condition.iloc[-1]}\")\n",
    "        if len(df) > 1:\n",
    "            print(f\"Last Closed Bar (index -2): {condition.iloc[-2]}\")\n",
    "        \n",
    "        if condition.iloc[-1] or (len(df) > 1 and condition.iloc[-2]):\n",
    "            detected_idx = -1 if condition.iloc[-1] else -2\n",
    "            volume_mean = df['volume'].rolling(7).mean().iloc[detected_idx]\n",
    "            bar_range = df['high'].iloc[detected_idx] - df['low'].iloc[detected_idx]\n",
    "            close_off_low = (df['close'].iloc[detected_idx] - df['low'].iloc[detected_idx]) / bar_range * 100 if bar_range > 0 else 0\n",
    "            volume_usd_detected = df['volume'].iloc[detected_idx] * df['close'].iloc[detected_idx]\n",
    "            \n",
    "            arctan_ratio = result['arctan_ratio'].iloc[detected_idx]  # From result DataFrame\n",
    "            \n",
    "            print(f\"\\nDetected at index {detected_idx} ({'Current' if detected_idx == -1 else 'Last Closed'} Bar):\")\n",
    "            print(f\"  Date: {df.index[detected_idx]}\")\n",
    "            print(f\"  Close: ${df['close'].iloc[detected_idx]:,.8f}\")\n",
    "            print(f\"  Volume Ratio: {df['volume'].iloc[detected_idx] / volume_mean if volume_mean > 0 else 0:.2f}x\")\n",
    "            print(f\"  {timeframe} Volume: ${volume_usd_detected:.2f}\")\n",
    "            print(f\"  Close Off Low: {close_off_low:.1f}%\")\n",
    "            print(f\"  Angular Ratio: {arctan_ratio:.2f}\")\n",
    "\n",
    "# Define the test case\n",
    "exchange_client = GateioSpotClient\n",
    "timeframe = \"1w\"\n",
    "symbol = \"PRCL_USDT\"\n",
    "strategy = \"loaded_bar\"\n",
    "await test_strategy(exchange_client, timeframe, symbol, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9bf3d-5437-413f-a6a8-2496412059e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#zip the project\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Go to parent directory of your project\n",
    "os.chdir(\"/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025\")\n",
    "\n",
    "# Create the zip file (this will include everything inside 'hbs_2025')\n",
    "shutil.make_archive(\"Project_VSA_2025_backup\", 'zip', \"Project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c74d8-f489-49e8-8596-c36b3960fbe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 -> Debug built weekly candles for mexc and kucoin \n",
    "import sys\n",
    "import os\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "import asyncio\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from exchanges.kucoin_client import KucoinClient\n",
    "from breakout_vsa.core import calculate_start_bar\n",
    "\n",
    "from scanner.main import kline_cache\n",
    "kline_cache.clear()  # Clear cache for fresh data\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "async def debug_start_bar_detection():\n",
    "    # Initialize client\n",
    "    client = KucoinClient(timeframe=\"1w\")\n",
    "    await client.init_session()\n",
    "    \n",
    "    # Symbol to debug\n",
    "    symbol = \"TAO-USDT\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch data\n",
    "        df = await client.fetch_klines(symbol)\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"Weekly candles for {symbol}:\")\n",
    "            print(df.tail())\n",
    "            \n",
    "            # Add intermediate calculations to see what's happening\n",
    "            # This is a modified version of calculate_start_bar that adds debugging\n",
    "            lookback = 5\n",
    "            volume_lookback = 30\n",
    "            volume_percentile = 50\n",
    "            low_percentile = 75\n",
    "            range_percentile = 75\n",
    "            close_off_lows_percent = 50\n",
    "            prev_close_range = 75\n",
    "            \n",
    "            # Calculate basic bar characteristics\n",
    "            df['bar_range'] = df['high'] - df['low']\n",
    "            df['volume_rank'] = df['volume'].rolling(lookback).apply(\n",
    "                lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                raw=True\n",
    "            )\n",
    "            \n",
    "            # Calculate rolling values\n",
    "            df['macro_low'] = df['low'].rolling(volume_lookback).min()\n",
    "            df['macro_high'] = df['high'].rolling(volume_lookback).max()\n",
    "            df['highest_high'] = df['high'].rolling(lookback).max()\n",
    "            \n",
    "            # Volume conditions\n",
    "            df['volume_sma'] = df['volume'].rolling(volume_lookback).mean()\n",
    "            df['volume_std'] = df['volume'].rolling(volume_lookback).std()\n",
    "            df['excess_volume'] = df['volume'] > (df['volume_sma'] + 3.0 * df['volume_std'])\n",
    "            \n",
    "            # Range conditions\n",
    "            df['range_sma'] = df['bar_range'].rolling(volume_lookback).mean()\n",
    "            df['range_std'] = df['bar_range'].rolling(volume_lookback).std()\n",
    "            df['excess_range'] = df['bar_range'] > (df['range_sma'] + 3.0 * df['range_std'])\n",
    "            \n",
    "            # Volume percentile condition\n",
    "            def is_in_top_percent(series, length, percent):\n",
    "                ranks = series.rolling(length).apply(\n",
    "                    lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                    raw=True\n",
    "                )\n",
    "                return ranks >= percent\n",
    "            \n",
    "            def is_in_bottom_percent(series, length, percent):\n",
    "                ranks = series.rolling(length).apply(\n",
    "                    lambda x: sum(1.0 for val in x if val <= x[-1]) / len(x) * 100, \n",
    "                    raw=True\n",
    "                )\n",
    "                return ranks <= percent\n",
    "            \n",
    "            # Volume conditions\n",
    "            df['is_higher_volume'] = is_in_top_percent(df['volume'], lookback, volume_percentile)\n",
    "            df['is_high_volume'] = (df['volume'] > 0.75 * df['volume_sma']) & (df['volume'] > df['volume'].shift(1))\n",
    "            \n",
    "            # Price action conditions\n",
    "            df['has_higher_high'] = df['high'] > df['high'].shift(1)\n",
    "            df['no_narrow_range'] = is_in_top_percent(df['bar_range'], lookback, range_percentile)\n",
    "            \n",
    "            # Low price condition\n",
    "            df['is_in_the_lows'] = (\n",
    "                (df['low'] - df['macro_low']).abs() < df['bar_range']\n",
    "            ) | is_in_bottom_percent(df['low'], volume_lookback, low_percentile)\n",
    "            \n",
    "            # Close position conditions\n",
    "            df['close_in_the_highs'] = (\n",
    "                (df['close'] - df['low']) / df['bar_range']\n",
    "            ) >= (close_off_lows_percent / 100)\n",
    "            \n",
    "            # Previous close distance condition\n",
    "            df['far_prev_close'] = (\n",
    "                (df['close'] - df['close'].shift(1)).abs() >=\n",
    "                (df['bar_range'].shift(1) * (prev_close_range / 100))\n",
    "            )\n",
    "            \n",
    "            # New highs condition\n",
    "            df['new_highs'] = df['high'] >= 0.75 * df['highest_high']\n",
    "            \n",
    "            # Optional strength condition\n",
    "            df['strong_close'] = df['close'] >= df['highest_high'].shift(1)\n",
    "            \n",
    "            # Now check the actual values for the last few bars\n",
    "            last_rows = df.tail(3)\n",
    "            \n",
    "            print(\"\\nAnalyzing last 3 bars:\")\n",
    "            for idx, row in last_rows.iterrows():\n",
    "                print(f\"\\nBar at {idx.strftime('%Y-%m-%d')}:\")\n",
    "                print(f\"  is_high_volume: {row['is_high_volume']}\")\n",
    "                print(f\"  has_higher_high: {row['has_higher_high']}\")\n",
    "                print(f\"  no_narrow_range: {row['no_narrow_range']}\")\n",
    "                print(f\"  close_in_the_highs: {row['close_in_the_highs']}\")\n",
    "                print(f\"  far_prev_close: {row['far_prev_close']}\")\n",
    "                print(f\"  excess_range: {row['excess_range']}\")\n",
    "                print(f\"  excess_volume: {row['excess_volume']}\")\n",
    "                print(f\"  new_highs: {row['new_highs']}\")\n",
    "                print(f\"  is_in_the_lows: {row['is_in_the_lows']}\")\n",
    "                print(f\"  volume: {row['volume']}, volume_sma: {row['volume_sma']}\")\n",
    "                print(f\"  bar_range: {row['bar_range']}, range_sma: {row['range_sma']}\")\n",
    "                \n",
    "            # Run the original function to confirm\n",
    "            start_bar_pattern = calculate_start_bar(df)\n",
    "            print(f\"\\nFinal Start Bar detection result:\")\n",
    "            print(start_bar_pattern.tail(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug: {str(e)}\")\n",
    "    finally:\n",
    "        await client.close_session()\n",
    "\n",
    "# Replace the last part of your script with this:\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # For Jupyter/IPython environments\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        asyncio.run(debug_start_bar_detection())\n",
    "    except ImportError:\n",
    "        # For regular Python environments\n",
    "        asyncio.run(debug_start_bar_detection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dffeddd-c2c8-4f75-887f-9ed7d1961a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025/Project_VSA_2025_backup.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip the project\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Go to parent directory of your project\n",
    "os.chdir(\"/home/jovyan/work/Crypto/sevenfigures-bot/hbs_2025\")\n",
    "\n",
    "# Create the zip file (this will include everything inside 'hbs_2025')\n",
    "shutil.make_archive(\"Project_VSA_2025_backup\", 'zip', \"Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29951e-0a3b-4d0b-928c-0f2bfce7c11c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ARCHIVE - Confluence Scanner with bar offset\n",
    "\n",
    "from telegram.ext import Application\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "from tqdm.asyncio import tqdm\n",
    "import sys\n",
    "import os\n",
    "import html\n",
    "\n",
    "# Add project path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from exchanges.sf_pairs_service import SFPairsService\n",
    "from custom_strategies import detect_confluence\n",
    "\n",
    "class ConfluenceScanner:\n",
    "    def __init__(self, telegram_token, telegram_chat_id, exchange, timeframe, offset=1):\n",
    "        self.telegram_token = telegram_token\n",
    "        self.telegram_chat_id = telegram_chat_id\n",
    "        self.telegram_app = None\n",
    "        self.exchange = exchange\n",
    "        self.timeframe = timeframe\n",
    "        self.offset = offset  # Added offset parameter\n",
    "        self.sf_service = SFPairsService()\n",
    "        \n",
    "    async def init_telegram(self):\n",
    "        if self.telegram_app is None:\n",
    "            self.telegram_app = Application.builder().token(self.telegram_token).build()\n",
    "\n",
    "    async def send_telegram_alert(self, results):\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            message = f\"🚨 Confluence Detection - {self.exchange} {self.timeframe}\\n\\n\"\n",
    "            \n",
    "            # Map timeframe to TradingView format\n",
    "            tv_timeframe_map = {\n",
    "                \"1d\": \"1D\",\n",
    "                \"2d\": \"2D\",\n",
    "                \"1w\": \"1W\"\n",
    "            }\n",
    "            tv_timeframe = tv_timeframe_map.get(self.timeframe.lower(), self.timeframe)\n",
    "            \n",
    "            for result in results:\n",
    "                exchange_name = self.exchange.upper()\n",
    "                formatted_symbol = f\"{result['symbol']}\"\n",
    "                tv_link = f\"https://www.tradingview.com/chart/?symbol={exchange_name}:{formatted_symbol}&interval={tv_timeframe}\"\n",
    "                \n",
    "                # Escape HTML entities in the URL\n",
    "                escaped_link = html.escape(tv_link)\n",
    "                \n",
    "                # Format according to specified requirements\n",
    "                time_str = \"\"\n",
    "                if result.get('timestamp') is not None:\n",
    "                    time_str = f\"Time: {result['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "                \n",
    "                message += (\n",
    "                    f\"Symbol: {result['symbol']}\\n\"\n",
    "                    f\"{time_str}\"\n",
    "                    f\"Volume USD: ${result['volume_usd']:,.2f}\\n\"\n",
    "                    f\"Close: <a href='{escaped_link}'>${result['close']:,.8f}</a>\\n\"\n",
    "                    f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                    f\"Close Off Low: {result['close_off_low']:.1f}%\\n\"\n",
    "                    f\"{'='*30}\\n\"\n",
    "                )\n",
    "            \n",
    "            # Split message more carefully to avoid breaking HTML tags\n",
    "            max_length = 4000  # Reduced from 4096 to be safer\n",
    "            \n",
    "            if len(message) > max_length:\n",
    "                # Split at natural breaks (between results) to avoid breaking HTML\n",
    "                sections = message.split('='*30 + '\\n')\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for section in sections:\n",
    "                    if len(current_chunk + section + '='*30 + '\\n') > max_length:\n",
    "                        if current_chunk:\n",
    "                            await self.telegram_app.bot.send_message(\n",
    "                                chat_id=self.telegram_chat_id,\n",
    "                                text=current_chunk.strip(),\n",
    "                                parse_mode='HTML',\n",
    "                                disable_web_page_preview=True\n",
    "                            )\n",
    "                        current_chunk = section + '\\n'\n",
    "                    else:\n",
    "                        current_chunk += section + '='*30 + '\\n'\n",
    "                \n",
    "                # Send remaining chunk\n",
    "                if current_chunk.strip():\n",
    "                    await self.telegram_app.bot.send_message(\n",
    "                        chat_id=self.telegram_chat_id,\n",
    "                        text=current_chunk.strip(),\n",
    "                        parse_mode='HTML',\n",
    "                        disable_web_page_preview=True\n",
    "                    )\n",
    "            else:\n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=message,\n",
    "                    parse_mode='HTML',\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error sending Telegram alert: {str(e)}\")\n",
    "            \n",
    "            # Fallback: send without HTML formatting\n",
    "            try:\n",
    "                simple_message = f\"🚨 Confluence Detection - {self.exchange} {self.timeframe}\\n\\n\"\n",
    "                for result in results:\n",
    "                    simple_message += (\n",
    "                        f\"Symbol: {result['symbol']}\\n\"\n",
    "                        f\"Volume USD: ${result['volume_usd']:,.2f}\\n\"\n",
    "                        f\"Close: ${result['close']:,.8f}\\n\"\n",
    "                        f\"Volume Ratio: {result['volume_ratio']:.2f}x\\n\"\n",
    "                        f\"Components: Vol={result['high_volume']}, Spread={result['spread_breakout']}, Mom={result['momentum_breakout']}\\n\\n\"\n",
    "                    )\n",
    "                \n",
    "                await self.telegram_app.bot.send_message(\n",
    "                    chat_id=self.telegram_chat_id,\n",
    "                    text=simple_message,\n",
    "                    disable_web_page_preview=True\n",
    "                )\n",
    "            except Exception as fallback_error:\n",
    "                logging.error(f\"Fallback Telegram send also failed: {str(fallback_error)}\")\n",
    "\n",
    "    def prepare_sf_data(self, raw_df):\n",
    "        \"\"\"Convert SF data to confluence-compatible format\"\"\"\n",
    "        if raw_df is None or len(raw_df) == 0:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(raw_df)\n",
    "        \n",
    "        # Convert datetime column to pandas datetime and set as index\n",
    "        if 'datetime' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df = df.set_index('datetime')\n",
    "        elif 'time' in df.columns:\n",
    "            # Convert Unix timestamp to datetime\n",
    "            df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "            df = df.set_index('time')\n",
    "        \n",
    "        # Select only OHLCV columns needed for confluence\n",
    "        required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        available_cols = [col for col in required_cols if col in df.columns]\n",
    "        \n",
    "        if len(available_cols) != 5:\n",
    "            return None\n",
    "        \n",
    "        # Select and clean data\n",
    "        result_df = df[required_cols].copy()\n",
    "        \n",
    "        # Ensure numeric types\n",
    "        for col in required_cols:\n",
    "            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "        \n",
    "        # Drop any NaN rows\n",
    "        result_df = result_df.dropna()\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def scan_single_market(self, pair, ohlcv_data):\n",
    "        \"\"\"Scan a single market for Confluence pattern in the specified bar\"\"\"\n",
    "        try:\n",
    "            # Prepare data for confluence analysis\n",
    "            df = self.prepare_sf_data(ohlcv_data)\n",
    "            \n",
    "            if df is None or len(df) < 50:  # Need enough data for confluence\n",
    "                return None\n",
    "            \n",
    "            # Calculate which bar to check based on offset\n",
    "            check_bar = -(self.offset + 1)  # offset=0 means current bar (-1), offset=1 means last closed (-2), etc.\n",
    "            \n",
    "            # Run confluence detection\n",
    "            detected, result = detect_confluence(df, check_bar=check_bar)\n",
    "            \n",
    "            if detected:\n",
    "                # Calculate volume in USD for the target bar\n",
    "                target_close = df['close'].iloc[check_bar]\n",
    "                target_volume = df['volume'].iloc[check_bar]\n",
    "                volume_usd = float(target_close) * float(target_volume)\n",
    "                \n",
    "                confluence_result = {\n",
    "                    'symbol': f\"{pair['Token']}{pair['Quote']}\",\n",
    "                    'volume_usd': volume_usd,\n",
    "                    'close': float(target_close),\n",
    "                    'volume': float(target_volume),\n",
    "                    'volume_ratio': result['volume_ratio'],\n",
    "                    'close_off_low': result['close_off_low'],\n",
    "                    'momentum_score': result['momentum_score'],\n",
    "                    'high_volume': result['high_volume'],\n",
    "                    'spread_breakout': result['spread_breakout'],\n",
    "                    'momentum_breakout': result['momentum_breakout'],\n",
    "                    'bar_range': result['bar_range']\n",
    "                }\n",
    "                return confluence_result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pair['Token']}{pair['Quote']}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    async def scan_all_markets(self):\n",
    "        \"\"\"Scan all markets for Confluence pattern\"\"\"\n",
    "        await self.init_telegram()\n",
    "        try:\n",
    "            # Define volume thresholds\n",
    "            volume_thresholds = {\n",
    "                \"1w\": 300000,\n",
    "                \"2d\": 100000,\n",
    "                \"1d\": 50000\n",
    "            }\n",
    "            min_volume = volume_thresholds.get(self.timeframe.lower(), 50000)\n",
    "            \n",
    "            # Create offset description\n",
    "            if self.offset == 0:\n",
    "                offset_desc = \"current candle\"\n",
    "            elif self.offset == 1:\n",
    "                offset_desc = \"last closed candle\"\n",
    "            else:\n",
    "                offset_desc = f\"{self.offset} candles ago\"\n",
    "            \n",
    "            print(f\"Scanning for Confluence patterns in {offset_desc}...\")\n",
    "            print(f\"Minimum volume threshold: ${min_volume:,.0f}\")\n",
    "            \n",
    "            # Get all pairs from SF service\n",
    "            pairs = self.sf_service.get_pairs_of_exchange(self.exchange)\n",
    "            print(f\"Found {len(pairs)} markets to scan...\")\n",
    "            \n",
    "            # Process all pairs with progress bar\n",
    "            all_results = []\n",
    "            with tqdm(total=len(pairs), desc=\"Scanning markets\") as pbar:\n",
    "                for pair in pairs:\n",
    "                    try:\n",
    "                        # Get OHLCV data from SF service\n",
    "                        ohlcv_data = self.sf_service.get_ohlcv_for_pair(\n",
    "                            pair['Token'], \n",
    "                            pair['Quote'], \n",
    "                            self.exchange, \n",
    "                            self.timeframe, \n",
    "                            100  # Get more data for confluence analysis\n",
    "                        )\n",
    "                        \n",
    "                        if ohlcv_data is None or len(ohlcv_data) == 0:\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "                        \n",
    "                        df = pd.DataFrame(ohlcv_data)\n",
    "                        \n",
    "                        # Check if we have enough data\n",
    "                        if len(df) >= 50:  # Need enough for confluence analysis\n",
    "                            target_idx = -(self.offset + 1)  # Adjust index based on offset\n",
    "                            \n",
    "                            # Check volume threshold for the target candle\n",
    "                            try:\n",
    "                                target_candle_volume = float(df['close'].iloc[target_idx]) * float(df['volume'].iloc[target_idx])\n",
    "                                \n",
    "                                # Only process if volume meets threshold\n",
    "                                if target_candle_volume >= min_volume:\n",
    "                                    result = self.scan_single_market(pair, ohlcv_data)\n",
    "                                    if result:\n",
    "                                        all_results.append(result)\n",
    "                                        print(f\"Found Confluence: {pair['Token']}{pair['Quote']} 🎯\")\n",
    "                            except (IndexError, ValueError):\n",
    "                                pass  # Skip if we can't calculate volume\n",
    "                                    \n",
    "                    except Exception as e:\n",
    "                        if \"500\" not in str(e):  # Don't log 500 errors\n",
    "                            logging.error(f\"Error processing {pair['Token']}{pair['Quote']}: {str(e)}\")\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            \n",
    "            # Sort by volume\n",
    "            all_results.sort(key=lambda x: x['volume_usd'], reverse=True)\n",
    "            \n",
    "            # Send Telegram alert if we found any patterns\n",
    "            if all_results:\n",
    "                await self.send_telegram_alert(all_results)\n",
    "            \n",
    "            return all_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scanning markets: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "async def run_confluence_scanner(exchange, timeframe, offset=1):\n",
    "    \"\"\"\n",
    "    Run the Confluence scanner\n",
    "    \n",
    "    Parameters:\n",
    "    exchange (str): Exchange name (Kucoin, Mexc, Binance)\n",
    "    timeframe (str): Time period (1d, 2d, 1w)\n",
    "    offset (int): Bar offset (0=current, 1=last closed, 2=two bars ago, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    if offset == 0:\n",
    "        offset_desc = \"current candle\"\n",
    "    elif offset == 1:\n",
    "        offset_desc = \"last closed candle\"\n",
    "    else:\n",
    "        offset_desc = f\"{offset} candles ago\"\n",
    "    \n",
    "    print(f\"Starting Confluence scan for {offset_desc} on {exchange} {timeframe}...\")\n",
    "    \n",
    "    # Use the confluence telegram token from your big project config\n",
    "    # You should replace this with the actual token from utils/config.py TELEGRAM_TOKENS[\"confluence\"]\n",
    "    telegram_token = \"8066329517:AAHVr6kufZWe8UqCKPfmsRhSPleNlt_7G-g\"  # Replace with confluence token\n",
    "    telegram_chat_id = \"375812423\"  # Your chat ID\n",
    "    \n",
    "    scanner = ConfluenceScanner(telegram_token, telegram_chat_id, exchange, timeframe, offset)\n",
    "    results = await scanner.scan_all_markets()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} Confluence patterns:\")\n",
    "        \n",
    "        # Convert results to DataFrame for console display\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Round numeric columns\n",
    "        df_results['volume_usd'] = df_results['volume_usd'].round(2)\n",
    "        df_results['close'] = df_results['close'].round(8)\n",
    "        df_results['volume'] = df_results['volume'].round(2)\n",
    "        df_results['volume_ratio'] = df_results['volume_ratio'].round(2)\n",
    "        df_results['close_off_low'] = df_results['close_off_low'].round(1)\n",
    "        df_results['momentum_score'] = df_results['momentum_score'].round(4)\n",
    "        \n",
    "        # Reorder columns for better display\n",
    "        display_cols = ['symbol', 'close', 'volume_usd', 'volume_ratio', 'close_off_low', \n",
    "                       'momentum_score', 'high_volume', 'spread_breakout', 'momentum_breakout']\n",
    "        available_cols = [col for col in display_cols if col in df_results.columns]\n",
    "        \n",
    "        # Display the results\n",
    "        print(df_results[available_cols])\n",
    "        \n",
    "        # Show component analysis\n",
    "        print(f\"\\n🔧 COMPONENT ANALYSIS:\")\n",
    "        vol_count = df_results['high_volume'].sum()\n",
    "        spread_count = df_results['spread_breakout'].sum()\n",
    "        momentum_count = df_results['momentum_breakout'].sum()\n",
    "        \n",
    "        print(f\"High Volume signals: {vol_count}/{len(results)} ({vol_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Spread Breakout signals: {spread_count}/{len(results)} ({spread_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"Momentum Breakout signals: {momentum_count}/{len(results)} ({momentum_count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo Confluence patterns found in {offset_desc}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Apply nest_asyncio to allow async operations in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Example usage functions\n",
    "async def scan_current_confluence():\n",
    "    \"\"\"Scan current candle for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=0)\n",
    "\n",
    "async def scan_closed_confluence():\n",
    "    \"\"\"Scan last closed candle for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=1)\n",
    "\n",
    "async def scan_previous_confluence():\n",
    "    \"\"\"Scan two candles ago for confluence\"\"\"\n",
    "    await run_confluence_scanner(\"Kucoin\", \"1w\", offset=2)\n",
    "\n",
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution - modify parameters here\n",
    "    \"\"\"\n",
    "    exchange = \"Mexc\"  # Binance, Kucoin, Mexc\n",
    "    timeframe = \"1w\"     # 1d, 2d, 1w\n",
    "    offset = 0           # 0 = current candle, 1 = last closed candle, 2 = two candles ago\n",
    "    \n",
    "    await run_confluence_scanner(exchange, timeframe, offset)\n",
    "\n",
    "# Run the async main function\n",
    "print(\"🔍 CONFLUENCE SCANNER\")\n",
    "print(\"=\" * 30)\n",
    "print(\"Available functions:\")\n",
    "print(\"• await main() - Run with default settings\")\n",
    "print(\"• await scan_current_confluence() - Scan current candle\")\n",
    "print(\"• await scan_closed_confluence() - Scan last closed candle\")\n",
    "print(\"• await scan_previous_confluence() - Scan two candles ago\")\n",
    "print(\"• await run_confluence_scanner('Exchange', 'timeframe', offset) - Custom scan\")\n",
    "print(\"\\nExample: await main()\")\n",
    "\n",
    "# Uncomment to auto-run:\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016f39c-4a43-4dba-ab57-5c161bff6e4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Direct Test Bar Pattern Tester\n",
    "# This script directly tests the calculate_test_bar function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Apply nest_asyncio to make asyncio work in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Add project directory to path\n",
    "project_dir = os.path.join(os.getcwd(), \"Project\")\n",
    "sys.path.insert(0, project_dir)\n",
    "print(f\"✓ Added {project_dir} to sys.path\")\n",
    "\n",
    "# Import the necessary modules\n",
    "from exchanges import MexcSpotClient, BybitSpotClient, KucoinSpotClient\n",
    "\n",
    "def calculate_test_bar_direct(df):\n",
    "    \"\"\"\n",
    "    Direct implementation of Test Bar pattern detection for testing\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    df['bar_range'] = df['high'] - df['low']\n",
    "    \n",
    "    # Function to check if current bar is an inside bar\n",
    "    df['is_inside_bar'] = (df['high'] < df['high'].shift(1)) & (df['low'] > df['low'].shift(1))\n",
    "    \n",
    "    # Function to check if bar is down (close < open)\n",
    "    df['is_down_bar'] = df['close'] < df['open']\n",
    "    \n",
    "    # Function to check if close is in higher 65% of the spread (closing off the lows)\n",
    "    df['close_position'] = np.where(df['bar_range'] != 0, (df['close'] - df['low']) / df['bar_range'], 0)\n",
    "    df['close_off_lows'] = df['close_position'] >= 0.65\n",
    "    \n",
    "    # Function to check if current volume is less than 40% of previous bar volume\n",
    "    df['lower_volume_than_prev'] = df['volume'] < (df['volume'].shift(1) * 0.4)\n",
    "    \n",
    "    # Function to check if current volume is the lowest in last 3 bars\n",
    "    df['lowest_volume_in_3_bars'] = (df['volume'] < df['volume'].shift(1)) & (df['volume'] < df['volume'].shift(2))\n",
    "    \n",
    "    # Debug columns\n",
    "    df['debug_inside'] = df['is_inside_bar']\n",
    "    df['debug_down'] = df['is_down_bar'] \n",
    "    df['debug_close_off_lows'] = df['close_off_lows']\n",
    "    df['debug_lower_vol'] = df['lower_volume_than_prev']\n",
    "    df['debug_lowest_vol_3'] = df['lowest_volume_in_3_bars']\n",
    "    \n",
    "    # Main condition: all criteria must be met\n",
    "    test_bar_pattern = (\n",
    "        df['is_inside_bar'] &\n",
    "        df['is_down_bar'] &\n",
    "        df['close_off_lows'] &\n",
    "        df['lower_volume_than_prev'] &\n",
    "        df['lowest_volume_in_3_bars']\n",
    "    )\n",
    "    \n",
    "    # Signal only new occurrences\n",
    "    test_bar = test_bar_pattern & ~test_bar_pattern.shift(1).fillna(False)\n",
    "    \n",
    "    return test_bar, df\n",
    "\n",
    "async def test_direct_test_bar(exchange_client_class, timeframe, symbol):\n",
    "    \"\"\"Test the direct test_bar function on a specific symbol\"\"\"\n",
    "    print(f\"Testing DIRECT Test Bar detection on {symbol} ({timeframe})\")\n",
    "    \n",
    "    # Initialize exchange client and fetch data\n",
    "    client = exchange_client_class(timeframe=timeframe)\n",
    "    try:\n",
    "        await client.init_session()\n",
    "        df = await client.fetch_klines(symbol)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {str(e)}\")\n",
    "        return\n",
    "    finally:\n",
    "        await client.close_session()\n",
    "    \n",
    "    if df is None or len(df) < 10:\n",
    "        print(f\"No data fetched for {symbol} or insufficient data\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\\\nLast 5 bars for {symbol}:\")\n",
    "    print(df[['open', 'high', 'low', 'close', 'volume']].tail(5))\n",
    "    \n",
    "    # Apply direct test_bar function\n",
    "    print(\"\\\\nApplying direct test_bar detection...\")\n",
    "    try:\n",
    "        test_bar_signals, df_with_debug = calculate_test_bar_direct(df)\n",
    "        \n",
    "        # Quick results first\n",
    "        current_detected = test_bar_signals.iloc[-1]\n",
    "        prev_detected = test_bar_signals.iloc[-2] if len(df) > 1 else False\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(\"QUICK RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Current Bar Test Pattern:  {current_detected}\")\n",
    "        print(f\"Previous Bar Test Pattern: {prev_detected}\")\n",
    "        \n",
    "        # If pattern detected, show which bar(s)\n",
    "        if current_detected or prev_detected:\n",
    "            print(\"\\\\nPATTERN DETECTED!\")\n",
    "            if current_detected:\n",
    "                print(\"  -> Current bar matches test pattern\")\n",
    "            if prev_detected:\n",
    "                print(\"  -> Previous bar matches test pattern\")\n",
    "        else:\n",
    "            print(\"\\\\nNO PATTERN DETECTED\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(\"CONDITION BREAKDOWN\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Show only last 2 bars for condition analysis\n",
    "        for i in range(max(0, len(df)-2), len(df)):\n",
    "            if i < 2:  # Skip early bars that don't have enough history\n",
    "                continue\n",
    "            \n",
    "            bar_range = df['high'].iloc[i] - df['low'].iloc[i]\n",
    "            close_position = (df['close'].iloc[i] - df['low'].iloc[i]) / bar_range * 100 if bar_range > 0 else 0\n",
    "            \n",
    "            print(f\"\\\\nBAR {i} ({df.index[i].date()})\")\n",
    "            print(f\"OHLC: O={df['open'].iloc[i]:.6f} H={df['high'].iloc[i]:.6f} L={df['low'].iloc[i]:.6f} C={df['close'].iloc[i]:.6f}\")\n",
    "            print(f\"Volume: {df['volume'].iloc[i]:,.0f}\")\n",
    "            \n",
    "            if i > 0:\n",
    "                prev_high = df['high'].iloc[i-1]\n",
    "                prev_low = df['low'].iloc[i-1]\n",
    "                prev_volume = df['volume'].iloc[i-1]\n",
    "                volume_40_pct = prev_volume * 0.4\n",
    "                \n",
    "                # Show conditions in a clean table format\n",
    "                conditions = [\n",
    "                    (\"Inside Bar\", df_with_debug['debug_inside'].iloc[i]),\n",
    "                    (\"Down Bar\", df_with_debug['debug_down'].iloc[i]),\n",
    "                    (\"Close >= 65% from Low\", df_with_debug['debug_close_off_lows'].iloc[i]),\n",
    "                    (\"Volume < 40% of Previous\", df_with_debug['debug_lower_vol'].iloc[i]),\n",
    "                    (\"Lowest Volume in 3 bars\", df_with_debug['debug_lowest_vol_3'].iloc[i])\n",
    "                ]\n",
    "                \n",
    "                print(\"\\\\nConditions:\")\n",
    "                for condition_name, result in conditions:\n",
    "                    status = \"✓\" if result else \"✗\"\n",
    "                    print(f\"  {status} {condition_name:<25} {result}\")\n",
    "                \n",
    "                # Show key metrics\n",
    "                print(\"\\\\nKey Metrics:\")\n",
    "                print(f\"  Close Position: {close_position:.1f}% from low\")\n",
    "                print(f\"  Current Volume: {df['volume'].iloc[i]:,.0f}\")\n",
    "                print(f\"  40% of Prev Vol: {volume_40_pct:,.0f}\")\n",
    "                \n",
    "                if i > 1:\n",
    "                    vol_prev_2 = df['volume'].iloc[i-2]\n",
    "                    print(f\"  Previous Vol: {prev_volume:,.0f}\")\n",
    "                    print(f\"  Previous-2 Vol: {vol_prev_2:,.0f}\")\n",
    "                \n",
    "                # Final result for this bar\n",
    "                all_conditions_met = all(result for _, result in conditions)\n",
    "                print(f\"\\\\nAll Conditions Met: {all_conditions_met}\")\n",
    "                print(f\"Test Bar Signal: {test_bar_signals.iloc[i]}\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running direct test_bar detection: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test parameters\n",
    "exchange_client = KucoinSpotClient\n",
    "timeframe = \"1d\"\n",
    "symbol = \"TEL-USDT\"\n",
    "\n",
    "# Run the test\n",
    "await test_direct_test_bar(exchange_client, timeframe, symbol)\n",
    "\n",
    "print(\"\\\\n✅ Direct test bar detection test completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0c836-afa7-4fba-94d7-0dfafef0f8c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standalone HLC Bar Chart Plotter\n",
    "A reusable function for plotting HLC bars with optional pattern highlighting\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_hlc_bars(data, highlighted_bars=None, title=\"HLC Chart\", symbol=\"SYMBOL\", \n",
    "                  interval=\"1d\", semilog=False, highlight_color=\"fuchsia\", \n",
    "                  highlight_label=\"Pattern\", figsize=(14, 10), show_volume=True):\n",
    "    \"\"\"\n",
    "    Plot HLC bar chart with optional pattern highlighting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame with columns: ['datetime', 'high', 'low', 'close', 'volume']\n",
    "        - datetime: timestamp column (will be used for x-axis)\n",
    "        - high: high prices\n",
    "        - low: low prices  \n",
    "        - close: close prices\n",
    "        - volume: volume data (optional if show_volume=False)\n",
    "        \n",
    "    highlighted_bars : pandas.Series or list/array, optional\n",
    "        Boolean series or array indicating which bars to highlight\n",
    "        Length must match data length\n",
    "        \n",
    "    title : str, default \"HLC Chart\"\n",
    "        Chart title\n",
    "        \n",
    "    symbol : str, default \"SYMBOL\" \n",
    "        Symbol name for display\n",
    "        \n",
    "    interval : str, default \"1d\"\n",
    "        Time interval for date formatting (1m, 5m, 15m, 30m, 1h, 4h, 1d, 3d, 1w, 1M)\n",
    "        \n",
    "    semilog : bool, default False\n",
    "        Use logarithmic scale for price chart\n",
    "        \n",
    "    highlight_color : str, default \"fuchsia\"\n",
    "        Color for highlighted bars\n",
    "        \n",
    "    highlight_label : str, default \"Pattern\"\n",
    "        Label for highlighted bars in legend\n",
    "        \n",
    "    figsize : tuple, default (14, 10)\n",
    "        Figure size (width, height)\n",
    "        \n",
    "    show_volume : bool, default True\n",
    "        Whether to show volume subplot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The created figure object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise ValueError(\"data must be a pandas DataFrame\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['datetime', 'high', 'low', 'close']\n",
    "    if show_volume:\n",
    "        required_cols.append('volume')\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        raise ValueError(\"data DataFrame is empty\")\n",
    "    \n",
    "    # Validate highlighted_bars\n",
    "    if highlighted_bars is not None:\n",
    "        if len(highlighted_bars) != len(data):\n",
    "            raise ValueError(\"highlighted_bars length must match data length\")\n",
    "        # Convert to boolean array\n",
    "        highlighted_bars = np.array(highlighted_bars, dtype=bool)\n",
    "    else:\n",
    "        highlighted_bars = np.zeros(len(data), dtype=bool)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    if show_volume:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, \n",
    "                                       gridspec_kw={'height_ratios': [3, 1]})\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "        ax2 = None\n",
    "    \n",
    "    # Convert dates to numbers for plotting\n",
    "    dates = mdates.date2num(data['datetime'])\n",
    "    \n",
    "    # Calculate margins and tick length\n",
    "    date_range = dates[-1] - dates[0] if len(dates) > 1 else 1\n",
    "    margin = date_range * 0.05\n",
    "    \n",
    "    # Calculate actual bar spacing for consistent tick length\n",
    "    if len(dates) > 1:\n",
    "        avg_bar_spacing = date_range / (len(dates) - 1)\n",
    "        tick_length = avg_bar_spacing * 0.4  # 40% of bar spacing\n",
    "        volume_bar_width = avg_bar_spacing * 0.8\n",
    "    else:\n",
    "        tick_length = date_range * 0.01\n",
    "        volume_bar_width = date_range * 0.02\n",
    "    \n",
    "    # Draw HLC bars\n",
    "    for i, (date, high, low, close) in enumerate(zip(dates, data['high'], data['low'], data['close'])):\n",
    "        is_highlighted = highlighted_bars[i]\n",
    "        color = highlight_color if is_highlighted else 'black'\n",
    "        line_width = 1.2\n",
    "        \n",
    "        # Vertical line from low to high\n",
    "        ax1.plot([date, date], [low, high], color=color, linewidth=line_width, solid_capstyle='butt')\n",
    "        \n",
    "        # Horizontal tick mark for close (on the right side)\n",
    "        ax1.plot([date, date + tick_length], [close, close], color=color, \n",
    "                linewidth=line_width+0.5, solid_capstyle='butt')\n",
    "    \n",
    "    # Configure price chart\n",
    "    if semilog:\n",
    "        ax1.set_yscale('log')\n",
    "        scale_text = \"Semilog Scale\"\n",
    "    else:\n",
    "        scale_text = \"Linear Scale\"\n",
    "    \n",
    "    ax1.set_title(f'{symbol} {title} - {scale_text}', fontsize=16, fontweight='bold')\n",
    "    ax1.set_ylabel('Price', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format x-axis based on timeframe\n",
    "    _format_datetime_axis(ax1, interval)\n",
    "    ax1.set_xlim(dates[0] - margin, dates[-1] + margin)\n",
    "    \n",
    "    # Volume chart\n",
    "    if show_volume and ax2 is not None:\n",
    "        volume_colors = [highlight_color if highlighted_bars[i] else 'orange' for i in range(len(dates))]\n",
    "        volume_edges = ['darkmagenta' if highlighted_bars[i] else 'darkorange' for i in range(len(dates))]\n",
    "        \n",
    "        ax2.bar(dates, data['volume'], width=volume_bar_width, alpha=0.6, \n",
    "                color=volume_colors, edgecolor=volume_edges)\n",
    "        ax2.set_ylabel('Volume', fontsize=12)\n",
    "        ax2.set_xlabel('Date', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        _format_datetime_axis(ax2, interval)\n",
    "        ax2.set_xlim(dates[0] - margin, dates[-1] + margin)\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "    else:\n",
    "        ax1.set_xlabel('Date', fontsize=12)\n",
    "    \n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='black', linewidth=2, label='High-Low Range'),\n",
    "        Line2D([0], [0], color='black', linewidth=3, label='Close Price (right tick)')\n",
    "    ]\n",
    "    \n",
    "    # Add highlighted bars to legend if any exist\n",
    "    if highlighted_bars.any():\n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0], color=highlight_color, linewidth=3, label=highlight_label)\n",
    "        )\n",
    "        highlight_count = highlighted_bars.sum()\n",
    "    else:\n",
    "        highlight_count = 0\n",
    "    \n",
    "    ax1.legend(handles=legend_elements, loc='upper left', title=scale_text)\n",
    "    \n",
    "    # Add pattern count if patterns exist\n",
    "    if highlight_count > 0:\n",
    "        ax1.text(0.99, 0.95, f'{highlight_label}: {highlight_count}', \n",
    "                transform=ax1.transAxes, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "                fontsize=10)\n",
    "    \n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def _format_datetime_axis(ax, interval):\n",
    "    \"\"\"Helper function to format datetime axis based on interval\"\"\"\n",
    "    if interval in ['1m', '5m', '15m', '30m']:\n",
    "        ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
    "    elif interval in ['1h', '2h', '4h', '6h', '12h']:\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    elif interval in ['1d', '3d']:\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    else:  # weekly, monthly\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Example usage and test function\n",
    "def example_usage():\n",
    "    \"\"\"Example showing how to use the plot_hlc_bars function\"\"\"\n",
    "    \n",
    "    # Create sample data\n",
    "    import datetime\n",
    "    dates = pd.date_range('2023-01-01', periods=100, freq='D')\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate realistic OHLC data\n",
    "    closes = 100 + np.cumsum(np.random.randn(100) * 0.02)\n",
    "    highs = closes + np.random.rand(100) * 5\n",
    "    lows = closes - np.random.rand(100) * 5\n",
    "    volumes = np.random.rand(100) * 1000000\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'high': highs,\n",
    "        'low': lows, \n",
    "        'close': closes,\n",
    "        'volume': volumes\n",
    "    })\n",
    "    \n",
    "    # Create some random pattern detections\n",
    "    pattern_detected = np.random.choice([True, False], size=100, p=[0.1, 0.9])\n",
    "    \n",
    "    # Plot with pattern highlighting\n",
    "    fig = plot_hlc_bars(\n",
    "        data=data,\n",
    "        highlighted_bars=pattern_detected,\n",
    "        title=\"Daily Chart with Pattern Detection\",\n",
    "        symbol=\"EXAMPLE\",\n",
    "        interval=\"1d\",\n",
    "        semilog=False,\n",
    "        highlight_color=\"red\",\n",
    "        highlight_label=\"Detected Pattern\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Scanner integration example\n",
    "def scanner_integration_example():\n",
    "    \"\"\"Example of how to integrate with a scanner function\"\"\"\n",
    "    \n",
    "    def my_pattern_scanner(data):\n",
    "        \"\"\"\n",
    "        Example scanner function - replace with your actual scanner logic\n",
    "        Returns boolean array indicating pattern detection\n",
    "        \"\"\"\n",
    "        # Example: detect when close > 20-period moving average\n",
    "        ma20 = data['close'].rolling(20).mean()\n",
    "        pattern = (data['close'] > ma20) & (data['volume'] > data['volume'].rolling(10).mean())\n",
    "        return pattern.fillna(False)\n",
    "    \n",
    "    # Your data loading logic here\n",
    "    # data = load_your_data()  # Replace with actual data loading\n",
    "    \n",
    "    # For demo, create sample data\n",
    "    dates = pd.date_range('2023-01-01', periods=200, freq='D')\n",
    "    np.random.seed(42)\n",
    "    closes = 100 + np.cumsum(np.random.randn(200) * 0.02)\n",
    "    highs = closes + np.random.rand(200) * 3\n",
    "    lows = closes - np.random.rand(200) * 3\n",
    "    volumes = np.random.rand(200) * 1000000\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'high': highs,\n",
    "        'low': lows,\n",
    "        'close': closes,\n",
    "        'volume': volumes\n",
    "    })\n",
    "    \n",
    "    # Run your scanner\n",
    "    detected_patterns = my_pattern_scanner(data)\n",
    "    \n",
    "    # Plot only if patterns are detected\n",
    "    if detected_patterns.any():\n",
    "        print(f\"Patterns detected! Found {detected_patterns.sum()} occurrences\")\n",
    "        fig = plot_hlc_bars(\n",
    "            data=data,\n",
    "            highlighted_bars=detected_patterns,\n",
    "            title=\"Scanner Results\",\n",
    "            symbol=\"SCANNED_SYMBOL\",\n",
    "            interval=\"1d\",\n",
    "            highlight_color=\"lime\",\n",
    "            highlight_label=\"Scanner Hit\"\n",
    "        )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No patterns detected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run example\n",
    "    example_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
